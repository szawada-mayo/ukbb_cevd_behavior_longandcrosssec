{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/cluster/dnax/jars/dnanexus-api-0.1.0-SNAPSHOT-jar-with-dependencies.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/cluster/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-19 22:52:23.031 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2023-09-19 22:52:23.981 WARN  Utils:69 - Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 43000. Attempting port 43001.\n",
      "2023-09-19 22:52:24.205 WARN  MetricsReporter:84 - No metrics configured for reporting\n",
      "2023-09-19 22:52:24.207 WARN  LineProtoUsageReporter:48 - Telegraf configurations: url [metrics.push.telegraf.hostport], user [metrics.push.telegraf.user] or password [metrics.push.telegraf.password] missing.\n",
      "2023-09-19 22:52:24.207 WARN  MetricsReporter:117 - metrics.scraping.httpserver.port\n"
     ]
    }
   ],
   "source": [
    "#run pip install fancyimpute\n",
    "#launch spark \n",
    "import pyspark\n",
    "import dxpy\n",
    "import dxdata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from fancyimpute import IterativeImputer\n",
    "from scipy.stats import chi2_contingency\n",
    "sc = pyspark.SparkContext()\n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#prep database \n",
    "dispensed_database_name = dxpy.find_one_data_object(classname=\"database\", name=\"app*\", folder=\"/\", name_mode=\"glob\", describe=True)[\"describe\"][\"name\"]\n",
    "dispensed_dataset_id = dxpy.find_one_data_object(typename=\"Dataset\", name=\"app*.dataset\", folder=\"/\", name_mode=\"glob\")[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#prep dataset\n",
    "dataset = dxdata.load_dataset(id=dispensed_dataset_id)\n",
    "participant = dataset[\"participant\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-19 22:52:35.289 WARN  ShellBasedUnixGroupsMapping:210 - unable to return groups for user y8bFk6BpbyJVJBX0bKy7JGP0fKQ54pzxkfx6V2x6__project-GG7jpPjJ4VK0VKqV89494XV5\n",
      "PartialGroupNameException The user name 'y8bFk6BpbyJVJBX0bKy7JGP0fKQ54pzxkfx6V2x6__project-GG7jpPjJ4VK0VKqV89494XV5' is not found. id: ‘y8bFk6BpbyJVJBX0bKy7JGP0fKQ54pzxkfx6V2x6__project-GG7jpPjJ4VK0VKqV89494XV5’: no such user\n",
      "id: ‘y8bFk6BpbyJVJBX0bKy7JGP0fKQ54pzxkfx6V2x6__project-GG7jpPjJ4VK0VKqV89494XV5’: no such user\n",
      "\n",
      "\tat org.apache.hadoop.security.ShellBasedUnixGroupsMapping.resolvePartialGroupNames(ShellBasedUnixGroupsMapping.java:294)\n",
      "\tat org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:207)\n",
      "\tat org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:97)\n",
      "\tat org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.getGroups(JniBasedUnixGroupsMappingWithFallback.java:51)\n",
      "\tat org.apache.hadoop.security.Groups$GroupCacheLoader.fetchGroupList(Groups.java:387)\n",
      "\tat org.apache.hadoop.security.Groups$GroupCacheLoader.load(Groups.java:321)\n",
      "\tat org.apache.hadoop.security.Groups$GroupCacheLoader.load(Groups.java:270)\n",
      "\tat org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)\n",
      "\tat org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)\n",
      "\tat org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)\n",
      "\tat org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)\n",
      "\tat org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache.get(LocalCache.java:3962)\n",
      "\tat org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3985)\n",
      "\tat org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4946)\n",
      "\tat org.apache.hadoop.security.Groups.getGroups(Groups.java:228)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.getGroups(UserGroupInformation.java:1734)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1722)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:517)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:254)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3650)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMetaStoreClient(Hive.java:3696)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.lambda$getMSC$0(Hive.java:3770)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3768)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3682)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1600)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1588)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:396)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:305)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:236)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:235)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:285)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:396)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)\n",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:150)\n",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)\n",
      "\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager$lzycompute(SharedState.scala:170)\n",
      "\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager(SharedState.scala:168)\n",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$2(HiveSessionStateBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager$lzycompute(SessionCatalog.scala:119)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager(SessionCatalog.scala:119)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupGlobalTempView(SessionCatalog.scala:940)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTempViews$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveTempViews$$lookupTempView(Analyzer.scala:950)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTempViews$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveTempViews$$lookupAndResolveTempView(Analyzer.scala:963)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTempViews$$anonfun$apply$12.applyOrElse(Analyzer.scala:901)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTempViews$$anonfun$apply$12.applyOrElse(Analyzer.scala:899)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1122)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1121)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode.mapChildren(LogicalPlan.scala:206)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1122)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1121)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode.mapChildren(LogicalPlan.scala:206)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTempViews$.apply(Analyzer.scala:899)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1164)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1131)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:222)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:218)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:167)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:218)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:203)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:202)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:75)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:183)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:183)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:75)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:73)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:65)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "#pull all relevant columns\n",
    "field_names = [\"eid\", \"p31\", \"p22189\", \"p21000_i0\", \"p21001_i0\", 'p21001_i1', 'p21003_i0',  'p21003_i1', \"p1558_i0\",\"p1558_i1\", \"p20002_i0\",  \"p131298\", \"p6150_i0\", \"p131286\", \"p131294\", \"p130814\", \"p130706\", \"p130708\", \"p53_i0\",  \"p53_i1\",  \"p130892\", \"p130894\", \"p130896\", \"p2050_i0\", \"p2060_i0\",  \"p2050_i1\",  \"p130838\", \"p131056\", \"p131058\", \"p131180\", \"p131360\", \"p131362\", \"p131364\", \"p131366\", \"p131368\", \"p131370\", \"p131372\", \"p131374\", \"p131376\", \"p131378\", \"p22032_i0\", 'p21001_i1', 'p1558_i1', 'p21003_i1',  'p21003_i1', 'p20116_i0', 'p20116_i1', 'p20003_i0', 'p1160_i0', 'p20003_i1', 'p1160_i1', 'p1070_i0', 'p1070_i1', 'p1080_i0', 'p1080_i1', 'p1120_i0', 'p1120_i1', 'p110005', 'p90051' , 'p90016', 'p90017', 'p90185', 'p90187', 'p90012', 'p90010', 'p90011', 'p6142_i0' , 'p134_i0', 'p131354', 'p131306', 'p40046_i0', 'p40047_i0', 'p40048_i0', 'p40049_i0', 'p90027', 'p90028', 'p90029', 'p90030', 'p90031', 'p90032', 'p90033', 'p90034', 'p90035', 'p90036', 'p90037', 'p90038', 'p90039', 'p90040', 'p90041', 'p90042', 'p90043', 'p90044', 'p90045', 'p90046', 'p90047', 'p90048', 'p90049', 'p90050', 'p90051', 'p40030_i0', 'p40031_i0', 'p40032_i0', 'p40033_i0', 'p6146_i0', 'p134_i1', 'p20002_i1', 'p22040_i0']\n",
    "df = participant.retrieve_fields(names=field_names, engine=dxdata.connect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-19 22:53:08.083 WARN  package:69 - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    }
   ],
   "source": [
    "#create pandas dataframe\n",
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "502364"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check that pandas conversion worked - this is the full UKBB rows of px\n",
    "#pdf.head(10)\n",
    "len(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pID</th>\n",
       "      <th>male</th>\n",
       "      <th>tdi</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bmi_1</th>\n",
       "      <th>age_i0</th>\n",
       "      <th>age_i1</th>\n",
       "      <th>alc_freq</th>\n",
       "      <th>alc_freq_1</th>\n",
       "      <th>...</th>\n",
       "      <th>pa_time_23</th>\n",
       "      <th>pa_time_24</th>\n",
       "      <th>pa_sleep_day_hour</th>\n",
       "      <th>pa_sed_day_hour</th>\n",
       "      <th>pa_light_day_hour</th>\n",
       "      <th>pa_mv_day_hour</th>\n",
       "      <th>gov_assistance</th>\n",
       "      <th>p134_i1</th>\n",
       "      <th>sr_illness_1</th>\n",
       "      <th>met_minweek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5545411</td>\n",
       "      <td>1</td>\n",
       "      <td>-4.62</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>28.8906</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>15.48</td>\n",
       "      <td>8.51</td>\n",
       "      <td>0.96,0.97,0.95,0.96,1,1,0.95,0.57,0.17,0.07,0,...</td>\n",
       "      <td>0.01,0.03,0.05,0.04,0,0,0.01,0.34,0.62,0.45,0....</td>\n",
       "      <td>0.03,0,0,0,0,0,0.04,0.09,0.21,0.35,0.33,0.22,0...</td>\n",
       "      <td>0,0,0,0,0,0,0,0,0,0.13,0.15,0.34,0.16,0,0.02,0...</td>\n",
       "      <td>[-7]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5180517</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>22.7933</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>31.26</td>\n",
       "      <td>32.41</td>\n",
       "      <td>0.98,1,1,1,1,1,1,0.44,0.02,0.14,0,0,0.04,0.12,...</td>\n",
       "      <td>0.02,0,0,0,0,0,0,0.46,0.52,0.22,0.28,0.33,0.5,...</td>\n",
       "      <td>0,0,0,0,0,0,0,0.11,0.46,0.58,0.57,0.57,0.35,0....</td>\n",
       "      <td>0,0,0,0,0,0,0,0,0,0.05,0.15,0.1,0.1,0.13,0.07,...</td>\n",
       "      <td>[-7]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1668262</td>\n",
       "      <td>1</td>\n",
       "      <td>-4.35</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>25.3439</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[-7]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2151806</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.06</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>32.7087</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[-7]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>594.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4202281</td>\n",
       "      <td>0</td>\n",
       "      <td>8.22</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>24.2078</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[2]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pID  male   tdi  ethnicity      bmi  bmi_1  age_i0  age_i1  alc_freq  \\\n",
       "0  5545411     1 -4.62     1001.0  28.8906    NaN      59     NaN       3.0   \n",
       "1  5180517     0 -0.42     1001.0  22.7933    NaN      52     NaN       5.0   \n",
       "2  1668262     1 -4.35     1001.0  25.3439    NaN      61     NaN       3.0   \n",
       "3  2151806     0 -3.06     1001.0  32.7087    NaN      57     NaN       4.0   \n",
       "4  4202281     0  8.22     1001.0  24.2078    NaN      51     NaN       1.0   \n",
       "\n",
       "   alc_freq_1  ... pa_time_23 pa_time_24  \\\n",
       "0         NaN  ...      15.48       8.51   \n",
       "1         NaN  ...      31.26      32.41   \n",
       "2         NaN  ...        NaN        NaN   \n",
       "3         NaN  ...        NaN        NaN   \n",
       "4         NaN  ...        NaN        NaN   \n",
       "\n",
       "                                   pa_sleep_day_hour  \\\n",
       "0  0.96,0.97,0.95,0.96,1,1,0.95,0.57,0.17,0.07,0,...   \n",
       "1  0.98,1,1,1,1,1,1,0.44,0.02,0.14,0,0,0.04,0.12,...   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                     pa_sed_day_hour  \\\n",
       "0  0.01,0.03,0.05,0.04,0,0,0.01,0.34,0.62,0.45,0....   \n",
       "1  0.02,0,0,0,0,0,0,0.46,0.52,0.22,0.28,0.33,0.5,...   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                   pa_light_day_hour  \\\n",
       "0  0.03,0,0,0,0,0,0.04,0.09,0.21,0.35,0.33,0.22,0...   \n",
       "1  0,0,0,0,0,0,0,0.11,0.46,0.58,0.57,0.57,0.35,0....   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                      pa_mv_day_hour gov_assistance p134_i1  \\\n",
       "0  0,0,0,0,0,0,0,0,0,0.13,0.15,0.34,0.16,0,0.02,0...           [-7]     NaN   \n",
       "1  0,0,0,0,0,0,0,0,0,0.05,0.15,0.1,0.1,0.13,0.07,...           [-7]     NaN   \n",
       "2                                               None           [-7]     NaN   \n",
       "3                                               None           [-7]     NaN   \n",
       "4                                               None            [2]     NaN   \n",
       "\n",
       "  sr_illness_1 met_minweek  \n",
       "0         None         NaN  \n",
       "1         None         NaN  \n",
       "2         None        99.0  \n",
       "3         None       594.0  \n",
       "4         None        99.0  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rename columns\n",
    "pdf_rename = pdf.rename(columns={'eid': 'pID', 'p31': 'male', 'p22189': 'tdi', 'p21000_i0': 'ethnicity', 'p21001_i0': 'bmi', 'p21001_i1': 'bmi_1', 'p1558_i0': 'alc_freq', 'p1558_i1': 'alc_freq_1', 'p20002_i0': 'sr_illness', 'p21003_i0': 'age_i0', 'p21003_i1': 'age_i1',  'p131298': 'i21_date', 'p6150_i0': 'vasc_diag', 'p131298': 'i21_date', 'p131286': 'i10_date', 'p131294': 'i15_date', 'p130814': 'e78_date', 'p130706': 'e10_date', 'p130708': 'e11_date', 'p53_i0': 'date_i0', 'p53_i1': 'date_i1', 'p41271': 'icd9', 'p41270': 'icd10', 'p130892': 'f31_date',  'p130894': 'f32_date', 'p130896': 'f33_date', 'p2050_i0': 'phq2_1', 'p130838': 'f01_date', 'p131056': 'g45_date', 'p131058': 'g46_date', 'p131180': 'h34_date', 'p131360': 'i60_date', 'p131362': 'i61_date', 'p131364': 'i62_date', 'p131366': 'i63_date', 'p131368': 'i64_date', 'p131370': 'i65_date', 'p131372': 'i66_date', 'p131374': 'i67_date', 'p131376': 'i68_date', 'p131378': 'i69_date', 'p20116_i0': 'smok_stat', \"p20116_i1\": 'smok_stat_1', 'p1160_i0': 'sleep', 'p1160_i1': 'sleep_1', 'p1070_i0':'tv', 'p1070_i1':'tv_1', 'p1080_i0':'computer', 'p1080_i1':'computer_1', 'p1120_i0': 'mobile phone', 'p1120_i1': 'mobile phone_1', 'p20510': 'phq2_1_1', 'p22032_i0': 'ipaq', 'p2050_i1': 'phq2_1_followup', 'p90012': 'aac_overall_avg', 'p90185': 'acc_exceed8g_aftercal', 'p90187': 'acc_total_data_read', 'p90017': 'acc_calib_own', 'p90016': 'acc_calib_all', 'p90051': 'acc_weartime', 'p90010': 'acc_start_date', 'p90011': 'acc_end_date', 'p6142_i0': 'employ_status',  'p131354': 'i50_date', 'p131306': 'i25_date', 'p40046_i0': 'pa_sleep_over_avg', 'p40047_i0': 'pa_seden_over_avg', 'p40048_i0': 'pa_light_over_avg',  'p40049_i0': 'pa_mv_over_avg', 'p40049_i0': 'pa_mv_over_avg', 'p90027':'pa_time_1', 'p90028':'pa_time_2', 'p90029':'pa_time_3', 'p90030':'pa_time_4', 'p90031':'pa_time_5', 'p90032':'pa_time_6', 'p90033':'pa_time_7', 'p90034':'pa_time_8', 'p90035':'pa_time_9', 'p90036':'pa_time_10', 'p90037':'pa_time_11', 'p90038':'pa_time_12', 'p90039':'pa_time_13', 'p90040':'pa_time_14', 'p90041':'pa_time_15', 'p90042':'pa_time_16', 'p90043':'pa_time_17', 'p90044':'pa_time_18', 'p90045':'pa_time_19', 'p90046':'pa_time_20', 'p90047':'pa_time_21', 'p90048':'pa_time_22', 'p90049':'pa_time_23', 'p90050':'pa_time_24', 'p90051': 'pa_sum_wear', 'p40030_i0': 'pa_sleep_day_hour', 'p40031_i0': 'pa_sed_day_hour', 'p40032_i0': 'pa_light_day_hour', 'p40033_i0': 'pa_mv_day_hour', 'p6146_i0': 'gov_assistance', 'p20002_i1':'sr_illness_1', 'p22040_i0': 'met_minweek'})\n",
    "#check that rename worked\n",
    "pdf_rename.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_rename['met_minweek'] = pd.to_numeric(pdf_rename['met_minweek'], errors='coerce')\n",
    "pdf_rename['met_minweek_filter'] = pdf_rename['met_minweek']/7\n",
    "pdf_rename['met_minweek_filter'] = pdf_rename['met_minweek_filter']/60\n",
    "pdf_rename.dropna(subset=['met_minweek_filter'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#convert ethnicity data to be workable\n",
    "pdf_rename['ethnicity'].dtype\n",
    "pdf_rename['ethnicity'] = pd.to_numeric(pdf_rename['ethnicity'], errors='coerce')\n",
    "pdf_rename['ethnicity'].dtype\n",
    "for index, row in pdf_rename.iterrows():\n",
    "    if row['ethnicity'] == 1.0 or row['ethnicity'] == 1001.0 or row['ethnicity'] == 1002.0 or row['ethnicity'] == 1003.0:\n",
    "        pdf_rename.at[index, 'white_yes'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after dropping px with missing sleep survey data 384383\n"
     ]
    }
   ],
   "source": [
    "#drop -1 (\"do not know\") and -3 (\"prefer not to answer\") from sleep duration survey\n",
    "pdf_rename = pdf_rename[~(pdf_rename['sleep'] == -3)]\n",
    "pdf_rename = pdf_rename[~(pdf_rename['sleep'] == -1)]\n",
    "print('after dropping px with missing sleep survey data', len(pdf_rename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after dropping px with missing smoking survey data 383441\n"
     ]
    }
   ],
   "source": [
    "#for smoking status, drop anyone who answered \"-3/prefer not to answer\"\n",
    "pdf_rename['smok_stat'] = pdf_rename['smok_stat'].astype(str)\n",
    "pdf_rename = pdf_rename[~pdf_rename['smok_stat'].str.contains('-3')]\n",
    "print('after dropping px with missing smoking survey data', len(pdf_rename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after dropping px with incomplete screen surveys data 379604\n"
     ]
    }
   ],
   "source": [
    "#drop -1 and -3 from tv and computer and mobile phone use; fix tv use value of -10\n",
    "pdf_rename = pdf_rename[~(pdf_rename['tv'] == -3)]\n",
    "pdf_rename = pdf_rename[~(pdf_rename['tv'] == -1)]\n",
    "pdf_rename = pdf_rename[~(pdf_rename['computer'] == -3)]\n",
    "pdf_rename = pdf_rename[~(pdf_rename['computer'] == -1)]\n",
    "pdf_rename = pdf_rename[~(pdf_rename['mobile phone'] == -3)]\n",
    "pdf_rename = pdf_rename[~(pdf_rename['mobile phone'] == -1)]\n",
    "pdf_rename['tv'] = pdf_rename['tv'].replace(-10, 0.5)\n",
    "print('after dropping px with incomplete screen surveys data', len(pdf_rename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after dropping px with missing sleep survey data 324750\n"
     ]
    }
   ],
   "source": [
    "pdf_rename.dropna(subset=['computer', 'tv', 'mobile phone'], inplace=True)\n",
    "print('after dropping px with missing sleep survey data', len(pdf_rename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after dropping px with incomplete alcohol survey data 324638\n"
     ]
    }
   ],
   "source": [
    "#drop alc_freq values for \"prefer not to answer\"/-3\n",
    "pdf_rename = pdf_rename[~(pdf_rename['alc_freq'] == -3)]\n",
    "print('after dropping px with incomplete alcohol survey data', len(pdf_rename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reorder alc_freq values \n",
    "#if 4\tOne to three times a month | 5\tSpecial occasions only | 6\tNever, then make seldom=1\n",
    "#if 2\tThree or four times a week | 3\tOnce or twice a week, then make sometimes=2\n",
    "#if 1\tDaily or almost daily, then make daily=3\n",
    "pdf_rename['alc_freq_filter'] = np.where(pdf_rename['alc_freq'].isin([4, 5, 6]), 1, \n",
    "                    np.where(pdf_rename['alc_freq'].isin([2, 3]), 2, \n",
    "                             np.where(pdf_rename['alc_freq'].isin([1]), 3, np.nan)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "323670"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop phq2 scores where response was \"-3/prefer not to answer\"\n",
    "pdf_rename = pdf_rename.drop(pdf_rename.loc[pdf_rename['phq2_1'] < -1].index)\n",
    "len(pdf_rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "323670"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace all occurrences of -1 (do not know if depressed) in column PHQ2_1 and PHQ2_2 with 0\n",
    "pdf_rename['phq2_1'] = pdf_rename['phq2_1'].replace(-1, 0)\n",
    "len(pdf_rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#convert sr_illness to string\n",
    "#pdf_rename['sr_illness'].dtype\n",
    "#pdf_rename['sr_illness'] = pdf_rename['sr_illness'].astype(str)\n",
    "#remove 1075= heart attack, 1081= stroke, 1082= tia, 1086= sah, 1282= brao/crao, 1491= brain hemorrhage, 1583= ischemic stroke\n",
    "#pdf_rename = pdf_rename[~pdf_rename['sr_illness'].str.contains('1075|1081|1082|1086|1282|1491|1583')]\n",
    "#print('after dropping px with self-reported HA or CeVD prior to baseline', len(pdf_rename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#convert dates to datetime values or analysis\n",
    "pdf_rename[\"date_i0\"] = pd.to_datetime(pdf_rename[\"date_i0\"])\n",
    "pdf_rename[\"i21_date\"] = pd.to_datetime(pdf_rename[\"i21_date\"])\n",
    "pdf_rename[\"f01_date\"] = pd.to_datetime(pdf_rename[\"f01_date\"])\n",
    "pdf_rename[\"g45_date\"] = pd.to_datetime(pdf_rename[\"g45_date\"])\n",
    "pdf_rename[\"g46_date\"] = pd.to_datetime(pdf_rename[\"g46_date\"])\n",
    "pdf_rename[\"h34_date\"] = pd.to_datetime(pdf_rename[\"h34_date\"])\n",
    "pdf_rename[\"i60_date\"] = pd.to_datetime(pdf_rename[\"i60_date\"])\n",
    "pdf_rename[\"i61_date\"] = pd.to_datetime(pdf_rename[\"i61_date\"])\n",
    "pdf_rename[\"i62_date\"] = pd.to_datetime(pdf_rename[\"i62_date\"])\n",
    "pdf_rename[\"i63_date\"] = pd.to_datetime(pdf_rename[\"i63_date\"])\n",
    "pdf_rename[\"i64_date\"] = pd.to_datetime(pdf_rename[\"i64_date\"])\n",
    "pdf_rename[\"i65_date\"] = pd.to_datetime(pdf_rename[\"i65_date\"])\n",
    "pdf_rename[\"i66_date\"] = pd.to_datetime(pdf_rename[\"i66_date\"])\n",
    "pdf_rename[\"i67_date\"] = pd.to_datetime(pdf_rename[\"i67_date\"])\n",
    "pdf_rename[\"i68_date\"] = pd.to_datetime(pdf_rename[\"i68_date\"])\n",
    "pdf_rename[\"i69_date\"] = pd.to_datetime(pdf_rename[\"i69_date\"])\n",
    "pdf_rename[\"f31_date\"] = pd.to_datetime(pdf_rename[\"f31_date\"])\n",
    "pdf_rename[\"f32_date\"] = pd.to_datetime(pdf_rename[\"f32_date\"])\n",
    "pdf_rename[\"f33_date\"] = pd.to_datetime(pdf_rename[\"f33_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#drop px with prior heart attack (HA) \n",
    "#convert date columns to days format\n",
    "#pdf_rename['i21_time'] = (pdf_rename['date_i0'] - pdf_rename['i21_date']).dt.days\n",
    "#drop cases with a prior heart attack (I21)\n",
    "#pdf_rename = pdf_rename.loc[(pdf_rename['i21_time'] < 0) | (pdf_rename['i21_time'].isna())]\n",
    "#print('after dropping px with medical record HA prior to baseline', len(pdf_rename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#convert vasc_diag to string\n",
    "#pdf_rename['vasc_diag'].dtype\n",
    "#pdf_rename['vasc_diag'] = pdf_rename['vasc_diag'].astype(str)\n",
    "#drop px who report prior vascular diagnoses of HA, STROKE, PREFER NOT TO ANSWER\n",
    "#pdf_rename = pdf_rename.loc[(pdf_rename['vasc_diag'] == '[-7]') | (pdf_rename['vasc_diag']  == '[4]') | (pdf_rename['vasc_diag']  == '[2]')]\n",
    "#print('after dropping px with other self-report question on HA/stroke/prefer not to answer prior to baseline', len(pdf_rename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop cases with a prior vascular dementia (F01)\n",
    "pdf_rename['f01_time'] = (pdf_rename['date_i0'] - pdf_rename['f01_date']).dt.days\n",
    "pdf_rename['g45_time'] = (pdf_rename['date_i0'] - pdf_rename['g45_date']).dt.days\n",
    "pdf_rename['g46_time'] = (pdf_rename['date_i0'] - pdf_rename['g46_date']).dt.days\n",
    "pdf_rename['h34_time'] = (pdf_rename['date_i0'] - pdf_rename['h34_date']).dt.days\n",
    "pdf_rename['i60_time'] = (pdf_rename['date_i0'] - pdf_rename['i60_date']).dt.days\n",
    "pdf_rename['i61_time'] = (pdf_rename['date_i0'] - pdf_rename['i61_date']).dt.days\n",
    "pdf_rename['i62_time'] = (pdf_rename['date_i0'] - pdf_rename['i62_date']).dt.days\n",
    "pdf_rename['i63_time'] = (pdf_rename['date_i0'] - pdf_rename['i63_date']).dt.days\n",
    "pdf_rename['i64_time'] = (pdf_rename['date_i0'] - pdf_rename['i64_date']).dt.days\n",
    "pdf_rename['i65_time'] = (pdf_rename['date_i0'] - pdf_rename['i65_date']).dt.days\n",
    "pdf_rename['i66_time'] = (pdf_rename['date_i0'] - pdf_rename['i66_date']).dt.days\n",
    "pdf_rename['i67_time'] = (pdf_rename['date_i0'] - pdf_rename['i67_date']).dt.days\n",
    "pdf_rename['i68_time'] = (pdf_rename['date_i0'] - pdf_rename['i68_date']).dt.days\n",
    "pdf_rename['i69_time'] = (pdf_rename['date_i0'] - pdf_rename['i69_date']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2640/3156182845.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  new_df = new_df.append(row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "def df_1yr_prior_cevd(df):\n",
    "    new_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if (row['f01_time'] > 0 and row['f01_time'] < 366) or (row['g45_time'] > 0 and row['g45_time'] < 366) or (row['g46_time'] > 0 and row['g46_time'] < 366) or (row['h34_time'] > 0 and row['h34_time'] < 366) or (row['i60_time'] > 0 and row['i60_time'] < 366) or (row['i61_time'] > 0 and row['i61_time'] < 366) or (row['i62_time'] > 0 and row['i62_time'] < 366) or (row['i63_time'] > 0 and row['i63_time'] < 366) or (row['i64_time'] > 0 and row['i64_time'] < 366) or (row['i65_time'] > 0 and row['i65_time'] < 366) or (row['i66_time'] > 0 and row['i66_time'] < 366) or (row['i67_time'] > 0 and row['i67_time'] < 366) or (row['i68_time'] > 0 and row['i68_time'] < 366) or (row['i69_time'] > 0 and row['i69_time'] < 366):\n",
    "            new_df = new_df.append(row, ignore_index=True)\n",
    "\n",
    "    return new_df\n",
    "pdf_rename_check = df_1yr_prior_cevd(pdf_rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the number of px with a CeVD event within 1 years before baseline: 807\n"
     ]
    }
   ],
   "source": [
    "print('This is the number of px with a CeVD event within 1 years before baseline:', len(pdf_rename_check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf_rename_check.to_csv('pdf_rename_check.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdf_rename = pdf_rename.loc[(pdf_rename['f01_time'] > 0) | (pdf_rename['g45_time'] > 0) | (pdf_rename['g46_time'] > 0) | (pdf_rename['h34_time'] > 0) | (pdf_rename['i60_time'] > 0) | (pdf_rename['i61_time'] > 0) | (pdf_rename['i62_time'] > 0) | (pdf_rename['i63_time'] > 0) | (pdf_rename['i64_time'] > 0) | (pdf_rename['i65_time'] > 0) | (pdf_rename['i66_time'] > 0) | (pdf_rename['i67_time'] > 0) | (pdf_rename['i68_time'] > 0) | (pdf_rename['i69_time'] > 0)]\n",
    "#pdf_rename_5yr = pdf_rename.loc[(pdf_rename['f01_time'] < 1826) | (pdf_rename['g45_time'] < 1826) | (pdf_rename['g46_time'] < 1826) | (pdf_rename['h34_time'] < 1826) | (pdf_rename['i60_time'] < 1826) | (pdf_rename['i61_time'] < 1826) | (pdf_rename['i62_time'] < 1826) | (pdf_rename['i63_time'] < 1826) | (pdf_rename['i64_time'] < 1826) | (pdf_rename['i65_time'] < 1826) | (pdf_rename['i66_time'] < 1826) | (pdf_rename['i67_time'] < 1826) | (pdf_rename['i68_time'] < 1826) | (pdf_rename['i69_time'] < 1826)]\n",
    "#pdf_rename_3yr = pdf_rename.loc[(pdf_rename['f01_time'] < 1096) | (pdf_rename['g45_time'] < 1096) | (pdf_rename['g46_time'] < 1096) | (pdf_rename['h34_time'] < 1096) | (pdf_rename['i60_time'] < 1096) | (pdf_rename['i61_time'] < 1096) | (pdf_rename['i62_time'] < 1096) | (pdf_rename['i63_time'] < 1096) | (pdf_rename['i64_time'] < 1096) | (pdf_rename['i65_time'] < 1096) | (pdf_rename['i66_time'] < 1096) | (pdf_rename['i67_time'] < 1096) | (pdf_rename['i68_time'] < 1096) | (pdf_rename['i69_time'] < 1096)]\n",
    "#pdf_rename_1yr = pdf_rename.loc[(pdf_rename['f01_time'] < 366) | (pdf_rename['g45_time'] < 366) | (pdf_rename['g46_time'] < 366) | (pdf_rename['h34_time'] < 366) | (pdf_rename['i60_time'] < 366) | (pdf_rename['i61_time'] < 366) | (pdf_rename['i62_time'] < 366) | (pdf_rename['i63_time'] < 366) | (pdf_rename['i64_time'] < 366) | (pdf_rename['i65_time'] < 366) | (pdf_rename['i66_time'] < 366) | (pdf_rename['i67_time'] < 366) | (pdf_rename['i68_time'] < 366) | (pdf_rename['i69_time'] < 366)]\n",
    "\n",
    "#pdf_rename['f01_time'] = pdf_rename['f01_time'].fillna(0)\n",
    "#pdf_rename['f01_time'] = pdf_rename['f01_time'].astype(str)\n",
    "#pdf_rename_5yr.to_csv('9.11.23_5yr.csv')\n",
    "#pdf_rename_3yr.to_csv('9.11.23_3yr.csv')\n",
    "#pdf_rename_1yr.to_csv('9.11.23_1yr.csv')\n",
    "#print('This is the number of px with a CeVD event within 5 years before baseline:', len(pdf_rename_5yr))\n",
    "#print('This is the number of px with a CeVD event within 3 years before baseline:', len(pdf_rename_3yr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can alternate analyses using these dfs - we used 1 yr for my analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "807"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_rename = pdf_rename_check\n",
    "len(pdf_rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of px with depression diagnosis (F31, F32, F33): 124\n"
     ]
    }
   ],
   "source": [
    "#prep depression cases in \"days\" format\n",
    "pdf_rename['f31_time'] = (pdf_rename['date_i0'] - pdf_rename['f31_date']).dt.days\n",
    "pdf_rename['f32_time'] = (pdf_rename['date_i0'] - pdf_rename['f32_date']).dt.days\n",
    "pdf_rename['f33_time'] = (pdf_rename['date_i0'] - pdf_rename['f33_date']).dt.days\n",
    "# check for diagnosed depression; if yes, return 1\n",
    "def check_for_dep_diag(row):\n",
    "    if row['f31_time'] > 0 or row['f32_time'] > 0 or row['f33_time'] > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "pdf_rename['dep_diag_yes'] = pdf_rename.apply(check_for_dep_diag, axis=1)\n",
    "check_dep_diag = pdf_rename['dep_diag_yes'].sum()\n",
    "print(\"Number of px with depression diagnosis (F31, F32, F33):\", check_dep_diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median depression duration: 611.5\n"
     ]
    }
   ],
   "source": [
    "#pull earliest depression diagnosis per row; make depression duration variable\n",
    "pdf_rename['dep_dur'] = pdf_rename[['f31_time', 'f32_time', 'f33_time']].max(axis=1)\n",
    "print(\"median depression duration:\", pdf_rename['dep_dur'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#check depression duration variable with histogram\n",
    "plt.hist(pdf_rename['dep_dur'], bins=20, color='blue', alpha=0.7) \n",
    "plt.xlabel('Depression Duration (Days)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#fill empty rows in white_yes with 0; some 0 variables did not convert correctly and need manual imput\n",
    "pdf_rename['white_yes'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#calculate baseline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create comorbidity covariate in multiple steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#convert comorbidity diagnosis dates to datetime format\n",
    "pdf_rename[\"i10_date\"] = pd.to_datetime(pdf_rename[\"i10_date\"])\n",
    "pdf_rename[\"i15_date\"] = pd.to_datetime(pdf_rename[\"i15_date\"])\n",
    "pdf_rename[\"e78_date\"] = pd.to_datetime(pdf_rename[\"e78_date\"])\n",
    "pdf_rename[\"e10_date\"] = pd.to_datetime(pdf_rename[\"e10_date\"])\n",
    "pdf_rename[\"e11_date\"] = pd.to_datetime(pdf_rename[\"e11_date\"])\n",
    "pdf_rename[\"i50_date\"] = pd.to_datetime(pdf_rename[\"i50_date\"])\n",
    "pdf_rename[\"i25_date\"] = pd.to_datetime(pdf_rename[\"i25_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create hypertension category\n",
    "#prep htn cases\n",
    "pdf_rename['i10_time'] = (pdf_rename['date_i0'] - pdf_rename['i10_date']).dt.days\n",
    "pdf_rename['i15_time'] = (pdf_rename['date_i0'] - pdf_rename['i15_date']).dt.days\n",
    "\n",
    "# check for htn; if yes, return 1\n",
    "def check_htn_criteria(row):\n",
    "    if row['i10_time'] > 0 or row['i15_time'] > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "pdf_rename['htn_yes'] = pdf_rename.apply(check_htn_criteria, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create heart failure category\n",
    "#prep  cases\n",
    "pdf_rename['i50_time'] = (pdf_rename['date_i0'] - pdf_rename['i50_date']).dt.days\n",
    "\n",
    "# check for htn; if yes, return 1\n",
    "def check_htn_criteria(row):\n",
    "    if row['i50_time'] > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "pdf_rename['hf_yes'] = pdf_rename.apply(check_htn_criteria, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create cad category\n",
    "#prep  cases\n",
    "pdf_rename['i25_time'] = (pdf_rename['date_i0'] - pdf_rename['i25_date']).dt.days\n",
    "\n",
    "# check for htn; if yes, return 1\n",
    "def check_htn_criteria(row):\n",
    "    if row['i25_time'] > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "pdf_rename['cad_yes'] = pdf_rename.apply(check_htn_criteria, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create cancer covariate\n",
    "pdf_rename['cancer_yes'] = pdf_rename['p134_i0'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create hyperlipidemia category\n",
    "#prep hchol cases\n",
    "pdf_rename['e78_time'] = (pdf_rename['date_i0'] - pdf_rename['e78_date']).dt.days\n",
    "def check_hchol_criteria(row):\n",
    "    if row['e78_time'] > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "pdf_rename['hchol_yes'] = pdf_rename.apply(check_hchol_criteria, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create diabetes category\n",
    "#prep diabetes cases\n",
    "pdf_rename['e10_date'] = pd.to_datetime(pdf_rename['e10_date'])\n",
    "pdf_rename['e11_date'] = pd.to_datetime(pdf_rename['e11_date'])\n",
    "pdf_rename['date_i0'] = pd.to_datetime(pdf_rename['date_i0'])\n",
    "pdf_rename['e10_time'] = (pdf_rename['date_i0'] - pdf_rename['e10_date']).dt.days\n",
    "pdf_rename['e11_time'] = (pdf_rename['date_i0'] - pdf_rename['e11_date']).dt.days\n",
    "\n",
    "def check_diab_criteria(row):\n",
    "    if row['e10_time'] > 0 or row['e11_time'] > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "pdf_rename['diab_yes'] = pdf_rename.apply(check_diab_criteria, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create comorbidity covariate for analysis and filter covariate for baseline characteristics table considering set of comorbidities created above\n",
    "pdf_rename['comorb_sum'] = pdf_rename[['htn_yes', 'hf_yes', 'cad_yes', 'cancer_yes', 'hchol_yes', 'diab_yes']].sum(axis=1)\n",
    "pdf_rename['comorb_sum_filter'] = pdf_rename['comorb_sum'].apply(lambda x: 0 if x == 0 else (1 if x == 1 else 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#prep clean cevd endpoint variable as first day in px chart with CeVD diagnosis after accelerometer study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#if assessment date - event date < 0, then copy absolute value to corresponding column\n",
    "pdf_rename.loc[pdf_rename['f01_time'] < 0, 'f01_t2e'] = abs(pdf_rename['f01_time'])\n",
    "pdf_rename.loc[pdf_rename['g45_time'] < 0, 'g45_t2e'] = abs(pdf_rename['g45_time'])\n",
    "pdf_rename.loc[pdf_rename['g46_time'] < 0, 'g46_t2e'] = abs(pdf_rename['g46_time'])\n",
    "pdf_rename.loc[pdf_rename['h34_time'] < 0, 'h34_t2e'] = abs(pdf_rename['h34_time'])\n",
    "pdf_rename.loc[pdf_rename['i60_time'] < 0, 'i60_t2e'] = abs(pdf_rename['i60_time'])\n",
    "pdf_rename.loc[pdf_rename['i61_time'] < 0, 'i61_t2e'] = abs(pdf_rename['i61_time'])\n",
    "pdf_rename.loc[pdf_rename['i62_time'] < 0, 'i62_t2e'] = abs(pdf_rename['i62_time'])\n",
    "pdf_rename.loc[pdf_rename['i63_time'] < 0, 'i63_t2e'] = abs(pdf_rename['i63_time'])\n",
    "pdf_rename.loc[pdf_rename['i64_time'] < 0, 'i64_t2e'] = abs(pdf_rename['i64_time'])\n",
    "pdf_rename.loc[pdf_rename['i65_time'] < 0, 'i65_t2e'] = abs(pdf_rename['i65_time'])\n",
    "pdf_rename.loc[pdf_rename['i66_time'] < 0, 'i66_t2e'] = abs(pdf_rename['i66_time'])\n",
    "pdf_rename.loc[pdf_rename['i67_time'] < 0, 'i67_t2e'] = abs(pdf_rename['i67_time'])\n",
    "pdf_rename.loc[pdf_rename['i68_time'] < 0, 'i68_t2e'] = abs(pdf_rename['i68_time'])\n",
    "pdf_rename.loc[pdf_rename['i69_time'] < 0, 'i69_t2e'] = abs(pdf_rename['i69_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#if there is a time-to-event in XXX_t2e, then place 1 in outcome column for each CeVD\n",
    "pdf_rename.loc[pdf_rename['f01_t2e'].notna(), 'f01_event'] = 1\n",
    "pdf_rename.loc[pdf_rename['g45_t2e'].notna(), 'g45_event'] = 1\n",
    "pdf_rename.loc[pdf_rename['g46_t2e'].notna(), 'g46_event'] = 1\n",
    "pdf_rename.loc[pdf_rename['h34_t2e'].notna(), 'h34_event'] = 1\n",
    "pdf_rename.loc[pdf_rename['i60_t2e'].notna(), 'i60_event'] = 1\n",
    "pdf_rename.loc[pdf_rename['i61_t2e'].notna(), 'i61_event'] = 1\n",
    "pdf_rename.loc[pdf_rename['i62_t2e'].notna(), 'i62_event'] = 1\n",
    "pdf_rename.loc[pdf_rename['i63_t2e'].notna(), 'i63_event'] = 1\n",
    "pdf_rename.loc[pdf_rename['i64_t2e'].notna(), 'i64_event'] = 1\n",
    "pdf_rename.loc[pdf_rename['i65_t2e'].notna(), 'i65_event'] = 1\n",
    "pdf_rename.loc[pdf_rename['i66_t2e'].notna(), 'i66_event'] = 1\n",
    "pdf_rename.loc[pdf_rename['i67_t2e'].notna(), 'i67_event'] = 1\n",
    "pdf_rename.loc[pdf_rename['i68_t2e'].notna(), 'i68_event'] = 1\n",
    "pdf_rename.loc[pdf_rename['i69_t2e'].notna(), 'i69_event'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CeVD events (total) 236.0\n"
     ]
    }
   ],
   "source": [
    "#create composite cevd outcome value\n",
    "pdf_rename.loc[(pdf_rename['f01_event'].notna()) | (pdf_rename['g45_event'].notna()) | (pdf_rename['g46_event'].notna()) | (pdf_rename['h34_event'].notna()) | (pdf_rename['i60_event'].notna()) | (pdf_rename['i61_event'].notna()) | (pdf_rename['i62_event'].notna()) | (pdf_rename['i63_event'].notna()) | (pdf_rename['i64_event'].notna()) | (pdf_rename['i65_event'].notna()) | (pdf_rename['i66_event'].notna()) | (pdf_rename['i67_event'].notna()) | (pdf_rename['i68_event'].notna()) | (pdf_rename['i69_event'].notna()), 'cevd_event'] = 1\n",
    "pdf_rename['cevd_event'] = pdf_rename['cevd_event'].fillna(0)\n",
    "\n",
    "#check number of future cevd_events \n",
    "sum_cevdevents = pdf_rename['cevd_event'].sum()\n",
    "print('Number of CeVD events (total)', sum_cevdevents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create cevd time-to-event value for earliest cevd event\n",
    "pdf_rename['cevd_t2e'] = pdf_rename[['f01_t2e', 'g45_t2e', 'g46_t2e', 'h34_t2e', 'i60_t2e', 'i61_t2e', 'i62_t2e', 'i63_t2e', 'i64_t2e', 'i65_t2e', 'i66_t2e', 'i67_t2e', 'i68_t2e', 'i69_t2e']].min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#convert all t2e columns to numeric for filtering\n",
    "def convert_t2e_to_numeric(df):\n",
    "    df['f01_t2e'] = pd.to_numeric(df['f01_t2e'], errors='coerce').astype(pd.Int64Dtype())\n",
    "    df['g45_t2e'] = pd.to_numeric(df['g45_t2e'], errors='coerce').astype(pd.Int64Dtype())\n",
    "    df['g46_t2e'] = pd.to_numeric(df['g46_t2e'], errors='coerce').astype(pd.Int64Dtype())\n",
    "    df['h34_t2e'] = pd.to_numeric(df['h34_t2e'], errors='coerce').astype(pd.Int64Dtype())\n",
    "    df['i60_t2e'] = pd.to_numeric(df['i60_t2e'], errors='coerce').astype(pd.Int64Dtype())\n",
    "    df['i61_t2e'] = pd.to_numeric(df['i61_t2e'], errors='coerce').astype(pd.Int64Dtype())\n",
    "    df['i62_t2e'] = pd.to_numeric(df['i62_t2e'], errors='coerce').astype(pd.Int64Dtype())\n",
    "    df['i63_t2e'] = pd.to_numeric(df['i63_t2e'], errors='coerce').astype(pd.Int64Dtype())\n",
    "    df['i64_t2e'] = pd.to_numeric(df['i64_t2e'], errors='coerce').astype(pd.Int64Dtype())\n",
    "    df['i65_t2e'] = pd.to_numeric(df['i65_t2e'], errors='coerce').astype(pd.Int64Dtype())\n",
    "    df['i66_t2e'] = pd.to_numeric(df['i66_t2e'], errors='coerce').astype(pd.Int64Dtype())\n",
    "    df['i67_t2e'] = pd.to_numeric(df['i67_t2e'], errors='coerce').astype(pd.Int64Dtype())\n",
    "    df['i68_t2e'] = pd.to_numeric(df['i68_t2e'], errors='coerce').astype(pd.Int64Dtype())\n",
    "    df['i69_t2e'] = pd.to_numeric(df['i69_t2e'], errors='coerce').astype(pd.Int64Dtype())\n",
    "    df['cevd_t2e'] = pd.to_numeric(df['cevd_t2e'], errors='coerce').astype(pd.Int64Dtype())\n",
    "    return df\n",
    "\n",
    "pdf_rename = convert_t2e_to_numeric(pdf_rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#clean all cevd types to only have t2e for the initial CeVD diagnosis after accelerometer study commencement\n",
    "def check_cevd_types(df):\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.notna(row['f01_t2e']):\n",
    "            if row['f01_t2e'] != row['cevd_t2e']:\n",
    "                df.at[index, 'f01_t2e'] = None\n",
    "        if pd.notna(row['g45_t2e']):\n",
    "            if row['g45_t2e'] != row['cevd_t2e']:\n",
    "                 df.at[index, 'g45_t2e'] = None\n",
    "        if pd.notna(row['g46_t2e']):\n",
    "            if row['g46_t2e'] != row['cevd_t2e']:\n",
    "                 df.at[index, 'g46_t2e'] = None\n",
    "        if pd.notna(row['h34_t2e']):\n",
    "            if row['h34_t2e'] != row['cevd_t2e']:\n",
    "                 df.at[index, 'h34_t2e'] = None\n",
    "        if pd.notna(row['i60_t2e']):\n",
    "            if row['i60_t2e'] != row['cevd_t2e']:\n",
    "                 df.at[index, 'i60_t2e'] = None\n",
    "        if pd.notna(row['i61_t2e']):\n",
    "            if row['i61_t2e'] != row['cevd_t2e']:\n",
    "                 df.at[index, 'i61_t2e'] = None\n",
    "        if pd.notna(row['i62_t2e']):\n",
    "            if row['i62_t2e'] != row['cevd_t2e']:\n",
    "                 df.at[index, 'i62_t2e'] = None\n",
    "        if pd.notna(row['i63_t2e']):\n",
    "            if row['i63_t2e'] != row['cevd_t2e']:\n",
    "                 df.at[index, 'i63_t2e'] = None\n",
    "        if pd.notna(row['i64_t2e']):\n",
    "            if row['i64_t2e'] != row['cevd_t2e']:\n",
    "                 df.at[index, 'i64_t2e'] = None\n",
    "        if pd.notna(row['i65_t2e']):\n",
    "            if row['i65_t2e'] != row['cevd_t2e']:\n",
    "                 df.at[index, 'i65_t2e'] = None\n",
    "        if pd.notna(row['i66_t2e']):\n",
    "            if row['i66_t2e'] != row['cevd_t2e']:\n",
    "                 df.at[index, 'i66_t2e'] = None\n",
    "        if pd.notna(row['i67_t2e']):\n",
    "            if row['i67_t2e'] != row['cevd_t2e']:\n",
    "                 df.at[index, 'i67_t2e'] = None\n",
    "        if pd.notna(row['i68_t2e']):\n",
    "            if row['i68_t2e'] != row['cevd_t2e']:\n",
    "                 df.at[index, 'i68_t2e'] = None\n",
    "        if pd.notna(row['i69_t2e']):\n",
    "            if row['i69_t2e'] != row['cevd_t2e']:\n",
    "                 df.at[index, 'i69_t2e'] = None\n",
    "    return df\n",
    "\n",
    "cevd_df_first = check_cevd_types(pdf_rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows with missing BMI data: 10\n",
      "number of rows with missing TDI data: 1\n"
     ]
    }
   ],
   "source": [
    "#get the number of missing BMI values for imputation\n",
    "missing_bmi = cevd_df_first['bmi'].isna().sum()\n",
    "print('number of rows with missing BMI data:', missing_bmi)\n",
    "#get the number of missing BMI values for imputation\n",
    "missing_tdi = cevd_df_first['tdi'].isna().sum()\n",
    "print('number of rows with missing TDI data:', missing_tdi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#USE MUTIPLE CHAIN IMPUTATION FOR MISSING BMI DATA\n",
    "cols_to_impute = ['bmi', 'tdi']\n",
    "# create an instance of the IterativeImputer class with MICE algorithm\n",
    "imputer = IterativeImputer(max_iter=10, sample_posterior=True)\n",
    "# impute the selected columns using multiple chains\n",
    "cevd_df_first[cols_to_impute] = imputer.fit_transform(cevd_df_first[cols_to_impute])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create a value with the day of intitial CeVD diagnosis after accelerometer study commencement\n",
    "def find_firstday_cevd(row):\n",
    "    dates = [row['f01_date'], row['g45_date'], row['g46_date'], row['h34_date'], row['i60_date'], row['i61_date'], row['i62_date'], row['i63_date'], row['i64_date'], row['i65_date'], row['i66_date'], row['i67_date'], row['i68_date'], row['i69_date']]\n",
    "    non_empty_dates = [date for date in dates if not pd.isnull(date)]\n",
    "    return min(non_empty_dates) if non_empty_dates else pd.NaT\n",
    "\n",
    "cevd_df_first['cevd_day'] = cevd_df_first.apply(find_firstday_cevd, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confirming the df length is 307,008 (complete) after wrangling 807\n"
     ]
    }
   ],
   "source": [
    "print('confirming the df length is 807 (complete) after wrangling', len(cevd_df_first))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patients in cohort with a future CeVD: 236\n"
     ]
    }
   ],
   "source": [
    "#check the number of px with a future CeVD in cohort\n",
    "total_futurecevd = cevd_df_first[cevd_df_first['cevd_event']==1]\n",
    "print('Number of patients in cohort with a future CeVD:', len(total_futurecevd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create screen use covariate in multiple steps\n",
    "screen_column = ['computer', 'tv', 'mobile phone']\n",
    "cevd_df_first['tv'] = cevd_df_first['tv'].replace(-10, 0.5)\n",
    "cevd_df_first['computer'] = cevd_df_first['computer'].replace(-10, 0.5)\n",
    "cevd_df_first[screen_column] = cevd_df_first[screen_column].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reorder mobile phone use per week values \n",
    "#convert to hours or percentage of hours tied to \n",
    "cevd_df_first['mobile phone_filter'] = np.where(cevd_df_first['mobile phone'].isin([5]), 6, \n",
    "                    np.where(cevd_df_first['mobile phone'].isin([4]), 4,\n",
    "                    np.where(cevd_df_first['mobile phone'].isin([3]), 2,\n",
    "                    np.where(cevd_df_first['mobile phone'].isin([2]), 0.742, \n",
    "                    np.where(cevd_df_first['mobile phone'].isin([1]), 0.2, \n",
    "                             np.where(cevd_df_first['mobile phone'].isin([0]), 0.083, np.nan))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4.142857\n",
       "1    2.285714\n",
       "2    6.285714\n",
       "3    3.142857\n",
       "Name: full_screen_time, dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#screentime per day\n",
    "cevd_df_first['full_screen_time'] = (cevd_df_first['computer']*1) + (cevd_df_first['tv']*1) + (cevd_df_first['mobile phone']/7)\n",
    "cevd_df_first['full_screen_time'].head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of non-future CeVD px: 571\n"
     ]
    }
   ],
   "source": [
    "total_futurecevd = cevd_df_first[cevd_df_first['cevd_event']==0]\n",
    "print('number of non-future CeVD px:', len(total_futurecevd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#screen time per day filter; greater than 2 = 0\n",
    "def assign_value(x):\n",
    "    if x <= 2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "cevd_df_first['full_screen_time_filter'] = cevd_df_first['full_screen_time'].apply(assign_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of px in first part of longitudinal analysis: 807\n"
     ]
    }
   ],
   "source": [
    "cevd_df_first_onlycevd = cevd_df_first.dropna(subset=['cevd_day'])\n",
    "print('Number of px in first part of longitudinal analysis:', len(cevd_df_first_onlycevd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#to csv for dataframe for baseline cross-sectional analysis\n",
    "cevd_df_first_onlycevd.to_csv('cevd_first_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create second part of longitudinal analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cevd_df_second = cevd_df_first_onlycevd.dropna(subset=['date_i1'])\n",
    "len(cevd_df_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cevd_df_second = cevd_df_second.dropna(subset=['phq2_1_followup'])\n",
    "len(cevd_df_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cevd_df_second['cevd_day'] = pd.to_datetime(cevd_df_second['cevd_day'])\n",
    "cevd_df_second['cevd_before_i1'] = np.where(cevd_df_second['cevd_day'] < cevd_df_second['date_i1'], 1, 0)\n",
    "len(cevd_df_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cevd_df_second = cevd_df_second.drop(cevd_df_second.loc[cevd_df_second['phq2_1_followup'] < -1].index)\n",
    "cevd_df_second['phq2_1_followup'] = cevd_df_second['phq2_1_followup'].replace(-1, 0)\n",
    "len(cevd_df_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cevd_df_second['met_minweek_1'] = pd.to_numeric(cevd_df_second['met_minweek_1'], errors='coerce')\n",
    "#cevd_df_second['met_minweek_filter_1'] = cevd_df_second['met_minweek_1']/7\n",
    "#cevd_df_second['met_minweek_filter_1'] = cevd_df_second['met_minweek_filter_1']/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#convert sr_illness to string\n",
    "cevd_df_second['sr_illness_1'].dtype\n",
    "cevd_df_second['sr_illness_1'] = cevd_df_second['sr_illness_1'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after dropping px with missing sleep survey data 41\n"
     ]
    }
   ],
   "source": [
    "#drop -1 (\"do not know\") and -3 (\"prefer not to answer\") from sleep duration survey\n",
    "cevd_df_second = cevd_df_second[~(cevd_df_second['sleep_1'] == -3)]\n",
    "cevd_df_second = cevd_df_second[~(cevd_df_second['sleep_1'] == -1)]\n",
    "print('after dropping px with missing sleep survey data', len(cevd_df_second))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after dropping px with missing smoking survey data 41\n"
     ]
    }
   ],
   "source": [
    "#for smoking status, drop anyone who answered \"-3/prefer not to answer\"\n",
    "cevd_df_second['smok_stat_1'] = cevd_df_second['smok_stat_1'].astype(str)\n",
    "cevd_df_second = cevd_df_second[~cevd_df_second['smok_stat_1'].str.contains('-3')]\n",
    "print('after dropping px with missing smoking survey data', len(cevd_df_second))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after dropping px with incomplete screen surveys data 41\n",
      "after dropping px with missing screen survey data 38\n"
     ]
    }
   ],
   "source": [
    "#drop -1 and -3 from tv and computer and mobile phone use; fix tv use value of -10\n",
    "cevd_df_second = cevd_df_second[~(cevd_df_second['tv_1'] == -3)]\n",
    "cevd_df_second = cevd_df_second[~(cevd_df_second['tv_1'] == -1)]\n",
    "cevd_df_second = cevd_df_second[~(cevd_df_second['computer_1'] == -3)]\n",
    "cevd_df_second = cevd_df_second[~(cevd_df_second['computer_1'] == -1)]\n",
    "cevd_df_second = cevd_df_second[~(cevd_df_second['mobile phone_1'] == -3)]\n",
    "cevd_df_second = cevd_df_second[~(cevd_df_second['mobile phone_1'] == -1)]\n",
    "cevd_df_second['tv_1'] = cevd_df_second['tv_1'].replace(-10, 0.5)\n",
    "print('after dropping px with incomplete screen surveys data', len(cevd_df_second))\n",
    "cevd_df_second.dropna(subset=['computer_1', 'tv_1', 'mobile phone_1'], inplace=True)\n",
    "print('after dropping px with missing screen survey data', len(cevd_df_second))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after dropping px with incomplete alcohol survey data 38\n"
     ]
    }
   ],
   "source": [
    "#drop alc_freq values for \"prefer not to answer\"/-3\n",
    "cevd_df_second = cevd_df_second[~(cevd_df_second['alc_freq_1'] == -3)]\n",
    "print('after dropping px with incomplete alcohol survey data', len(cevd_df_second))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reorder alc_freq values \n",
    "#if 4\tOne to three times a month | 5\tSpecial occasions only | 6\tNever, then make seldom=1\n",
    "#if 2\tThree or four times a week | 3\tOnce or twice a week, then make sometimes=2\n",
    "#if 1\tDaily or almost daily, then make daily=3\n",
    "cevd_df_second['alc_freq_filter_1'] = np.where(cevd_df_second['alc_freq_1'].isin([4, 5, 6]), 1, \n",
    "                    np.where(cevd_df_second['alc_freq_1'].isin([2, 3]), 2, \n",
    "                             np.where(cevd_df_second['alc_freq_1'].isin([1]), 3, np.nan)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop phq2 scores where response was \"-3/prefer not to answer\"\n",
    "cevd_df_second = cevd_df_second.drop(cevd_df_second.loc[cevd_df_second['phq2_1_followup'] < -1].index)\n",
    "len(cevd_df_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace all occurrences of -1 (do not know if depressed) in column PHQ2_1 and PHQ2_2 with 0\n",
    "cevd_df_second['phq2_1'] = cevd_df_second['phq2_1'].replace(-1, 0)\n",
    "len(cevd_df_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create screen use covariate in multiple steps\n",
    "screen_column_1 = ['computer_1', 'tv_1', 'mobile phone_1']\n",
    "cevd_df_second['tv_1'] = cevd_df_second['tv_1'].replace(-10, 0.5)\n",
    "cevd_df_second['computer_1'] = cevd_df_second['computer_1'].replace(-10, 0.5)\n",
    "cevd_df_second[screen_column_1] = cevd_df_second[screen_column_1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reorder mobile phone use per week values \n",
    "#convert to hours or percentage of hours tied to \n",
    "cevd_df_second['mobile phone_filter_1'] = np.where(cevd_df_second['mobile phone_1'].isin([5]), 6, \n",
    "                    np.where(cevd_df_second['mobile phone_1'].isin([4]), 4,\n",
    "                    np.where(cevd_df_second['mobile phone_1'].isin([3]), 2,\n",
    "                    np.where(cevd_df_second['mobile phone_1'].isin([2]), 0.742, \n",
    "                    np.where(cevd_df_second['mobile phone_1'].isin([1]), 0.2, \n",
    "                             np.where(cevd_df_second['mobile phone_1'].isin([0]), 0.083, np.nan))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#screentime per day\n",
    "cevd_df_second['full_screen_time_1'] = (cevd_df_second['computer_1']*1) + (cevd_df_second['tv_1']*1) + (cevd_df_second['mobile phone_1']/7)\n",
    "cevd_df_second['full_screen_time_1'].head(4)\n",
    "#screen time per day filter; greater than 2 = 0\n",
    "def assign_value(x):\n",
    "    if x <= 2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "cevd_df_second['full_screen_time_filter_1'] = cevd_df_second['full_screen_time_1'].apply(assign_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cevd_df_second[\"date_i1\"] = pd.to_datetime(cevd_df_second[\"date_i1\"])\n",
    "#create hypertension category\n",
    "#prep htn cases\n",
    "cevd_df_second['i10_time'] = (cevd_df_second['date_i1'] - cevd_df_second['i10_date']).dt.days\n",
    "cevd_df_second['i15_time'] = (cevd_df_second['date_i1'] - cevd_df_second['i15_date']).dt.days\n",
    "\n",
    "# check for htn; if yes, return 1\n",
    "def check_htn_criteria(row):\n",
    "    if row['i10_time'] > 0 or row['i15_time'] > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "cevd_df_second['htn_yes_1'] = cevd_df_second.apply(check_htn_criteria, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create heart failure category\n",
    "#prep  cases\n",
    "cevd_df_second['i50_time'] = (cevd_df_second['date_i1'] - cevd_df_second['i50_date']).dt.days\n",
    "\n",
    "# check for htn; if yes, return 1\n",
    "def check_htn_criteria(row):\n",
    "    if row['i50_time'] > 0  or '1076' in row['sr_illness_1']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "cevd_df_second['hf_yes_1'] = cevd_df_second.apply(check_htn_criteria, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create cad category\n",
    "#prep  cases\n",
    "cevd_df_second['i25_time'] = (cevd_df_second['date_i1'] - cevd_df_second['i25_date']).dt.days\n",
    "\n",
    "# check for htn; if yes, return 1\n",
    "def check_htn_criteria(row):\n",
    "    if row['i25_time'] > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "cevd_df_second['cad_yes_1'] = cevd_df_second.apply(check_htn_criteria, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create cancer covariate\n",
    "cevd_df_second['cancer_yes_1'] = cevd_df_second['p134_i1'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create hyperlipidemia category\n",
    "#prep hchol cases\n",
    "cevd_df_second['e78_time'] = (cevd_df_second['date_i1'] - cevd_df_second['e78_date']).dt.days\n",
    "def check_hchol_criteria(row):\n",
    "    if row['e78_time'] > 0 or '1473' in row['sr_illness_1']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "cevd_df_second['hchol_yes_1'] = cevd_df_second.apply(check_hchol_criteria, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create diabetes category\n",
    "#prep diabetes cases\n",
    "cevd_df_second['e10_date'] = pd.to_datetime(cevd_df_second['e10_date'])\n",
    "cevd_df_second['e11_date'] = pd.to_datetime(cevd_df_second['e11_date'])\n",
    "cevd_df_second['e10_time'] = (cevd_df_second['date_i0'] - cevd_df_second['e10_date']).dt.days\n",
    "cevd_df_second['e11_time'] = (cevd_df_second['date_i0'] - cevd_df_second['e11_date']).dt.days\n",
    "\n",
    "def check_diab_criteria(row):\n",
    "    if row['e10_time'] > 0 or row['e11_time'] > 0 or '1222' in row['sr_illness_1'] or '1223' in row['sr_illness_1']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "cevd_df_second['diab_yes_1'] = cevd_df_second.apply(check_diab_criteria, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create comorbidity covariate for analysis and filter covariate for baseline characteristics table considering set of comorbidities created above\n",
    "cevd_df_second['comorb_sum_1'] = cevd_df_second[['htn_yes_1', 'hf_yes_1', 'cad_yes_1', 'cancer_yes_1', 'hchol_yes_1', 'diab_yes_1']].sum(axis=1)\n",
    "cevd_df_second['comorb_sum_filter_1'] = cevd_df_second['comorb_sum_1'].apply(lambda x: 0 if x == 0 else (1 if x == 1 else 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of px with depression diagnosis (F31, F32, F33): 7\n"
     ]
    }
   ],
   "source": [
    "#prep depression cases in \"days\" format\n",
    "cevd_df_second['f31_time'] = (cevd_df_second['date_i1'] - cevd_df_second['f31_date']).dt.days\n",
    "cevd_df_second['f32_time'] = (cevd_df_second['date_i1'] - cevd_df_second['f32_date']).dt.days\n",
    "cevd_df_second['f33_time'] = (cevd_df_second['date_i1'] - cevd_df_second['f33_date']).dt.days\n",
    "# check for diagnosed depression; if yes, return 1\n",
    "def check_for_dep_diag(row):\n",
    "    if row['f31_time'] > 0 or row['f32_time'] > 0 or row['f33_time'] > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "cevd_df_second['dep_diag_yes_1'] = cevd_df_second.apply(check_for_dep_diag, axis=1)\n",
    "check_dep_diag = cevd_df_second['dep_diag_yes_1'].sum()\n",
    "print(\"Number of px with depression diagnosis (F31, F32, F33):\", check_dep_diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#to csv for dataframe for follow-up cross-sectional analysis\n",
    "cevd_df_second.to_csv('cevd_second_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of participants in baseline dataset: 807\n",
      "Number of participants in baseline dataset w/o depression: 683\n",
      "Number of participants in baseline dataset w/depression: 124\n",
      "Number of participants in follow-up dataset: 38\n",
      "Number of participants in follow-up dataset w/depression: 7\n",
      "Number of participants in follow-up dataset w/o depression: 31\n",
      "Number of participants in follow-up dataset w/cevd: 38\n",
      "Number of participants in follow-up dataset w/cevd w/dep: 7\n",
      "Number of participants in follow-up dataset w/cevd w/o dep: 31\n"
     ]
    }
   ],
   "source": [
    "print('Number of participants in baseline dataset:', len(cevd_df_first_onlycevd))\n",
    "print('Number of participants in baseline dataset w/o depression:', len(cevd_df_first_onlycevd[(cevd_df_first_onlycevd['dep_diag_yes'] == 0)]))\n",
    "print('Number of participants in baseline dataset w/depression:', len(cevd_df_first_onlycevd[(cevd_df_first_onlycevd['dep_diag_yes'] == 1)]))\n",
    "print('Number of participants in follow-up dataset:', len(cevd_df_second))\n",
    "print('Number of participants in follow-up dataset w/depression:', len(cevd_df_second[(cevd_df_second['dep_diag_yes_1'] == 1)]))\n",
    "print('Number of participants in follow-up dataset w/o depression:', len(cevd_df_second[(cevd_df_second['dep_diag_yes_1'] == 0)]))\n",
    "print('Number of participants in follow-up dataset w/cevd:', len(cevd_df_second['cevd_before_i1'] == 1))\n",
    "print('Number of participants in follow-up dataset w/cevd w/dep:', len(cevd_df_second[(cevd_df_second['cevd_before_i1'] == 1) & (cevd_df_second['dep_diag_yes_1'] == 1)]))\n",
    "print('Number of participants in follow-up dataset w/cevd w/o dep:', len(cevd_df_second[(cevd_df_second['cevd_before_i1'] == 1) & (cevd_df_second['dep_diag_yes_1'] == 0)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of px with F31: 1\n"
     ]
    }
   ],
   "source": [
    "def count_f31(df):\n",
    "    new_df = pd.DataFrame(columns=df.columns) \n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        value_1 = row['f31_time']\n",
    "        value_2 = row['f32_time']\n",
    "        value_3 = row['f33_time']\n",
    "        \n",
    "        if value_1 > 0 and (pd.isna(value_2) or value_2 < 0) and (pd.isna(value_3) or value_3 < 0):\n",
    "            new_df = pd.concat([new_df, pd.DataFrame([row], columns=df.columns)], ignore_index=True)\n",
    "    \n",
    "    new_df.reset_index(drop=True, inplace=True) \n",
    "    return new_df\n",
    "\n",
    "count_f31 = count_f31(cevd_df_first_onlycevd)\n",
    "print('Number of px with F31:', len(count_f31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of px with F32: 111\n"
     ]
    }
   ],
   "source": [
    "def count_f32(df):\n",
    "    new_df = pd.DataFrame(columns=df.columns) \n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        value_1 = row['f31_time']\n",
    "        value_2 = row['f32_time']\n",
    "        value_3 = row['f33_time']\n",
    "        \n",
    "        if value_2 > 0 and (pd.isna(value_1) or value_1 < 0) and (pd.isna(value_3) or value_3 < 0):\n",
    "            new_df = pd.concat([new_df, pd.DataFrame([row], columns=df.columns)], ignore_index=True)\n",
    "    \n",
    "    new_df.reset_index(drop=True, inplace=True)  # Reset the index of the new DataFrame\n",
    "    return new_df\n",
    "\n",
    "count_f32 = count_f32(cevd_df_first_onlycevd)\n",
    "print('Number of px with F32:', len(count_f32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of px with F33: 1\n"
     ]
    }
   ],
   "source": [
    "def count_f33(df):\n",
    "    new_df = pd.DataFrame(columns=df.columns) \n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        value_1 = row['f31_time']\n",
    "        value_2 = row['f32_time']\n",
    "        value_3 = row['f33_time']\n",
    "        \n",
    "        if value_3 > 0 and (pd.isna(value_1) or value_1 < 0) and (pd.isna(value_2) or value_2 < 0):\n",
    "            new_df = pd.concat([new_df, pd.DataFrame([row], columns=df.columns)], ignore_index=True)\n",
    "    \n",
    "    new_df.reset_index(drop=True, inplace=True)  # Reset the index of the new DataFrame\n",
    "    return new_df\n",
    "\n",
    "count_f33 = count_f33(cevd_df_first_onlycevd)\n",
    "print('Number of px with F33:', len(count_f33))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get baseline characteristics table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cevd_df_first_onlycevd[cevd_df_first_onlycevd['cevd_event'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Age with exp cohort: 58.58064516129032\n",
      "Standard Deviation of Age with exp cohort: 6.868209757385094\n",
      "P-value for tdi:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStandard Deviation of Age with exp cohort:\u001b[39m\u001b[38;5;124m\"\u001b[39m, std_dev_age)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP-value for tdi:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mstats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmannwhitneyu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcevd_df_first_onlycevd\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mage_i0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcevd_df_first_onlycevd\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdep_diag_yes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malternative\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtwo-sided\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m subset \u001b[38;5;241m=\u001b[39m cevd_df_first_onlycevd[cevd_df_first_onlycevd[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdep_diag_yes\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     10\u001b[0m mean_age \u001b[38;5;241m=\u001b[39m subset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage_i0\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/scipy/stats/_axis_nan_policy.py:503\u001b[0m, in \u001b[0;36m_axis_nan_policy_factory.<locals>.axis_nan_policy_decorator.<locals>.axis_nan_policy_wrapper\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sentinel:\n\u001b[1;32m    502\u001b[0m     samples \u001b[38;5;241m=\u001b[39m _remove_sentinel(samples, paired, sentinel)\n\u001b[0;32m--> 503\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mhypotest_fun_out\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m res \u001b[38;5;241m=\u001b[39m result_to_tuple(res)\n\u001b[1;32m    505\u001b[0m res \u001b[38;5;241m=\u001b[39m _add_reduced_axes(res, reduced_axes, keepdims)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/scipy/stats/_mannwhitneyu.py:460\u001b[0m, in \u001b[0;36mmannwhitneyu\u001b[0;34m(x, y, use_continuity, alternative, axis, method)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;129m@_axis_nan_policy_factory\u001b[39m(MannwhitneyuResult, n_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmannwhitneyu\u001b[39m(x, y, use_continuity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, alternative\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtwo-sided\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    251\u001b[0m                  axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    252\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m'''Perform the Mann-Whitney U rank test on two independent samples.\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m    The Mann-Whitney U test is a nonparametric test of the null hypothesis\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    456\u001b[0m \n\u001b[1;32m    457\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m    459\u001b[0m     x, y, use_continuity, alternative, axis_int, method \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 460\u001b[0m         \u001b[43m_mwu_input_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_continuity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malternative\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    462\u001b[0m     x, y, xy \u001b[38;5;241m=\u001b[39m _broadcast_concatenate(x, y, axis)\n\u001b[1;32m    464\u001b[0m     n1, n2 \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/scipy/stats/_mannwhitneyu.py:200\u001b[0m, in \u001b[0;36m_mwu_input_validation\u001b[0;34m(x, y, use_continuity, alternative, axis, method)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# Would use np.asarray_chkfinite, but infs are OK\u001b[39;00m\n\u001b[1;32m    199\u001b[0m x, y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_1d(x), np\u001b[38;5;241m.\u001b[39matleast_1d(y)\n\u001b[0;32m--> 200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39many() \u001b[38;5;129;01mor\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(y)\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`x` and `y` must not contain NaNs.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39msize(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m np\u001b[38;5;241m.\u001b[39msize(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "subset = cevd_df_first_onlycevd[cevd_df_first_onlycevd['dep_diag_yes'] == 1]\n",
    "mean_age = subset['age_i0'].mean()\n",
    "std_dev_age = subset['age_i0'].std()\n",
    "\n",
    "print(\"Mean Age with exp cohort:\", mean_age)\n",
    "print(\"Standard Deviation of Age with exp cohort:\", std_dev_age)\n",
    "print('P-value for tdi:')\n",
    "stats.mannwhitneyu(x=cevd_df_first_onlycevd['age_i0'], y=cevd_df_first_onlycevd['dep_diag_yes'], alternative = 'two-sided')\n",
    "subset = cevd_df_first_onlycevd[cevd_df_first_onlycevd['dep_diag_yes'] == 0]\n",
    "mean_age = subset['age_i0'].mean()\n",
    "std_dev_age = subset['age_i0'].std()\n",
    "\n",
    "print(\"Mean Age with control cohort:\", mean_age)\n",
    "print(\"Standard Deviation of Age with control cohort:\", std_dev_age)\n",
    "print('P-value for tdi:')\n",
    "stats.mannwhitneyu(x=cevd_df_first_onlycevd['age_i0'], y=cevd_df_first_onlycevd['dep_diag_yes'], alternative = 'two-sided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Age with exp cohort: 63.0\n",
      "Standard Deviation of Age with exp cohort: 6.0553007081949835\n",
      "P-value for tdi:\n",
      "Mean Age with control cohort: 63.84375\n",
      "Standard Deviation of Age with control cohort: 7.2071173379223294\n",
      "P-value for tdi:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MannwhitneyuResult(statistic=1444.0, pvalue=7.891278357487499e-15)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = cevd_df_second[cevd_df_second['dep_diag_yes_1'] == 1]\n",
    "mean_age = subset['age_i1'].mean()\n",
    "std_dev_age = subset['age_i1'].std()\n",
    "\n",
    "print(\"Mean Age with exp cohort:\", mean_age)\n",
    "print(\"Standard Deviation of Age with exp cohort:\", std_dev_age)\n",
    "print('P-value for tdi:')\n",
    "stats.mannwhitneyu(x=cevd_df_second['age_i1'], y=cevd_df_second['dep_diag_yes_1'], alternative = 'two-sided')\n",
    "subset = cevd_df_second[cevd_df_second['dep_diag_yes'] == 0]\n",
    "mean_age = subset['age_i1'].mean()\n",
    "std_dev_age = subset['age_i1'].std()\n",
    "\n",
    "print(\"Mean Age with control cohort:\", mean_age)\n",
    "print(\"Standard Deviation of Age with control cohort:\", std_dev_age)\n",
    "print('P-value for tdi:')\n",
    "stats.mannwhitneyu(x=cevd_df_second['age_i1'], y=cevd_df_second['dep_diag_yes_1'], alternative = 'two-sided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp Cohort Percent male: 51.61290322580645\n",
      "exp Cohort Total male: 64\n",
      "Control Cohort Percent male: 63.250366032210835\n",
      "Control Cohort Total male: 432\n",
      "% of men in total 18,873 cohort 2.628093042971441\n",
      "MALE - Chi-square test statistic: 5.519280324098183\n",
      "MALE - P-value: 0.018807994786910268\n"
     ]
    }
   ],
   "source": [
    "#get male/female data\n",
    "percent_male_d = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 1, 'male'].mean() * 100\n",
    "male_sum_d = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 1, 'male'].sum()\n",
    "print('exp Cohort Percent male:', percent_male_d)\n",
    "print('exp Cohort Total male:', male_sum_d)\n",
    "percent_male_c = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 0, 'male'].mean() * 100\n",
    "male_sum_c = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 0, 'male'].sum()\n",
    "print('Control Cohort Percent male:', percent_male_c)\n",
    "print('Control Cohort Total male:', male_sum_c)\n",
    "perc_men_total = ((male_sum_d+male_sum_c)/18873)*100\n",
    "print('% of men in total 18,873 cohort', perc_men_total)\n",
    "data = pd.crosstab(cevd_df_first_onlycevd['male'], cevd_df_first_onlycevd['dep_diag_yes']).to_numpy()\n",
    "# chi-square test for independence for male data\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(data)\n",
    "print('MALE - Chi-square test statistic:', chi2_stat)\n",
    "print('MALE - P-value:', p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp Cohort Percent male: 28.57142857142857\n",
      "exp Cohort Total male: 2\n",
      "Control Cohort Percent male: 70.96774193548387\n",
      "Control Cohort Total male: 22\n",
      "% of men in total 18,873 cohort 0.12716579240184392\n",
      "MALE - Chi-square test statistic: 2.7773480359885894\n",
      "MALE - P-value: 0.09560635798394375\n"
     ]
    }
   ],
   "source": [
    "#get male/female data\n",
    "percent_male_d = cevd_df_second.loc[cevd_df_second['dep_diag_yes_1'] == 1, 'male'].mean() * 100\n",
    "male_sum_d = cevd_df_second.loc[cevd_df_second['dep_diag_yes_1'] == 1, 'male'].sum()\n",
    "print('exp Cohort Percent male:', percent_male_d)\n",
    "print('exp Cohort Total male:', male_sum_d)\n",
    "percent_male_c = cevd_df_second.loc[cevd_df_second['dep_diag_yes_1'] == 0, 'male'].mean() * 100\n",
    "male_sum_c = cevd_df_second.loc[cevd_df_second['dep_diag_yes_1'] == 0, 'male'].sum()\n",
    "print('Control Cohort Percent male:', percent_male_c)\n",
    "print('Control Cohort Total male:', male_sum_c)\n",
    "perc_men_total = ((male_sum_d+male_sum_c)/18873)*100\n",
    "print('% of men in total 18,873 cohort', perc_men_total)\n",
    "data = pd.crosstab(cevd_df_second['male'], cevd_df_second['dep_diag_yes_1']).to_numpy()\n",
    "# chi-square test for independence for male data\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(data)\n",
    "print('MALE - Chi-square test statistic:', chi2_stat)\n",
    "print('MALE - P-value:', p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp Cohort Percent white: 95.96774193548387\n",
      "exp Cohort Total white: 119.0\n",
      "Control Cohort Percent white: 95.02196193265007\n",
      "Control Cohort Total white: 649.0\n",
      "% of white in total 76505 cohort 1.0038559571269852\n",
      "white - Chi-square test statistic: 0.05026660856255372\n",
      "white - P-value: 0.8226000027927111\n"
     ]
    }
   ],
   "source": [
    "#get white/ethnicity data\n",
    "percent_male_d = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 1, 'white_yes'].mean() * 100\n",
    "male_sum_d = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 1, 'white_yes'].sum()\n",
    "print('exp Cohort Percent white:', percent_male_d)\n",
    "print('exp Cohort Total white:', male_sum_d)\n",
    "percent_male_c = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 0, 'white_yes'].mean() * 100\n",
    "male_sum_c = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 0, 'white_yes'].sum()\n",
    "print('Control Cohort Percent white:', percent_male_c)\n",
    "print('Control Cohort Total white:', male_sum_c)\n",
    "perc_men_total = ((male_sum_d+male_sum_c)/76505)*100\n",
    "print('% of white in total 76505 cohort', perc_men_total)\n",
    "data = pd.crosstab(cevd_df_first_onlycevd['white_yes'], cevd_df_first_onlycevd['dep_diag_yes']).to_numpy()\n",
    "# chi-square test for independence for male data\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(data)\n",
    "print('white - Chi-square test statistic:', chi2_stat)\n",
    "print('white - P-value:', p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp Cohort Percent white: 100.0\n",
      "exp Cohort Total white: 7.0\n",
      "Control Cohort Percent white: 96.7741935483871\n",
      "Control Cohort Total white: 30.0\n",
      "% of men in total 18,873 cohort 0.19604726328617603\n",
      "white - Chi-square test statistic: 0.0\n",
      "white - P-value: 1.0\n"
     ]
    }
   ],
   "source": [
    "#get male/female data\n",
    "percent_male_d = cevd_df_second.loc[cevd_df_second['dep_diag_yes_1'] == 1, 'white_yes'].mean() * 100\n",
    "male_sum_d = cevd_df_second.loc[cevd_df_second['dep_diag_yes_1'] == 1, 'white_yes'].sum()\n",
    "print('exp Cohort Percent white:', percent_male_d)\n",
    "print('exp Cohort Total white:', male_sum_d)\n",
    "percent_male_c = cevd_df_second.loc[cevd_df_second['dep_diag_yes_1'] == 0, 'white_yes'].mean() * 100\n",
    "male_sum_c = cevd_df_second.loc[cevd_df_second['dep_diag_yes_1'] == 0, 'white_yes'].sum()\n",
    "print('Control Cohort Percent white:', percent_male_c)\n",
    "print('Control Cohort Total white:', male_sum_c)\n",
    "perc_men_total = ((male_sum_d+male_sum_c)/18873)*100\n",
    "print('% of men in total 18,873 cohort', perc_men_total)\n",
    "data = pd.crosstab(cevd_df_second['white_yes'], cevd_df_second['dep_diag_yes_1']).to_numpy()\n",
    "# chi-square test for independence for male data\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(data)\n",
    "print('white - Chi-square test statistic:', chi2_stat)\n",
    "print('white - P-value:', p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp Percentages of smoke\n",
      "1    50.0\n",
      "0    50.0\n",
      "Name: smok_filter, dtype: float64\n",
      "CONTROL Percentages of smoke\n",
      "1    59.590044\n",
      "0    40.409956\n",
      "Name: smok_filter, dtype: float64\n",
      "smoke - Chi-square test statistic: 3.5810255519310594\n",
      "smoke - P-value: 0.058443060466698796\n"
     ]
    }
   ],
   "source": [
    "#get smoking data\n",
    "cevd_df_first_onlycevd['smok_stat'] = pd.to_numeric(cevd_df_first_onlycevd['smok_stat'], errors='coerce').astype(pd.Int64Dtype())\n",
    "cevd_df_first_onlycevd['smok_filter'] = cevd_df_first_onlycevd['smok_stat'].apply(lambda x: 1 if (x > 0) else 0)\n",
    "perc_alc_freq_d = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 1, 'smok_filter'].value_counts(normalize=True)\n",
    "print('exp Percentages of smoke')\n",
    "print(perc_alc_freq_d * 100)\n",
    "perc_alc_freq_c = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 0, 'smok_filter'].value_counts(normalize=True)\n",
    "# print the value counts as percentages\n",
    "print('CONTROL Percentages of smoke')\n",
    "print(perc_alc_freq_c * 100)\n",
    "\n",
    "data = pd.crosstab(cevd_df_first_onlycevd['smok_filter'], cevd_df_first_onlycevd['dep_diag_yes']).to_numpy()\n",
    "# chi-square test for independence\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(data)\n",
    "print('smoke - Chi-square test statistic:', chi2_stat)\n",
    "print('smoke - P-value:', p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp Percentages of smoke\n",
      "0    71.428571\n",
      "1    28.571429\n",
      "Name: smok_filter_1, dtype: float64\n",
      "CONTROL Percentages of smoke\n",
      "1    54.83871\n",
      "0    45.16129\n",
      "Name: smok_filter_1, dtype: float64\n",
      "smoke - Chi-square test statistic: 0.7004608294930875\n",
      "smoke - P-value: 0.4026288921800596\n"
     ]
    }
   ],
   "source": [
    "#get smoking data\n",
    "cevd_df_second['smok_stat_1'] = pd.to_numeric(cevd_df_second['smok_stat_1'], errors='coerce').astype(pd.Int64Dtype())\n",
    "cevd_df_second['smok_filter_1'] = cevd_df_second['smok_stat_1'].apply(lambda x: 1 if (x > 0) else 0)\n",
    "perc_alc_freq_d = cevd_df_second.loc[cevd_df_second['dep_diag_yes_1'] == 1, 'smok_filter_1'].value_counts(normalize=True)\n",
    "print('exp Percentages of smoke')\n",
    "print(perc_alc_freq_d * 100)\n",
    "perc_alc_freq_c = cevd_df_second.loc[cevd_df_second['dep_diag_yes_1'] == 0, 'smok_filter_1'].value_counts(normalize=True)\n",
    "# print the value counts as percentages\n",
    "print('CONTROL Percentages of smoke')\n",
    "print(perc_alc_freq_c * 100)\n",
    "\n",
    "data = pd.crosstab(cevd_df_second['smok_filter_1'], cevd_df_second['dep_diag_yes_1']).to_numpy()\n",
    "# chi-square test for independence\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(data)\n",
    "print('smoke - Chi-square test statistic:', chi2_stat)\n",
    "print('smoke - P-value:', p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp Cohort Percent bmi: 26.61290322580645\n",
      "exp Cohort Total bmi: 33\n",
      "Control Cohort Percent bmi: 34.11420204978038\n",
      "Control Cohort Total bmi: 233\n",
      "bmi - Chi-square test statistic: 2.3437602363968235\n",
      "bmi - P-value: 0.12578559829450872\n"
     ]
    }
   ],
   "source": [
    "#get bmi data\n",
    "cevd_df_first_onlycevd['bmi_filter'] = cevd_df_first_onlycevd['bmi'].apply(lambda x: 1 if (x > 18.4 and x < 26) else 0)\n",
    "percent_male_d = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 1, 'bmi_filter'].mean() * 100\n",
    "male_sum_d = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 1, 'bmi_filter'].sum()\n",
    "print('exp Cohort Percent bmi:', percent_male_d)\n",
    "print('exp Cohort Total bmi:', male_sum_d)\n",
    "percent_male_c = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 0, 'bmi_filter'].mean() * 100\n",
    "male_sum_c = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 0, 'bmi_filter'].sum()\n",
    "print('Control Cohort Percent bmi:', percent_male_c)\n",
    "print('Control Cohort Total bmi:', male_sum_c)\n",
    "data = pd.crosstab(cevd_df_first_onlycevd['bmi_filter'], cevd_df_first_onlycevd['dep_diag_yes']).to_numpy()\n",
    "# chi-square test for independence for bmi data\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(data)\n",
    "print('bmi - Chi-square test statistic:', chi2_stat)\n",
    "print('bmi - P-value:', p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp Cohort Percent bmi: 16.666666666666664\n",
      "exp Cohort Total bmi: 1\n",
      "Control Cohort Percent bmi: 43.75\n",
      "Control Cohort Total bmi: 14\n",
      "bmi - Chi-square test statistic: 0.6247282608695649\n",
      "bmi - P-value: 0.42929564225055517\n"
     ]
    }
   ],
   "source": [
    "#get bmi data\n",
    "cevd_df_second['bmi_filter_1'] = cevd_df_second['bmi_1'].apply(lambda x: 1 if (x > 18.4 and x < 26) else 0)\n",
    "percent_male_d = cevd_df_second.loc[cevd_df_second['dep_diag_yes'] == 1, 'bmi_filter_1'].mean() * 100\n",
    "male_sum_d = cevd_df_second.loc[cevd_df_second['dep_diag_yes'] == 1, 'bmi_filter_1'].sum()\n",
    "print('exp Cohort Percent bmi:', percent_male_d)\n",
    "print('exp Cohort Total bmi:', male_sum_d)\n",
    "percent_male_c = cevd_df_second.loc[cevd_df_second['dep_diag_yes'] == 0, 'bmi_filter_1'].mean() * 100\n",
    "male_sum_c = cevd_df_second.loc[cevd_df_second['dep_diag_yes'] == 0, 'bmi_filter_1'].sum()\n",
    "print('Control Cohort Percent bmi:', percent_male_c)\n",
    "print('Control Cohort Total bmi:', male_sum_c)\n",
    "data = pd.crosstab(cevd_df_second['bmi_filter_1'], cevd_df_second['dep_diag_yes']).to_numpy()\n",
    "# chi-square test for independence for bmi data\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(data)\n",
    "print('bmi - Chi-square test statistic:', chi2_stat)\n",
    "print('bmi - P-value:', p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp Cohort Percent sleep: 25.806451612903224\n",
      "exp Cohort Total sleep: 32\n",
      "Control Cohort Percent sleep: 28.989751098096633\n",
      "Control Cohort Total sleep: 198\n",
      "sleep - Chi-square test statistic: 0.37735159097711346\n",
      "sleep - P-value: 0.5390240391687992\n"
     ]
    }
   ],
   "source": [
    "#get bmi data\n",
    "cevd_df_first_onlycevd['sleep_filter'] = cevd_df_first_onlycevd['sleep'].apply(lambda x: 1 if (x > 7 and x < 9) else 0)\n",
    "percent_male_d = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 1, 'sleep_filter'].mean() * 100\n",
    "male_sum_d = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 1, 'sleep_filter'].sum()\n",
    "print('exp Cohort Percent sleep:', percent_male_d)\n",
    "print('exp Cohort Total sleep:', male_sum_d)\n",
    "percent_male_c = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 0, 'sleep_filter'].mean() * 100\n",
    "male_sum_c = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 0, 'sleep_filter'].sum()\n",
    "print('Control Cohort Percent sleep:', percent_male_c)\n",
    "print('Control Cohort Total sleep:', male_sum_c)\n",
    "data = pd.crosstab(cevd_df_first_onlycevd['sleep_filter'], cevd_df_first_onlycevd['dep_diag_yes']).to_numpy()\n",
    "# chi-square test for independence for bmi data\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(data)\n",
    "print('sleep - Chi-square test statistic:', chi2_stat)\n",
    "print('sleep - P-value:', p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp Cohort Percent sleep: 33.33333333333333\n",
      "exp Cohort Total sleep: 2\n",
      "Control Cohort Percent sleep: 21.875\n",
      "Control Cohort Total sleep: 7\n",
      "sleep - Chi-square test statistic: 0.006824712643678132\n",
      "sleep - P-value: 0.9341602031737535\n"
     ]
    }
   ],
   "source": [
    "#get bmi data\n",
    "cevd_df_second['sleep_filter_1'] = cevd_df_second['sleep_1'].apply(lambda x: 1 if (x > 7 and x < 9) else 0)\n",
    "percent_male_d = cevd_df_second.loc[cevd_df_second['dep_diag_yes'] == 1, 'sleep_filter_1'].mean() * 100\n",
    "male_sum_d = cevd_df_second.loc[cevd_df_second['dep_diag_yes'] == 1, 'sleep_filter_1'].sum()\n",
    "print('exp Cohort Percent sleep:', percent_male_d)\n",
    "print('exp Cohort Total sleep:', male_sum_d)\n",
    "percent_male_c = cevd_df_second.loc[cevd_df_second['dep_diag_yes'] == 0, 'sleep_filter_1'].mean() * 100\n",
    "male_sum_c = cevd_df_second.loc[cevd_df_second['dep_diag_yes'] == 0, 'sleep_filter_1'].sum()\n",
    "print('Control Cohort Percent sleep:', percent_male_c)\n",
    "print('Control Cohort Total sleep:', male_sum_c)\n",
    "data = pd.crosstab(cevd_df_second['sleep_filter_1'], cevd_df_second['dep_diag_yes']).to_numpy()\n",
    "# chi-square test for independence for bmi data\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(data)\n",
    "print('sleep - Chi-square test statistic:', chi2_stat)\n",
    "print('sleep - P-value:', p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean full_screen_time with exp cohort: 5.370967741935483\n",
      "Standard Deviation of full_screen_time with exp cohort: 3.895725614950078\n",
      "P-value for full_screen_time:\n",
      "Mean full_screen_time with control cohort: 4.58690650491529\n",
      "Standard Deviation of full_screen_time with control cohort: 2.3801575561155857\n",
      "P-value for full_screen_time:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MannwhitneyuResult(statistic=647093.0, pvalue=4.704306609094037e-280)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = cevd_df_first_onlycevd[cevd_df_first_onlycevd['dep_diag_yes'] == 1]\n",
    "mean_age = subset['full_screen_time'].mean()\n",
    "std_dev_age = subset['full_screen_time'].std()\n",
    "\n",
    "print(\"Mean full_screen_time with exp cohort:\", mean_age)\n",
    "print(\"Standard Deviation of full_screen_time with exp cohort:\", std_dev_age)\n",
    "print('P-value for full_screen_time:')\n",
    "stats.mannwhitneyu(x=cevd_df_first_onlycevd['full_screen_time'], y=cevd_df_first_onlycevd['dep_diag_yes'], alternative = 'two-sided')\n",
    "subset = cevd_df_first_onlycevd[cevd_df_first_onlycevd['dep_diag_yes'] == 0]\n",
    "mean_age = subset['full_screen_time'].mean()\n",
    "std_dev_age = subset['full_screen_time'].std()\n",
    "\n",
    "print(\"Mean full_screen_time with control cohort:\", mean_age)\n",
    "print(\"Standard Deviation of full_screen_time with control cohort:\", std_dev_age)\n",
    "print('P-value for full_screen_time:')\n",
    "stats.mannwhitneyu(x=cevd_df_first_onlycevd['full_screen_time'], y=cevd_df_first_onlycevd['dep_diag_yes'], alternative = 'two-sided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean full_screen_time_1 with exp cohort: 5.163265306122448\n",
      "Standard Deviation of full_screen_time_1 with exp cohort: 2.167208573929942\n",
      "P-value for full_screen_time_1:\n",
      "Mean full_screen_time_1 with control cohort: 4.423963133640552\n",
      "Standard Deviation of full_screen_time_1 with control cohort: 2.263127650668742\n",
      "P-value for full_screen_time_1:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MannwhitneyuResult(statistic=1444.0, pvalue=7.962664950610099e-15)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = cevd_df_second[cevd_df_second['dep_diag_yes_1'] == 1]\n",
    "mean_age = subset['full_screen_time_1'].mean()\n",
    "std_dev_age = subset['full_screen_time_1'].std()\n",
    "\n",
    "print(\"Mean full_screen_time_1 with exp cohort:\", mean_age)\n",
    "print(\"Standard Deviation of full_screen_time_1 with exp cohort:\", std_dev_age)\n",
    "print('P-value for full_screen_time_1:')\n",
    "stats.mannwhitneyu(x=cevd_df_second['full_screen_time_1'], y=cevd_df_second['dep_diag_yes_1'], alternative = 'two-sided')\n",
    "subset = cevd_df_second[cevd_df_second['dep_diag_yes_1'] == 0]\n",
    "mean_age = subset['full_screen_time_1'].mean()\n",
    "std_dev_age = subset['full_screen_time_1'].std()\n",
    "\n",
    "print(\"Mean full_screen_time_1 with control cohort:\", mean_age)\n",
    "print(\"Standard Deviation of full_screen_time_1 with control cohort:\", std_dev_age)\n",
    "print('P-value for full_screen_time_1:')\n",
    "stats.mannwhitneyu(x=cevd_df_second['full_screen_time_1'], y=cevd_df_second['dep_diag_yes_1'], alternative = 'two-sided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean met with exp cohort: 4.682244623655914\n",
      "Standard Deviation of met with exp cohort: 5.584099084141212\n",
      "P-value for met:\n",
      "Mean met with control cohort: 6.357694694275953\n",
      "Standard Deviation of met with control cohort: 6.6993575173665025\n",
      "P-value for met:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MannwhitneyuResult(statistic=620725.0, pvalue=1.5956476465246654e-239)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cevd_df_first_onlycevd['met_minweek_filter'] = pd.to_numeric(cevd_df_first_onlycevd['met_minweek_filter'], errors='coerce')\n",
    "subset = cevd_df_first_onlycevd[cevd_df_first_onlycevd['dep_diag_yes'] == 1]\n",
    "mean_age = subset['met_minweek_filter'].mean()\n",
    "std_dev_age = subset['met_minweek_filter'].std()\n",
    "\n",
    "print(\"Mean met with exp cohort:\", mean_age)\n",
    "print(\"Standard Deviation of met with exp cohort:\", std_dev_age)\n",
    "print('P-value for met:')\n",
    "stats.mannwhitneyu(x=cevd_df_first_onlycevd['met_minweek_filter'], y=cevd_df_first_onlycevd['dep_diag_yes'], alternative = 'two-sided')\n",
    "subset = cevd_df_first_onlycevd[cevd_df_first_onlycevd['dep_diag_yes'] == 0]\n",
    "mean_age = subset['met_minweek_filter'].mean()\n",
    "std_dev_age = subset['met_minweek_filter'].std()\n",
    "\n",
    "print(\"Mean met with control cohort:\", mean_age)\n",
    "print(\"Standard Deviation of met with control cohort:\", std_dev_age)\n",
    "\n",
    "print('P-value for met:')\n",
    "stats.mannwhitneyu(x=cevd_df_first_onlycevd['met_minweek_filter'], y=cevd_df_first_onlycevd['dep_diag_yes'], alternative = 'two-sided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean met with exp cohort: 3.2792517006802724\n",
      "Standard Deviation of met with exp cohort: 3.576855589420598\n",
      "P-value for met:\n",
      "Mean met with control cohort: 6.133717357910906\n",
      "Standard Deviation of met with control cohort: 6.989935125863806\n",
      "P-value for met:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MannwhitneyuResult(statistic=1372.5, pvalue=2.1789566443599836e-12)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cevd_df_second['met_minweek_filter'] = pd.to_numeric(cevd_df_second['met_minweek_filter'], errors='coerce')\n",
    "subset = cevd_df_second[cevd_df_second['dep_diag_yes_1'] == 1]\n",
    "mean_age = subset['met_minweek_filter'].mean()\n",
    "std_dev_age = subset['met_minweek_filter'].std()\n",
    "\n",
    "print(\"Mean met with exp cohort:\", mean_age)\n",
    "print(\"Standard Deviation of met with exp cohort:\", std_dev_age)\n",
    "print('P-value for met:')\n",
    "stats.mannwhitneyu(x=cevd_df_second['met_minweek_filter'], y=cevd_df_second['dep_diag_yes_1'], alternative = 'two-sided')\n",
    "subset = cevd_df_second[cevd_df_second['dep_diag_yes_1'] == 0]\n",
    "mean_age = subset['met_minweek_filter'].mean()\n",
    "std_dev_age = subset['met_minweek_filter'].std()\n",
    "\n",
    "print(\"Mean met with control cohort:\", mean_age)\n",
    "print(\"Standard Deviation of met with control cohort:\", std_dev_age)\n",
    "\n",
    "print('P-value for met:')\n",
    "stats.mannwhitneyu(x=cevd_df_second['met_minweek_filter'], y=cevd_df_second['dep_diag_yes_1'], alternative = 'two-sided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset = cevd_df_first_onlycevd[cevd_df_first_onlycevd['dep_diag_yes_1'] == 1]\n",
    "#mean_age = subset['met_minweek_filter_1'].mean()\n",
    "#std_dev_age = subset['met_minweek_filter_1'].std()\n",
    "\n",
    "#print(\"Mean met with exp cohort:\", mean_age)\n",
    "#print(\"Standard Deviation of met with exp cohort:\", std_dev_age)\n",
    "#print('P-value for met:')\n",
    "#stats.mannwhitneyu(x=cevd_df_first_onlycevd['met_minweek_filter_1'], y=cevd_df_first_onlycevd['dep_diag_yes_1'], alternative = 'two-sided')\n",
    "#subset = cevd_df_first_onlycevd[cevd_df_first_onlycevd['dep_diag_yes_1'] == 0]\n",
    "#mean_age = subset['met_minweek_filter_1'].mean()\n",
    "#std_dev_age = subset['met_minweek_filter_1'].std()\n",
    "\n",
    "#print(\"Mean met with control cohort:\", mean_age)\n",
    "#print(\"Standard Deviation of met with control cohort:\", std_dev_age)\n",
    "#print('P-value for met:')\n",
    "#stats.mannwhitneyu(x=cevd_df_first_onlycevd['met_minweek_filter_1'], y=cevd_df_first_onlycevd['dep_diag_yes_1'], alternative = 'two-sided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean tdi with exp cohort: -0.6976612903225807\n",
      "Standard Deviation of tdi with exp cohort: 3.317944830855871\n",
      "P-value for tdi:\n",
      "Mean tdi with control cohort: -1.0294845112631257\n",
      "Standard Deviation of tdi with control cohort: 3.1213533207689514\n",
      "P-value for tdi:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MannwhitneyuResult(statistic=207656.0, pvalue=2.870147338473737e-39)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = cevd_df_first_onlycevd[cevd_df_first_onlycevd['dep_diag_yes'] == 1]\n",
    "mean_age = subset['tdi'].mean()\n",
    "std_dev_age = subset['tdi'].std()\n",
    "\n",
    "print(\"Mean tdi with exp cohort:\", mean_age)\n",
    "print(\"Standard Deviation of tdi with exp cohort:\", std_dev_age)\n",
    "print('P-value for tdi:')\n",
    "stats.mannwhitneyu(x=cevd_df_first_onlycevd['tdi'], y=cevd_df_first_onlycevd['dep_diag_yes'], alternative = 'two-sided')\n",
    "subset = cevd_df_first_onlycevd[cevd_df_first_onlycevd['dep_diag_yes'] == 0]\n",
    "mean_age = subset['tdi'].mean()\n",
    "std_dev_age = subset['tdi'].std()\n",
    "\n",
    "print(\"Mean tdi with control cohort:\", mean_age)\n",
    "print(\"Standard Deviation of tdi with control cohort:\", std_dev_age)\n",
    "print('P-value for tdi:')\n",
    "stats.mannwhitneyu(x=cevd_df_first_onlycevd['tdi'], y=cevd_df_first_onlycevd['dep_diag_yes'], alternative = 'two-sided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean tdi with exp cohort: 0.3057142857142857\n",
      "Standard Deviation of tdi with exp cohort: 3.5314013136565543\n",
      "P-value for tdi:\n",
      "Mean tdi with control cohort: -1.5812903225806452\n",
      "Standard Deviation of tdi with control cohort: 2.93672010008386\n",
      "P-value for tdi:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MannwhitneyuResult(statistic=497.0, pvalue=0.015665827891245248)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = cevd_df_second[cevd_df_second['dep_diag_yes_1'] == 1]\n",
    "mean_age = subset['tdi'].mean()\n",
    "std_dev_age = subset['tdi'].std()\n",
    "\n",
    "print(\"Mean tdi with exp cohort:\", mean_age)\n",
    "print(\"Standard Deviation of tdi with exp cohort:\", std_dev_age)\n",
    "print('P-value for tdi:')\n",
    "stats.mannwhitneyu(x=cevd_df_second['tdi'], y=cevd_df_second['dep_diag_yes_1'], alternative = 'two-sided')\n",
    "subset = cevd_df_second[cevd_df_second['dep_diag_yes_1'] == 0]\n",
    "mean_age = subset['tdi'].mean()\n",
    "std_dev_age = subset['tdi'].std()\n",
    "\n",
    "print(\"Mean tdi with control cohort:\", mean_age)\n",
    "print(\"Standard Deviation of tdi with control cohort:\", std_dev_age)\n",
    "print('P-value for tdi:')\n",
    "stats.mannwhitneyu(x=cevd_df_second['tdi'], y=cevd_df_second['dep_diag_yes_1'], alternative = 'two-sided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp Percentages of alc freq\n",
      "1.0    47.580645\n",
      "2.0    36.290323\n",
      "3.0    16.129032\n",
      "Name: alc_freq_filter, dtype: float64\n",
      "econtrol Percentages of alc freq\n",
      "2.0    45.095168\n",
      "1.0    34.699854\n",
      "3.0    20.204978\n",
      "Name: alc_freq_filter, dtype: float64\n",
      "ALC FREQ - Chi-square test statistic: 7.497690178653304\n",
      "ALC FREQ - P-value: 0.023544922441955163\n"
     ]
    }
   ],
   "source": [
    "# get the %s of each category for alcohol frequency \n",
    "perc_alc_freq_d = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 1, 'alc_freq_filter'].value_counts(normalize=True)\n",
    "print('exp Percentages of alc freq')\n",
    "print(perc_alc_freq_d * 100)\n",
    "perc_alc_freq_c = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 0, 'alc_freq_filter'].value_counts(normalize=True)\n",
    "# print the value counts as percentages\n",
    "print('econtrol Percentages of alc freq')\n",
    "print(perc_alc_freq_c * 100)\n",
    "data = pd.crosstab(cevd_df_first_onlycevd['alc_freq_filter'], cevd_df_first_onlycevd['dep_diag_yes']).to_numpy()\n",
    "# chi-square test for independence\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(data)\n",
    "print('ALC FREQ - Chi-square test statistic:', chi2_stat)\n",
    "print('ALC FREQ - P-value:', p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp Percentages of alc freq\n",
      "2.0    71.428571\n",
      "1.0    14.285714\n",
      "3.0    14.285714\n",
      "Name: alc_freq_filter_1, dtype: float64\n",
      "econtrol Percentages of alc freq\n",
      "2.0    61.290323\n",
      "1.0    22.580645\n",
      "3.0    16.129032\n",
      "Name: alc_freq_filter_1, dtype: float64\n",
      "ALC FREQ - Chi-square test statistic: 0.2918586789554529\n",
      "ALC FREQ - P-value: 0.8642187671766579\n"
     ]
    }
   ],
   "source": [
    "# get the %s of each category for alcohol frequency \n",
    "perc_alc_freq_d = cevd_df_second.loc[cevd_df_second['dep_diag_yes_1'] == 1, 'alc_freq_filter_1'].value_counts(normalize=True)\n",
    "print('exp Percentages of alc freq')\n",
    "print(perc_alc_freq_d * 100)\n",
    "perc_alc_freq_c = cevd_df_second.loc[cevd_df_second['dep_diag_yes_1'] == 0, 'alc_freq_filter_1'].value_counts(normalize=True)\n",
    "# print the value counts as percentages\n",
    "print('econtrol Percentages of alc freq')\n",
    "print(perc_alc_freq_c * 100)\n",
    "data = pd.crosstab(cevd_df_second['alc_freq_filter_1'], cevd_df_second['dep_diag_yes_1']).to_numpy()\n",
    "# chi-square test for independence\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(data)\n",
    "print('ALC FREQ - Chi-square test statistic:', chi2_stat)\n",
    "print('ALC FREQ - P-value:', p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp Percentages of comorbid\n",
      "2    45.967742\n",
      "1    34.677419\n",
      "0    19.354839\n",
      "Name: comorb_sum_filter, dtype: float64\n",
      "CONTROL Percentages of comorbid\n",
      "2    46.559297\n",
      "1    32.503660\n",
      "0    20.937042\n",
      "Name: comorb_sum_filter, dtype: float64\n",
      "comorbid - Chi-square test statistic: 0.28587318083201146\n",
      "comorbid - P-value: 0.8668090311529716\n"
     ]
    }
   ],
   "source": [
    "# get comorbid filter data\n",
    "perc_alc_freq_d = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 1, 'comorb_sum_filter'].value_counts(normalize=True)\n",
    "print('exp Percentages of comorbid')\n",
    "print(perc_alc_freq_d * 100)\n",
    "perc_alc_freq_c = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 0, 'comorb_sum_filter'].value_counts(normalize=True)\n",
    "print('CONTROL Percentages of comorbid')\n",
    "print(perc_alc_freq_c * 100)\n",
    "data = pd.crosstab(cevd_df_first_onlycevd['comorb_sum_filter'], cevd_df_first_onlycevd['dep_diag_yes']).to_numpy()\n",
    "# chi-square test for independence\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(data)\n",
    "print('comorbid - Chi-square test statistic:', chi2_stat)\n",
    "print('comorbid - P-value:', p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp Percentages of comorbid\n",
      "1    57.142857\n",
      "2    28.571429\n",
      "0    14.285714\n",
      "Name: comorb_sum_filter_1, dtype: float64\n",
      "CONTROL Percentages of comorbid\n",
      "2    45.161290\n",
      "1    29.032258\n",
      "0    25.806452\n",
      "Name: comorb_sum_filter_1, dtype: float64\n",
      "comorbid - Chi-square test statistic: 2.0123281736184957\n",
      "comorbid - P-value: 0.36561877499003015\n"
     ]
    }
   ],
   "source": [
    "# get comorbid filter data\n",
    "perc_alc_freq_d = cevd_df_second.loc[cevd_df_second['dep_diag_yes_1'] == 1, 'comorb_sum_filter_1'].value_counts(normalize=True)\n",
    "print('exp Percentages of comorbid')\n",
    "print(perc_alc_freq_d * 100)\n",
    "perc_alc_freq_c = cevd_df_second.loc[cevd_df_second['dep_diag_yes_1'] == 0, 'comorb_sum_filter_1'].value_counts(normalize=True)\n",
    "print('CONTROL Percentages of comorbid')\n",
    "print(perc_alc_freq_c * 100)\n",
    "data = pd.crosstab(cevd_df_second['comorb_sum_filter_1'], cevd_df_second['dep_diag_yes_1']).to_numpy()\n",
    "# chi-square test for independence\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(data)\n",
    "print('comorbid - Chi-square test statistic:', chi2_stat)\n",
    "print('comorbid - P-value:', p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#person year calculation if needed\n",
    "#i69_days = cevd_df_first['i69_t2e'].sum()\n",
    "#i69_personyears = i69_days/365\n",
    "#print(i69_personyears)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sensitivity analysis 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#drop all px on disability assistance\n",
    "cevd_df_noassist = cevd_df_first[cevd_df_first['gov_assistance_filter']==0]\n",
    "len(cevd_df_noassist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#drop all px with comorbidities\n",
    "cevd_df_noassist_nocomor = cevd_df_noassist[cevd_df_noassist['comorb_sum_filter']==0]\n",
    "len(cevd_df_noassist_nocomor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cevd_df_noassist_nocomor.to_csv('cevd_first_model_sa.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sensitivity analysis 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#filter time to event for cevd for sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_cevd_t2e(value):\n",
    "    if pd.isna(value):\n",
    "        return 0\n",
    "    elif value < 1096:\n",
    "        return 1\n",
    "    elif 1096 <= value <= 1825:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "cevd_df_first['cevd_t2e_filter'] = cevd_df_first['cevd_t2e'].apply(filter_cevd_t2e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cevd_0 = cevd_df_first[cevd_df_first['cevd_t2e_filter']==0]\n",
    "print('Number of px with no cevd:',len(cevd_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cevd_1 = cevd_df_first[cevd_df_first['cevd_t2e_filter']==1]\n",
    "print('Number of px with cevd within 3 years of accelerometer study:',len(cevd_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cevd_2 = cevd_df_first[cevd_df_first['cevd_t2e_filter']==2]\n",
    "print('Number of px with cevd between 3-5 years after accelerometer study:',len(cevd_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cevd_3 = cevd_df_first[cevd_df_first['cevd_t2e_filter']==3]\n",
    "print('Number of px with cevd 5 or more years after accelerometer study:',len(cevd_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cevd_df_first.to_csv('cevd_first_model_sa_cevdt2e.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_t2e_cevd = cevd_df_first['cevd_t2e'].mean()\n",
    "std_t2e_cevd = cevd_df_first['cevd_t2e'].std()\n",
    "\n",
    "print('mean years from end of study to CeVD:', (mean_t2e_cevd/365))\n",
    "print('std:', (std_t2e_cevd/365))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sensitvity analysis 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_dep_dur(value):\n",
    "    if pd.isna(value):\n",
    "        return 0\n",
    "    elif value < 1896:\n",
    "        return 1\n",
    "    elif 1896 <= value <= 3650:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "cevd_df_first['dep_dur_filter'] = cevd_df_first['dep_dur'].apply(filter_dep_dur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dep_0 = cevd_df_first[cevd_df_first['dep_dur_filter']==0]\n",
    "print('number of patients without depression diagnosis prior to accelerometer study:', len(dep_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dep_1 = cevd_df_first[cevd_df_first['dep_dur_filter']==1]\n",
    "print('number of patients with depression diagnosis within 5 yrs before accelerometer study:', len(dep_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dep_2 = cevd_df_first[cevd_df_first['dep_dur_filter']==2]\n",
    "print('number of patients with depression diagnosis conferred between 5 and 10 yrs before accelerometer study:', len(dep_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dep_3 = cevd_df_first[cevd_df_first['dep_dur_filter']==3]\n",
    "print('number of patients with depression diagnosis conferred 10 or more yrs before accelerometer study:', len(dep_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_t2e_dep = cevd_df_first['dep_dur'].mean()\n",
    "std_t2e_dep = cevd_df_first['dep_dur'].std()\n",
    "\n",
    "print('mean years from dep diag to start of study:', (mean_t2e_dep/365))\n",
    "print('std:', (std_t2e_dep/365))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cevd_df_first.to_csv('cevd_first_model_sa_cevdt2eanddep.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#to upload on RAP UKBB\n",
    "#%%bash\n",
    "#dx upload <insert file name here>.ipynb --dest /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
