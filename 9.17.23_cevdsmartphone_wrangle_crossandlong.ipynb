{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/cluster/dnax/jars/dnanexus-api-0.1.0-SNAPSHOT-jar-with-dependencies.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/cluster/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-19 21:51:46.615 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2023-09-19 21:51:47.693 WARN  Utils:69 - Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 43000. Attempting port 43001.\n",
      "2023-09-19 21:51:47.966 WARN  MetricsReporter:84 - No metrics configured for reporting\n",
      "2023-09-19 21:51:47.967 WARN  LineProtoUsageReporter:48 - Telegraf configurations: url [metrics.push.telegraf.hostport], user [metrics.push.telegraf.user] or password [metrics.push.telegraf.password] missing.\n",
      "2023-09-19 21:51:47.967 WARN  MetricsReporter:117 - metrics.scraping.httpserver.port\n"
     ]
    }
   ],
   "source": [
    "#run pip install fancyimpute\n",
    "#launch spark \n",
    "import pyspark\n",
    "import dxpy\n",
    "import dxdata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from fancyimpute import IterativeImputer\n",
    "from scipy.stats import chi2_contingency\n",
    "sc = pyspark.SparkContext()\n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#prep database \n",
    "dispensed_database_name = dxpy.find_one_data_object(classname=\"database\", name=\"app*\", folder=\"/\", name_mode=\"glob\", describe=True)[\"describe\"][\"name\"]\n",
    "dispensed_dataset_id = dxpy.find_one_data_object(typename=\"Dataset\", name=\"app*.dataset\", folder=\"/\", name_mode=\"glob\")[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#prep dataset\n",
    "dataset = dxdata.load_dataset(id=dispensed_dataset_id)\n",
    "participant = dataset[\"participant\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-19 21:51:59.546 WARN  ShellBasedUnixGroupsMapping:210 - unable to return groups for user y8bFk6BpbyJVJBX0bKy7JGP0fKQ54pzxkfx6V2x6__project-GG7jpPjJ4VK0VKqV89494XV5\n",
      "PartialGroupNameException The user name 'y8bFk6BpbyJVJBX0bKy7JGP0fKQ54pzxkfx6V2x6__project-GG7jpPjJ4VK0VKqV89494XV5' is not found. id: ‘y8bFk6BpbyJVJBX0bKy7JGP0fKQ54pzxkfx6V2x6__project-GG7jpPjJ4VK0VKqV89494XV5’: no such user\n",
      "id: ‘y8bFk6BpbyJVJBX0bKy7JGP0fKQ54pzxkfx6V2x6__project-GG7jpPjJ4VK0VKqV89494XV5’: no such user\n",
      "\n",
      "\tat org.apache.hadoop.security.ShellBasedUnixGroupsMapping.resolvePartialGroupNames(ShellBasedUnixGroupsMapping.java:294)\n",
      "\tat org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:207)\n",
      "\tat org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:97)\n",
      "\tat org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.getGroups(JniBasedUnixGroupsMappingWithFallback.java:51)\n",
      "\tat org.apache.hadoop.security.Groups$GroupCacheLoader.fetchGroupList(Groups.java:387)\n",
      "\tat org.apache.hadoop.security.Groups$GroupCacheLoader.load(Groups.java:321)\n",
      "\tat org.apache.hadoop.security.Groups$GroupCacheLoader.load(Groups.java:270)\n",
      "\tat org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)\n",
      "\tat org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)\n",
      "\tat org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)\n",
      "\tat org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)\n",
      "\tat org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache.get(LocalCache.java:3962)\n",
      "\tat org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3985)\n",
      "\tat org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4946)\n",
      "\tat org.apache.hadoop.security.Groups.getGroups(Groups.java:228)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.getGroups(UserGroupInformation.java:1734)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1722)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:517)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:254)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3650)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMetaStoreClient(Hive.java:3696)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.lambda$getMSC$0(Hive.java:3770)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3768)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3682)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1600)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1588)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:396)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:305)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:236)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:235)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:285)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:396)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)\n",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:150)\n",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)\n",
      "\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager$lzycompute(SharedState.scala:170)\n",
      "\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager(SharedState.scala:168)\n",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$2(HiveSessionStateBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager$lzycompute(SessionCatalog.scala:119)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager(SessionCatalog.scala:119)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupGlobalTempView(SessionCatalog.scala:940)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTempViews$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveTempViews$$lookupTempView(Analyzer.scala:950)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTempViews$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveTempViews$$lookupAndResolveTempView(Analyzer.scala:963)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTempViews$$anonfun$apply$12.applyOrElse(Analyzer.scala:901)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTempViews$$anonfun$apply$12.applyOrElse(Analyzer.scala:899)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1122)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1121)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode.mapChildren(LogicalPlan.scala:206)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1122)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1121)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode.mapChildren(LogicalPlan.scala:206)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTempViews$.apply(Analyzer.scala:899)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1164)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1131)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:222)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:218)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:167)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:218)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:203)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:202)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:75)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:183)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:183)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:75)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:73)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:65)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "#pull all relevant columns\n",
    "field_names = [\"eid\", \"p31\", \"p22189\", \"p21000_i0\", \"p21001_i0\", 'p21001_i1', 'p21003_i0',  'p21003_i1', \"p1558_i0\",\"p1558_i1\", \"p20002_i0\",  \"p131298\", \"p6150_i0\", \"p131286\", \"p131294\", \"p130814\", \"p130706\", \"p130708\", \"p53_i0\",  \"p53_i1\",  \"p130892\", \"p130894\", \"p130896\", \"p2050_i0\", \"p2060_i0\",  \"p2050_i1\",  \"p130838\", \"p131056\", \"p131058\", \"p131180\", \"p131360\", \"p131362\", \"p131364\", \"p131366\", \"p131368\", \"p131370\", \"p131372\", \"p131374\", \"p131376\", \"p131378\", \"p22032_i0\", 'p21001_i1', 'p1558_i1', 'p21003_i1',  'p21003_i1', 'p20116_i0', 'p20116_i1', 'p20003_i0', 'p1160_i0', 'p20003_i1', 'p1160_i1', 'p1070_i0', 'p1070_i1', 'p1080_i0', 'p1080_i1', 'p1120_i0', 'p1120_i1', 'p110005', 'p90051' , 'p90016', 'p90017', 'p90185', 'p90187', 'p90012', 'p90010', 'p90011', 'p6142_i0' , 'p134_i0', 'p131354', 'p131306', 'p40046_i0', 'p40047_i0', 'p40048_i0', 'p40049_i0', 'p90027', 'p90028', 'p90029', 'p90030', 'p90031', 'p90032', 'p90033', 'p90034', 'p90035', 'p90036', 'p90037', 'p90038', 'p90039', 'p90040', 'p90041', 'p90042', 'p90043', 'p90044', 'p90045', 'p90046', 'p90047', 'p90048', 'p90049', 'p90050', 'p90051', 'p40030_i0', 'p40031_i0', 'p40032_i0', 'p40033_i0', 'p6146_i0', 'p134_i1', 'p20002_i1', 'p22040_i0']\n",
    "df = participant.retrieve_fields(names=field_names, engine=dxdata.connect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-19 21:52:30.913 WARN  package:69 - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    }
   ],
   "source": [
    "#create pandas dataframe\n",
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "502364"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check that pandas conversion worked - this is the full UKBB rows of px\n",
    "#pdf.head(10)\n",
    "len(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pID</th>\n",
       "      <th>male</th>\n",
       "      <th>tdi</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bmi_1</th>\n",
       "      <th>age_i0</th>\n",
       "      <th>age_i1</th>\n",
       "      <th>alc_freq</th>\n",
       "      <th>alc_freq_1</th>\n",
       "      <th>...</th>\n",
       "      <th>pa_time_23</th>\n",
       "      <th>pa_time_24</th>\n",
       "      <th>pa_sleep_day_hour</th>\n",
       "      <th>pa_sed_day_hour</th>\n",
       "      <th>pa_light_day_hour</th>\n",
       "      <th>pa_mv_day_hour</th>\n",
       "      <th>gov_assistance</th>\n",
       "      <th>p134_i1</th>\n",
       "      <th>sr_illness_1</th>\n",
       "      <th>met_minweek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5545411</td>\n",
       "      <td>1</td>\n",
       "      <td>-4.62</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>28.8906</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>15.48</td>\n",
       "      <td>8.51</td>\n",
       "      <td>0.96,0.97,0.95,0.96,1,1,0.95,0.57,0.17,0.07,0,...</td>\n",
       "      <td>0.01,0.03,0.05,0.04,0,0,0.01,0.34,0.62,0.45,0....</td>\n",
       "      <td>0.03,0,0,0,0,0,0.04,0.09,0.21,0.35,0.33,0.22,0...</td>\n",
       "      <td>0,0,0,0,0,0,0,0,0,0.13,0.15,0.34,0.16,0,0.02,0...</td>\n",
       "      <td>[-7]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5180517</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>22.7933</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>31.26</td>\n",
       "      <td>32.41</td>\n",
       "      <td>0.98,1,1,1,1,1,1,0.44,0.02,0.14,0,0,0.04,0.12,...</td>\n",
       "      <td>0.02,0,0,0,0,0,0,0.46,0.52,0.22,0.28,0.33,0.5,...</td>\n",
       "      <td>0,0,0,0,0,0,0,0.11,0.46,0.58,0.57,0.57,0.35,0....</td>\n",
       "      <td>0,0,0,0,0,0,0,0,0,0.05,0.15,0.1,0.1,0.13,0.07,...</td>\n",
       "      <td>[-7]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1668262</td>\n",
       "      <td>1</td>\n",
       "      <td>-4.35</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>25.3439</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[-7]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2151806</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.06</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>32.7087</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[-7]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>594.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4202281</td>\n",
       "      <td>0</td>\n",
       "      <td>8.22</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>24.2078</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[2]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pID  male   tdi  ethnicity      bmi  bmi_1  age_i0  age_i1  alc_freq  \\\n",
       "0  5545411     1 -4.62     1001.0  28.8906    NaN      59     NaN       3.0   \n",
       "1  5180517     0 -0.42     1001.0  22.7933    NaN      52     NaN       5.0   \n",
       "2  1668262     1 -4.35     1001.0  25.3439    NaN      61     NaN       3.0   \n",
       "3  2151806     0 -3.06     1001.0  32.7087    NaN      57     NaN       4.0   \n",
       "4  4202281     0  8.22     1001.0  24.2078    NaN      51     NaN       1.0   \n",
       "\n",
       "   alc_freq_1  ... pa_time_23 pa_time_24  \\\n",
       "0         NaN  ...      15.48       8.51   \n",
       "1         NaN  ...      31.26      32.41   \n",
       "2         NaN  ...        NaN        NaN   \n",
       "3         NaN  ...        NaN        NaN   \n",
       "4         NaN  ...        NaN        NaN   \n",
       "\n",
       "                                   pa_sleep_day_hour  \\\n",
       "0  0.96,0.97,0.95,0.96,1,1,0.95,0.57,0.17,0.07,0,...   \n",
       "1  0.98,1,1,1,1,1,1,0.44,0.02,0.14,0,0,0.04,0.12,...   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                     pa_sed_day_hour  \\\n",
       "0  0.01,0.03,0.05,0.04,0,0,0.01,0.34,0.62,0.45,0....   \n",
       "1  0.02,0,0,0,0,0,0,0.46,0.52,0.22,0.28,0.33,0.5,...   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                   pa_light_day_hour  \\\n",
       "0  0.03,0,0,0,0,0,0.04,0.09,0.21,0.35,0.33,0.22,0...   \n",
       "1  0,0,0,0,0,0,0,0.11,0.46,0.58,0.57,0.57,0.35,0....   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                      pa_mv_day_hour gov_assistance p134_i1  \\\n",
       "0  0,0,0,0,0,0,0,0,0,0.13,0.15,0.34,0.16,0,0.02,0...           [-7]     NaN   \n",
       "1  0,0,0,0,0,0,0,0,0,0.05,0.15,0.1,0.1,0.13,0.07,...           [-7]     NaN   \n",
       "2                                               None           [-7]     NaN   \n",
       "3                                               None           [-7]     NaN   \n",
       "4                                               None            [2]     NaN   \n",
       "\n",
       "  sr_illness_1 met_minweek  \n",
       "0         None         NaN  \n",
       "1         None         NaN  \n",
       "2         None        99.0  \n",
       "3         None       594.0  \n",
       "4         None        99.0  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rename columns\n",
    "pdf_rename = pdf.rename(columns={'eid': 'pID', 'p31': 'male', 'p22189': 'tdi', 'p21000_i0': 'ethnicity', 'p21001_i0': 'bmi', 'p21001_i1': 'bmi_1', 'p1558_i0': 'alc_freq', 'p1558_i1': 'alc_freq_1', 'p20002_i0': 'sr_illness', 'p21003_i0': 'age_i0', 'p21003_i1': 'age_i1',  'p131298': 'i21_date', 'p6150_i0': 'vasc_diag', 'p131298': 'i21_date', 'p131286': 'i10_date', 'p131294': 'i15_date', 'p130814': 'e78_date', 'p130706': 'e10_date', 'p130708': 'e11_date', 'p53_i0': 'date_i0', 'p53_i1': 'date_i1', 'p41271': 'icd9', 'p41270': 'icd10', 'p130892': 'f31_date',  'p130894': 'f32_date', 'p130896': 'f33_date', 'p2050_i0': 'phq2_1', 'p130838': 'f01_date', 'p131056': 'g45_date', 'p131058': 'g46_date', 'p131180': 'h34_date', 'p131360': 'i60_date', 'p131362': 'i61_date', 'p131364': 'i62_date', 'p131366': 'i63_date', 'p131368': 'i64_date', 'p131370': 'i65_date', 'p131372': 'i66_date', 'p131374': 'i67_date', 'p131376': 'i68_date', 'p131378': 'i69_date', 'p20116_i0': 'smok_stat', \"p20116_i1\": 'smok_stat_1', 'p1160_i0': 'sleep', 'p1160_i1': 'sleep_1', 'p1070_i0':'tv', 'p1070_i1':'tv_1', 'p1080_i0':'computer', 'p1080_i1':'computer_1', 'p1120_i0': 'mobile phone', 'p1120_i1': 'mobile phone_1', 'p20510': 'phq2_1_1', 'p22032_i0': 'ipaq', 'p2050_i1': 'phq2_1_followup', 'p90012': 'aac_overall_avg', 'p90185': 'acc_exceed8g_aftercal', 'p90187': 'acc_total_data_read', 'p90017': 'acc_calib_own', 'p90016': 'acc_calib_all', 'p90051': 'acc_weartime', 'p90010': 'acc_start_date', 'p90011': 'acc_end_date', 'p6142_i0': 'employ_status',  'p131354': 'i50_date', 'p131306': 'i25_date', 'p40046_i0': 'pa_sleep_over_avg', 'p40047_i0': 'pa_seden_over_avg', 'p40048_i0': 'pa_light_over_avg',  'p40049_i0': 'pa_mv_over_avg', 'p40049_i0': 'pa_mv_over_avg', 'p90027':'pa_time_1', 'p90028':'pa_time_2', 'p90029':'pa_time_3', 'p90030':'pa_time_4', 'p90031':'pa_time_5', 'p90032':'pa_time_6', 'p90033':'pa_time_7', 'p90034':'pa_time_8', 'p90035':'pa_time_9', 'p90036':'pa_time_10', 'p90037':'pa_time_11', 'p90038':'pa_time_12', 'p90039':'pa_time_13', 'p90040':'pa_time_14', 'p90041':'pa_time_15', 'p90042':'pa_time_16', 'p90043':'pa_time_17', 'p90044':'pa_time_18', 'p90045':'pa_time_19', 'p90046':'pa_time_20', 'p90047':'pa_time_21', 'p90048':'pa_time_22', 'p90049':'pa_time_23', 'p90050':'pa_time_24', 'p90051': 'pa_sum_wear', 'p40030_i0': 'pa_sleep_day_hour', 'p40031_i0': 'pa_sed_day_hour', 'p40032_i0': 'pa_light_day_hour', 'p40033_i0': 'pa_mv_day_hour', 'p6146_i0': 'gov_assistance', 'p20002_i1':'sr_illness_1', 'p22040_i0': 'met_minweek'})\n",
    "#check that rename worked\n",
    "pdf_rename.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_rename['met_minweek'] = pd.to_numeric(pdf_rename['met_minweek'], errors='coerce')\n",
    "pdf_rename['met_minweek_filter'] = pdf_rename['met_minweek']/7\n",
    "pdf_rename['met_minweek_filter'] = pdf_rename['met_minweek_filter']/60\n",
    "pdf_rename.dropna(subset=['met_minweek_filter'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#convert ethnicity data to be workable\n",
    "pdf_rename['ethnicity'].dtype\n",
    "pdf_rename['ethnicity'] = pd.to_numeric(pdf_rename['ethnicity'], errors='coerce')\n",
    "pdf_rename['ethnicity'].dtype\n",
    "for index, row in pdf_rename.iterrows():\n",
    "    if row['ethnicity'] == 1.0 or row['ethnicity'] == 1001.0 or row['ethnicity'] == 1002.0 or row['ethnicity'] == 1003.0:\n",
    "        pdf_rename.at[index, 'white_yes'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after dropping px with missing sleep survey data 384383\n"
     ]
    }
   ],
   "source": [
    "#drop -1 (\"do not know\") and -3 (\"prefer not to answer\") from sleep duration survey\n",
    "pdf_rename = pdf_rename[~(pdf_rename['sleep'] == -3)]\n",
    "pdf_rename = pdf_rename[~(pdf_rename['sleep'] == -1)]\n",
    "print('after dropping px with missing sleep survey data', len(pdf_rename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after dropping px with missing smoking survey data 383441\n"
     ]
    }
   ],
   "source": [
    "#for smoking status, drop anyone who answered \"-3/prefer not to answer\"\n",
    "pdf_rename['smok_stat'] = pdf_rename['smok_stat'].astype(str)\n",
    "pdf_rename = pdf_rename[~pdf_rename['smok_stat'].str.contains('-3')]\n",
    "print('after dropping px with missing smoking survey data', len(pdf_rename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after dropping px with incomplete screen surveys data 379604\n"
     ]
    }
   ],
   "source": [
    "#drop -1 and -3 from tv and computer and mobile phone use; fix tv use value of -10\n",
    "pdf_rename = pdf_rename[~(pdf_rename['tv'] == -3)]\n",
    "pdf_rename = pdf_rename[~(pdf_rename['tv'] == -1)]\n",
    "pdf_rename = pdf_rename[~(pdf_rename['computer'] == -3)]\n",
    "pdf_rename = pdf_rename[~(pdf_rename['computer'] == -1)]\n",
    "pdf_rename = pdf_rename[~(pdf_rename['mobile phone'] == -3)]\n",
    "pdf_rename = pdf_rename[~(pdf_rename['mobile phone'] == -1)]\n",
    "pdf_rename['tv'] = pdf_rename['tv'].replace(-10, 0.5)\n",
    "print('after dropping px with incomplete screen surveys data', len(pdf_rename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after dropping px with missing sleep survey data 324750\n"
     ]
    }
   ],
   "source": [
    "pdf_rename.dropna(subset=['computer', 'tv', 'mobile phone'], inplace=True)\n",
    "print('after dropping px with missing sleep survey data', len(pdf_rename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after dropping px with incomplete alcohol survey data 324638\n"
     ]
    }
   ],
   "source": [
    "#drop alc_freq values for \"prefer not to answer\"/-3\n",
    "pdf_rename = pdf_rename[~(pdf_rename['alc_freq'] == -3)]\n",
    "print('after dropping px with incomplete alcohol survey data', len(pdf_rename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reorder alc_freq values \n",
    "#if 4\tOne to three times a month | 5\tSpecial occasions only | 6\tNever, then make seldom=1\n",
    "#if 2\tThree or four times a week | 3\tOnce or twice a week, then make sometimes=2\n",
    "#if 1\tDaily or almost daily, then make daily=3\n",
    "pdf_rename['alc_freq_filter'] = np.where(pdf_rename['alc_freq'].isin([4, 5, 6]), 1, \n",
    "                    np.where(pdf_rename['alc_freq'].isin([2, 3]), 2, \n",
    "                             np.where(pdf_rename['alc_freq'].isin([1]), 3, np.nan)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "323670"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop phq2 scores where response was \"-3/prefer not to answer\"\n",
    "pdf_rename = pdf_rename.drop(pdf_rename.loc[pdf_rename['phq2_1'] < -1].index)\n",
    "len(pdf_rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "323670"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace all occurrences of -1 (do not know if depressed) in column PHQ2_1 and PHQ2_2 with 0\n",
    "pdf_rename['phq2_1'] = pdf_rename['phq2_1'].replace(-1, 0)\n",
    "len(pdf_rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after dropping px with self-reported HA or CeVD prior to baseline 312617\n"
     ]
    }
   ],
   "source": [
    "#convert sr_illness to string\n",
    "pdf_rename['sr_illness'].dtype\n",
    "pdf_rename['sr_illness'] = pdf_rename['sr_illness'].astype(str)\n",
    "#remove 1075= heart attack, 1081= stroke, 1082= tia, 1086= sah, 1282= brao/crao, 1491= brain hemorrhage, 1583= ischemic stroke\n",
    "pdf_rename = pdf_rename[~pdf_rename['sr_illness'].str.contains('1075|1081|1082|1086|1282|1491|1583')]\n",
    "print('after dropping px with self-reported HA or CeVD prior to baseline', len(pdf_rename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#convert dates to datetime values or analysis\n",
    "pdf_rename[\"date_i0\"] = pd.to_datetime(pdf_rename[\"date_i0\"])\n",
    "pdf_rename[\"i21_date\"] = pd.to_datetime(pdf_rename[\"i21_date\"])\n",
    "pdf_rename[\"f01_date\"] = pd.to_datetime(pdf_rename[\"f01_date\"])\n",
    "pdf_rename[\"g45_date\"] = pd.to_datetime(pdf_rename[\"g45_date\"])\n",
    "pdf_rename[\"g46_date\"] = pd.to_datetime(pdf_rename[\"g46_date\"])\n",
    "pdf_rename[\"h34_date\"] = pd.to_datetime(pdf_rename[\"h34_date\"])\n",
    "pdf_rename[\"i60_date\"] = pd.to_datetime(pdf_rename[\"i60_date\"])\n",
    "pdf_rename[\"i61_date\"] = pd.to_datetime(pdf_rename[\"i61_date\"])\n",
    "pdf_rename[\"i62_date\"] = pd.to_datetime(pdf_rename[\"i62_date\"])\n",
    "pdf_rename[\"i63_date\"] = pd.to_datetime(pdf_rename[\"i63_date\"])\n",
    "pdf_rename[\"i64_date\"] = pd.to_datetime(pdf_rename[\"i64_date\"])\n",
    "pdf_rename[\"i65_date\"] = pd.to_datetime(pdf_rename[\"i65_date\"])\n",
    "pdf_rename[\"i66_date\"] = pd.to_datetime(pdf_rename[\"i66_date\"])\n",
    "pdf_rename[\"i67_date\"] = pd.to_datetime(pdf_rename[\"i67_date\"])\n",
    "pdf_rename[\"i68_date\"] = pd.to_datetime(pdf_rename[\"i68_date\"])\n",
    "pdf_rename[\"i69_date\"] = pd.to_datetime(pdf_rename[\"i69_date\"])\n",
    "pdf_rename[\"f31_date\"] = pd.to_datetime(pdf_rename[\"f31_date\"])\n",
    "pdf_rename[\"f32_date\"] = pd.to_datetime(pdf_rename[\"f32_date\"])\n",
    "pdf_rename[\"f33_date\"] = pd.to_datetime(pdf_rename[\"f33_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after dropping px with medical record HA prior to baseline 312394\n"
     ]
    }
   ],
   "source": [
    "#drop px with prior heart attack (HA) \n",
    "#convert date columns to days format\n",
    "pdf_rename['i21_time'] = (pdf_rename['date_i0'] - pdf_rename['i21_date']).dt.days\n",
    "#drop cases with a prior heart attack (I21)\n",
    "pdf_rename = pdf_rename.loc[(pdf_rename['i21_time'] < 0) | (pdf_rename['i21_time'].isna())]\n",
    "print('after dropping px with medical record HA prior to baseline', len(pdf_rename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after dropping px with other self-report question on HA/stroke/prefer not to answer prior to baseline 308621\n"
     ]
    }
   ],
   "source": [
    "#convert vasc_diag to string\n",
    "pdf_rename['vasc_diag'].dtype\n",
    "pdf_rename['vasc_diag'] = pdf_rename['vasc_diag'].astype(str)\n",
    "#drop px who report prior vascular diagnoses of HA, STROKE, PREFER NOT TO ANSWER\n",
    "pdf_rename = pdf_rename.loc[(pdf_rename['vasc_diag'] == '[-7]') | (pdf_rename['vasc_diag']  == '[4]') | (pdf_rename['vasc_diag']  == '[2]')]\n",
    "print('after dropping px with other self-report question on HA/stroke/prefer not to answer prior to baseline', len(pdf_rename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check number of px with TIA prior to baseline 569\n",
      "check number of px with I63 prior to baseline 58\n",
      "check number of px with I64  prior to baseline 106\n"
     ]
    }
   ],
   "source": [
    "#check cases with a prior vascular brain syndromes of CeVD (G46)\n",
    "pdf_rename['g45_time'] = (pdf_rename['date_i0'] - pdf_rename['g45_date']).dt.days\n",
    "pdf_rename_check_g45 = pdf_rename.loc[(pdf_rename['g45_time'] > 0)]\n",
    "print('check number of px with TIA prior to baseline', len(pdf_rename_check_g45))\n",
    "#drop cases with a prior vascular brain syndromes of CeVD (G46)\n",
    "pdf_rename['i63_time'] = (pdf_rename['date_i0'] - pdf_rename['i63_date']).dt.days\n",
    "pdf_rename_check_i63 = pdf_rename.loc[(pdf_rename['i63_time'] > 0)]\n",
    "print('check number of px with I63 prior to baseline', len(pdf_rename_check_i63))\n",
    "#drop cases with a prior vascular brain syndromes of CeVD (G46)\n",
    "pdf_rename['i64_time'] = (pdf_rename['date_i0'] - pdf_rename['i64_date']).dt.days\n",
    "pdf_rename_check_i64 = pdf_rename.loc[(pdf_rename['i64_time'] > 0)]\n",
    "print('check number of px with I64  prior to baseline', len(pdf_rename_check_i64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after dropping px with CeVD diagnoses in medical records prior to baseline 307008\n"
     ]
    }
   ],
   "source": [
    "#drop prior CevD px who have recorded diagnosis in medical records\n",
    "#drop cases with a prior vascular dementia (F01)\n",
    "pdf_rename['f01_time'] = (pdf_rename['date_i0'] - pdf_rename['f01_date']).dt.days\n",
    "pdf_rename = pdf_rename.loc[(pdf_rename['f01_time'] < 0) | (pdf_rename['f01_time'].isna())]\n",
    "\n",
    "#drop cases with a prior TIA (G45)\n",
    "pdf_rename['g45_time'] = (pdf_rename['date_i0'] - pdf_rename['g45_date']).dt.days\n",
    "pdf_rename = pdf_rename.loc[(pdf_rename['g45_time'] < 0) | (pdf_rename['g45_time'].isna())]\n",
    "\n",
    "#drop cases with a prior vascular brain syndromes of CeVD (G46)\n",
    "pdf_rename['g46_time'] = (pdf_rename['date_i0'] - pdf_rename['g46_date']).dt.days\n",
    "pdf_rename = pdf_rename.loc[(pdf_rename['g46_time'] < 0) | (pdf_rename['g46_time'].isna())]\n",
    "\n",
    "#drop cases with a prior RAO (H34)\n",
    "pdf_rename['h34_time'] = (pdf_rename['date_i0'] - pdf_rename['h34_date']).dt.days\n",
    "pdf_rename = pdf_rename.loc[(pdf_rename['h34_time'] < 0) | (pdf_rename['h34_time'].isna())]\n",
    "\n",
    "#drop cases with a prior Nontraumatic subarachnoid hemorrhage (I60)\n",
    "pdf_rename['i60_time'] = (pdf_rename['date_i0'] - pdf_rename['i60_date']).dt.days\n",
    "pdf_rename = pdf_rename.loc[(pdf_rename['i60_time'] < 0) | (pdf_rename['i60_time'].isna())]\n",
    "\n",
    "#drop cases with a prior Nontraumatic intracerebral hemorrhage (I61)\n",
    "pdf_rename['i61_time'] = (pdf_rename['date_i0'] - pdf_rename['i61_date']).dt.days\n",
    "pdf_rename = pdf_rename.loc[(pdf_rename['i61_time'] < 0) | (pdf_rename['i61_time'].isna())]\n",
    "\n",
    "#drop cases with a prior Other and unspecified nontraumatic intracranial hemorrhage (I62)\n",
    "pdf_rename['i62_time'] = (pdf_rename['date_i0'] - pdf_rename['i62_date']).dt.days\n",
    "pdf_rename = pdf_rename.loc[(pdf_rename['i62_time'] < 0) | (pdf_rename['i62_time'].isna())]\n",
    "\n",
    "#drop cases with a prior intracerebral infarction (I63)\n",
    "pdf_rename['i63_time'] = (pdf_rename['date_i0'] - pdf_rename['i63_date']).dt.days\n",
    "pdf_rename = pdf_rename.loc[(pdf_rename['i63_time'] < 0) | (pdf_rename['i63_time'].isna())]\n",
    "\n",
    "#drop cases with a prior unspecified stroke (I64)\n",
    "pdf_rename['i64_time'] = (pdf_rename['date_i0'] - pdf_rename['i64_date']).dt.days\n",
    "pdf_rename = pdf_rename.loc[(pdf_rename['i64_time'] < 0) | (pdf_rename['i64_time'].isna())]\n",
    "\n",
    "#drop cases with a prior occlusion/stenosis precerebral arteries (I65)\n",
    "pdf_rename['i65_time'] = (pdf_rename['date_i0'] - pdf_rename['i65_date']).dt.days\n",
    "pdf_rename = pdf_rename.loc[(pdf_rename['i65_time'] < 0) | (pdf_rename['i65_time'].isna())]\n",
    "\n",
    "#drop cases with a prior occlusion/stenosis cerebral arteries (I66)\n",
    "pdf_rename['i66_time'] = (pdf_rename['date_i0'] - pdf_rename['i66_date']).dt.days\n",
    "pdf_rename = pdf_rename.loc[(pdf_rename['i66_time'] < 0) | (pdf_rename['i66_time'].isna())]\n",
    "\n",
    "#drop cases with a prior Other cerebrovascular diseases (I67)\n",
    "pdf_rename['i67_time'] = (pdf_rename['date_i0'] - pdf_rename['i67_date']).dt.days\n",
    "pdf_rename = pdf_rename.loc[(pdf_rename['i67_time'] < 0) | (pdf_rename['i67_time'].isna())]\n",
    "\n",
    "#drop cases with a prior Cerebrovascular disorders in diseases classified elsewhere (I68)\n",
    "pdf_rename['i68_time'] = (pdf_rename['date_i0'] - pdf_rename['i68_date']).dt.days\n",
    "pdf_rename = pdf_rename.loc[(pdf_rename['i68_time'] < 0) | (pdf_rename['i68_time'].isna())]\n",
    "\n",
    "#drop cases with a prior Sequelae of cerebrovascular disease (I69)\n",
    "pdf_rename['i69_time'] = (pdf_rename['date_i0'] - pdf_rename['i69_date']).dt.days\n",
    "pdf_rename = pdf_rename.loc[(pdf_rename['i69_time'] < 0) | (pdf_rename['i69_time'].isna())]\n",
    "\n",
    "print('after dropping px with CeVD diagnoses in medical records prior to baseline', len(pdf_rename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of px with depression diagnosis (F31, F32, F33): 24971\n"
     ]
    }
   ],
   "source": [
    "#prep depression cases in \"days\" format\n",
    "pdf_rename['f31_time'] = (pdf_rename['date_i0'] - pdf_rename['f31_date']).dt.days\n",
    "pdf_rename['f32_time'] = (pdf_rename['date_i0'] - pdf_rename['f32_date']).dt.days\n",
    "pdf_rename['f33_time'] = (pdf_rename['date_i0'] - pdf_rename['f33_date']).dt.days\n",
    "# check for diagnosed depression; if yes, return 1\n",
    "def check_for_dep_diag(row):\n",
    "    if row['f31_time'] > 0 or row['f32_time'] > 0 or row['f33_time'] > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "pdf_rename['dep_diag_yes'] = pdf_rename.apply(check_for_dep_diag, axis=1)\n",
    "check_dep_diag = pdf_rename['dep_diag_yes'].sum()\n",
    "print(\"Number of px with depression diagnosis (F31, F32, F33):\", check_dep_diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median depression duration: 1562.0\n"
     ]
    }
   ],
   "source": [
    "#pull earliest depression diagnosis per row; make depression duration variable\n",
    "pdf_rename['dep_dur'] = pdf_rename[['f31_time', 'f32_time', 'f33_time']].max(axis=1)\n",
    "print(\"median depression duration:\", pdf_rename['dep_dur'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#check depression duration variable with histogram\n",
    "plt.hist(pdf_rename['dep_dur'], bins=20, color='blue', alpha=0.7) \n",
    "plt.xlabel('Depression Duration (Days)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#fill empty rows in white_yes with 0; some 0 variables did not convert correctly and need manual imput\n",
    "pdf_rename['white_yes'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#calculate baseline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create comorbidity covariate in multiple steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#convert comorbidity diagnosis dates to datetime format\n",
    "pdf_rename[\"i10_date\"] = pd.to_datetime(pdf_rename[\"i10_date\"])\n",
    "pdf_rename[\"i15_date\"] = pd.to_datetime(pdf_rename[\"i15_date\"])\n",
    "pdf_rename[\"e78_date\"] = pd.to_datetime(pdf_rename[\"e78_date\"])\n",
    "pdf_rename[\"e10_date\"] = pd.to_datetime(pdf_rename[\"e10_date\"])\n",
    "pdf_rename[\"e11_date\"] = pd.to_datetime(pdf_rename[\"e11_date\"])\n",
    "pdf_rename[\"i50_date\"] = pd.to_datetime(pdf_rename[\"i50_date\"])\n",
    "pdf_rename[\"i25_date\"] = pd.to_datetime(pdf_rename[\"i25_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create hypertension category\n",
    "#prep htn cases\n",
    "pdf_rename['i10_time'] = (pdf_rename['date_i0'] - pdf_rename['i10_date']).dt.days\n",
    "pdf_rename['i15_time'] = (pdf_rename['date_i0'] - pdf_rename['i15_date']).dt.days\n",
    "\n",
    "# check for htn; if yes, return 1\n",
    "def check_htn_criteria(row):\n",
    "    if row['i10_time'] > 0 or row['i15_time'] > 0 or '1065' in row['sr_illness'] or '1072' in row['sr_illness']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "pdf_rename['htn_yes'] = pdf_rename.apply(check_htn_criteria, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create heart failure category\n",
    "#prep  cases\n",
    "pdf_rename['i50_time'] = (pdf_rename['date_i0'] - pdf_rename['i50_date']).dt.days\n",
    "\n",
    "# check for htn; if yes, return 1\n",
    "def check_htn_criteria(row):\n",
    "    if row['i50_time'] > 0  or '1076' in row['sr_illness']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "pdf_rename['hf_yes'] = pdf_rename.apply(check_htn_criteria, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create cad category\n",
    "#prep  cases\n",
    "pdf_rename['i25_time'] = (pdf_rename['date_i0'] - pdf_rename['i25_date']).dt.days\n",
    "\n",
    "# check for htn; if yes, return 1\n",
    "def check_htn_criteria(row):\n",
    "    if row['i25_time'] > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "pdf_rename['cad_yes'] = pdf_rename.apply(check_htn_criteria, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create cancer covariate\n",
    "pdf_rename['cancer_yes'] = pdf_rename['p134_i0'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create hyperlipidemia category\n",
    "#prep hchol cases\n",
    "pdf_rename['e78_time'] = (pdf_rename['date_i0'] - pdf_rename['e78_date']).dt.days\n",
    "def check_hchol_criteria(row):\n",
    "    if row['e78_time'] > 0 or '1473' in row['sr_illness']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "pdf_rename['hchol_yes'] = pdf_rename.apply(check_hchol_criteria, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create diabetes category\n",
    "#prep diabetes cases\n",
    "pdf_rename['e10_date'] = pd.to_datetime(pdf_rename['e10_date'])\n",
    "pdf_rename['e11_date'] = pd.to_datetime(pdf_rename['e11_date'])\n",
    "pdf_rename['date_i0'] = pd.to_datetime(pdf_rename['date_i0'])\n",
    "pdf_rename['e10_time'] = (pdf_rename['date_i0'] - pdf_rename['e10_date']).dt.days\n",
    "pdf_rename['e11_time'] = (pdf_rename['date_i0'] - pdf_rename['e11_date']).dt.days\n",
    "\n",
    "def check_diab_criteria(row):\n",
    "    if row['e10_time'] > 0 or row['e11_time'] > 0 or '1222' in row['sr_illness'] or '1223' in row['sr_illness']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "pdf_rename['diab_yes'] = pdf_rename.apply(check_diab_criteria, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create comorbidity covariate for analysis and filter covariate for baseline characteristics table considering set of comorbidities created above\n",
    "pdf_rename['comorb_sum'] = pdf_rename[['htn_yes', 'hf_yes', 'cad_yes', 'cancer_yes', 'hchol_yes', 'diab_yes']].sum(axis=1)\n",
    "pdf_rename['comorb_sum_filter'] = pdf_rename['comorb_sum'].apply(lambda x: 0 if x == 0 else (1 if x == 1 else 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#prep clean cevd endpoint variable as first day in px chart with CeVD diagnosis after accelerometer study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#if assessment date - event date < 0, then copy absolute value to corresponding column\n",
    "pdf_rename.loc[pdf_rename['f01_time'] < 0, 'f01_t2e'] = abs(pdf_rename['f01_time'])\n",
    "pdf_rename.loc[pdf_rename['g45_time'] < 0, 'g45_t2e'] = abs(pdf_rename['g45_time'])\n",
    "pdf_rename.loc[pdf_rename['g46_time'] < 0, 'g46_t2e'] = abs(pdf_rename['g46_time'])\n",
    "pdf_rename.loc[pdf_rename['h34_time'] < 0, 'h34_t2e'] = abs(pdf_rename['h34_time'])\n",
    "pdf_rename.loc[pdf_rename['i60_time'] < 0, 'i60_t2e'] = abs(pdf_rename['i60_time'])\n",
    "pdf_rename.loc[pdf_rename['i61_time'] < 0, 'i61_t2e'] = abs(pdf_rename['i61_time'])\n",
    "pdf_rename.loc[pdf_rename['i62_time'] < 0, 'i62_t2e'] = abs(pdf_rename['i62_time'])\n",
    "pdf_rename.loc[pdf_rename['i63_time'] < 0, 'i63_t2e'] = abs(pdf_rename['i63_time'])\n",
    "pdf_rename.loc[pdf_rename['i64_time'] < 0, 'i64_t2e'] = abs(pdf_rename['i64_time'])\n",
    "pdf_rename.loc[pdf_rename['i65_time'] < 0, 'i65_t2e'] = abs(pdf_rename['i65_time'])\n",
    "pdf_rename.loc[pdf_rename['i66_time'] < 0, 'i66_t2e'] = abs(pdf_rename['i66_time'])\n",
    "pdf_rename.loc[pdf_rename['i67_time'] < 0, 'i67_t2e'] = abs(pdf_rename['i67_time'])\n",
    "pdf_rename.loc[pdf_rename['i68_time'] < 0, 'i68_t2e'] = abs(pdf_rename['i68_time'])\n",
    "pdf_rename.loc[pdf_rename['i69_time'] < 0, 'i69_t2e'] = abs(pdf_rename['i69_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#if there is a time-to-event in XXX_t2e, then place 1 in outcome column for each CeVD\n",
    "pdf_rename.loc[pdf_rename['f01_t2e'].notna(), 'f01_event'] = 1\n",
    "pdf_rename.loc[pdf_rename['g45_t2e'].notna(), 'g45_event'] = 1\n",
    "pdf_rename.loc[pdf_rename['g46_t2e'].notna(), 'g46_event'] = 1\n",
    "pdf_rename.loc[pdf_rename['h34_t2e'].notna(), 'h34_event'] = 1\n",
    "pdf_rename.loc[pdf_rename['i60_t2e'].notna(), 'i60_event'] = 1\n",
    "pdf_rename.loc[pdf_rename['i61_t2e'].notna(), 'i61_event'] = 1\n",
    "pdf_rename.loc[pdf_rename['i62_t2e'].notna(), 'i62_event'] = 1\n",
    "pdf_rename.loc[pdf_rename['i63_t2e'].notna(), 'i63_event'] = 1\n",
    "pdf_rename.loc[pdf_rename['i64_t2e'].notna(), 'i64_event'] = 1\n",
    "pdf_rename.loc[pdf_rename['i65_t2e'].notna(), 'i65_event'] = 1\n",
    "pdf_rename.loc[pdf_rename['i66_t2e'].notna(), 'i66_event'] = 1\n",
    "pdf_rename.loc[pdf_rename['i67_t2e'].notna(), 'i67_event'] = 1\n",
    "pdf_rename.loc[pdf_rename['i68_t2e'].notna(), 'i68_event'] = 1\n",
    "pdf_rename.loc[pdf_rename['i69_t2e'].notna(), 'i69_event'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CeVD events (total) 14508.0\n"
     ]
    }
   ],
   "source": [
    "#create composite cevd outcome value\n",
    "pdf_rename.loc[(pdf_rename['f01_event'].notna()) | (pdf_rename['g45_event'].notna()) | (pdf_rename['g46_event'].notna()) | (pdf_rename['h34_event'].notna()) | (pdf_rename['i60_event'].notna()) | (pdf_rename['i61_event'].notna()) | (pdf_rename['i62_event'].notna()) | (pdf_rename['i63_event'].notna()) | (pdf_rename['i64_event'].notna()) | (pdf_rename['i65_event'].notna()) | (pdf_rename['i66_event'].notna()) | (pdf_rename['i67_event'].notna()) | (pdf_rename['i68_event'].notna()) | (pdf_rename['i69_event'].notna()), 'cevd_event'] = 1\n",
    "pdf_rename['cevd_event'] = pdf_rename['cevd_event'].fillna(0)\n",
    "\n",
    "#check number of future cevd_events \n",
    "sum_cevdevents = pdf_rename['cevd_event'].sum()\n",
    "print('Number of CeVD events (total)', sum_cevdevents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create cevd time-to-event value for earliest cevd event\n",
    "pdf_rename['cevd_t2e'] = pdf_rename[['f01_t2e', 'g45_t2e', 'g46_t2e', 'h34_t2e', 'i60_t2e', 'i61_t2e', 'i62_t2e', 'i63_t2e', 'i64_t2e', 'i65_t2e', 'i66_t2e', 'i67_t2e', 'i68_t2e', 'i69_t2e']].min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#convert all t2e columns to numeric for filtering\n",
    "def convert_t2e_to_numeric(df):\n",
    "    df['f01_t2e'] = pd.to_numeric(df['f01_t2e'], errors='coerce').astype(pd.Int64Dtype())\n",
    "    df['g45_t2e'] = pd.to_numeric(df['g45_t2e'], errors='coerce').astype(pd.Int64Dtype())\n",
    "    df['g46_t2e'] = pd.to_numeric(df['g46_t2e'], errors='coerce').astype(pd.Int64Dtype())\n",
    "    df['h34_t2e'] = pd.to_numeric(df['h34_t2e'], errors='coerce').astype(pd.Int64Dtype())\n",
    "    df['i60_t2e'] = pd.to_numeric(df['i60_t2e'], errors='coerce').astype(pd.Int64Dtype())\n",
    "    df['i61_t2e'] = pd.to_numeric(df['i61_t2e'], errors='coerce').astype(pd.Int64Dtype())\n",
    "    df['i62_t2e'] = pd.to_numeric(df['i62_t2e'], errors='coerce').astype(pd.Int64Dtype())\n",
    "    df['i63_t2e'] = pd.to_numeric(df['i63_t2e'], errors='coerce').astype(pd.Int64Dtype())\n",
    "    df['i64_t2e'] = pd.to_numeric(df['i64_t2e'], errors='coerce').astype(pd.Int64Dtype())\n",
    "    df['i65_t2e'] = pd.to_numeric(df['i65_t2e'], errors='coerce').astype(pd.Int64Dtype())\n",
    "    df['i66_t2e'] = pd.to_numeric(df['i66_t2e'], errors='coerce').astype(pd.Int64Dtype())\n",
    "    df['i67_t2e'] = pd.to_numeric(df['i67_t2e'], errors='coerce').astype(pd.Int64Dtype())\n",
    "    df['i68_t2e'] = pd.to_numeric(df['i68_t2e'], errors='coerce').astype(pd.Int64Dtype())\n",
    "    df['i69_t2e'] = pd.to_numeric(df['i69_t2e'], errors='coerce').astype(pd.Int64Dtype())\n",
    "    df['cevd_t2e'] = pd.to_numeric(df['cevd_t2e'], errors='coerce').astype(pd.Int64Dtype())\n",
    "    return df\n",
    "\n",
    "pdf_rename = convert_t2e_to_numeric(pdf_rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#clean all cevd types to only have t2e for the initial CeVD diagnosis after accelerometer study commencement\n",
    "def check_cevd_types(df):\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.notna(row['f01_t2e']):\n",
    "            if row['f01_t2e'] != row['cevd_t2e']:\n",
    "                df.at[index, 'f01_t2e'] = None\n",
    "        if pd.notna(row['g45_t2e']):\n",
    "            if row['g45_t2e'] != row['cevd_t2e']:\n",
    "                 df.at[index, 'g45_t2e'] = None\n",
    "        if pd.notna(row['g46_t2e']):\n",
    "            if row['g46_t2e'] != row['cevd_t2e']:\n",
    "                 df.at[index, 'g46_t2e'] = None\n",
    "        if pd.notna(row['h34_t2e']):\n",
    "            if row['h34_t2e'] != row['cevd_t2e']:\n",
    "                 df.at[index, 'h34_t2e'] = None\n",
    "        if pd.notna(row['i60_t2e']):\n",
    "            if row['i60_t2e'] != row['cevd_t2e']:\n",
    "                 df.at[index, 'i60_t2e'] = None\n",
    "        if pd.notna(row['i61_t2e']):\n",
    "            if row['i61_t2e'] != row['cevd_t2e']:\n",
    "                 df.at[index, 'i61_t2e'] = None\n",
    "        if pd.notna(row['i62_t2e']):\n",
    "            if row['i62_t2e'] != row['cevd_t2e']:\n",
    "                 df.at[index, 'i62_t2e'] = None\n",
    "        if pd.notna(row['i63_t2e']):\n",
    "            if row['i63_t2e'] != row['cevd_t2e']:\n",
    "                 df.at[index, 'i63_t2e'] = None\n",
    "        if pd.notna(row['i64_t2e']):\n",
    "            if row['i64_t2e'] != row['cevd_t2e']:\n",
    "                 df.at[index, 'i64_t2e'] = None\n",
    "        if pd.notna(row['i65_t2e']):\n",
    "            if row['i65_t2e'] != row['cevd_t2e']:\n",
    "                 df.at[index, 'i65_t2e'] = None\n",
    "        if pd.notna(row['i66_t2e']):\n",
    "            if row['i66_t2e'] != row['cevd_t2e']:\n",
    "                 df.at[index, 'i66_t2e'] = None\n",
    "        if pd.notna(row['i67_t2e']):\n",
    "            if row['i67_t2e'] != row['cevd_t2e']:\n",
    "                 df.at[index, 'i67_t2e'] = None\n",
    "        if pd.notna(row['i68_t2e']):\n",
    "            if row['i68_t2e'] != row['cevd_t2e']:\n",
    "                 df.at[index, 'i68_t2e'] = None\n",
    "        if pd.notna(row['i69_t2e']):\n",
    "            if row['i69_t2e'] != row['cevd_t2e']:\n",
    "                 df.at[index, 'i69_t2e'] = None\n",
    "    return df\n",
    "\n",
    "cevd_df_first = check_cevd_types(pdf_rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows with missing BMI data: 1215\n",
      "number of rows with missing TDI data: 427\n"
     ]
    }
   ],
   "source": [
    "#get the number of missing BMI values for imputation\n",
    "missing_bmi = cevd_df_first['bmi'].isna().sum()\n",
    "print('number of rows with missing BMI data:', missing_bmi)\n",
    "#get the number of missing BMI values for imputation\n",
    "missing_tdi = cevd_df_first['tdi'].isna().sum()\n",
    "print('number of rows with missing TDI data:', missing_tdi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#USE MUTIPLE CHAIN IMPUTATION FOR MISSING BMI DATA\n",
    "cols_to_impute = ['bmi', 'tdi']\n",
    "# create an instance of the IterativeImputer class with MICE algorithm\n",
    "imputer = IterativeImputer(max_iter=10, sample_posterior=True)\n",
    "# impute the selected columns using multiple chains\n",
    "cevd_df_first[cols_to_impute] = imputer.fit_transform(cevd_df_first[cols_to_impute])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create a value with the day of intitial CeVD diagnosis after accelerometer study commencement\n",
    "def find_firstday_cevd(row):\n",
    "    dates = [row['f01_date'], row['g45_date'], row['g46_date'], row['h34_date'], row['i60_date'], row['i61_date'], row['i62_date'], row['i63_date'], row['i64_date'], row['i65_date'], row['i66_date'], row['i67_date'], row['i68_date'], row['i69_date']]\n",
    "    non_empty_dates = [date for date in dates if not pd.isnull(date)]\n",
    "    return min(non_empty_dates) if non_empty_dates else pd.NaT\n",
    "\n",
    "cevd_df_first['cevd_day'] = cevd_df_first.apply(find_firstday_cevd, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confirming the df length is 307,008 (complete) after wrangling 307008\n"
     ]
    }
   ],
   "source": [
    "print('confirming the df length is 307,008 (complete) after wrangling', len(cevd_df_first))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patients in cohort with a future CeVD: 14508\n"
     ]
    }
   ],
   "source": [
    "#check the number of px with a future CeVD in cohort\n",
    "total_futurecevd = cevd_df_first[cevd_df_first['cevd_event']==1]\n",
    "print('Number of patients in cohort with a future CeVD:', len(total_futurecevd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create screen use covariate in multiple steps\n",
    "screen_column = ['computer', 'tv', 'mobile phone']\n",
    "cevd_df_first['tv'] = cevd_df_first['tv'].replace(-10, 0.5)\n",
    "cevd_df_first['computer'] = cevd_df_first['computer'].replace(-10, 0.5)\n",
    "cevd_df_first[screen_column] = cevd_df_first[screen_column].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reorder mobile phone use per week values \n",
    "#convert to hours or percentage of hours tied to \n",
    "cevd_df_first['mobile phone_filter'] = np.where(cevd_df_first['mobile phone'].isin([5]), 6, \n",
    "                    np.where(cevd_df_first['mobile phone'].isin([4]), 4,\n",
    "                    np.where(cevd_df_first['mobile phone'].isin([3]), 2,\n",
    "                    np.where(cevd_df_first['mobile phone'].isin([2]), 0.742, \n",
    "                    np.where(cevd_df_first['mobile phone'].isin([1]), 0.2, \n",
    "                             np.where(cevd_df_first['mobile phone'].isin([0]), 0.083, np.nan))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    6.285714\n",
       "7    3.142857\n",
       "8    7.142857\n",
       "9    3.142857\n",
       "Name: full_screen_time, dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#screentime per day\n",
    "cevd_df_first['full_screen_time'] = (cevd_df_first['computer']*1) + (cevd_df_first['tv']*1) + (cevd_df_first['mobile phone']/7)\n",
    "cevd_df_first['full_screen_time'].head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of non-future CeVD px: 292500\n"
     ]
    }
   ],
   "source": [
    "total_futurecevd = cevd_df_first[cevd_df_first['cevd_event']==0]\n",
    "print('number of non-future CeVD px:', len(total_futurecevd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#screen time per day filter; greater than 2 = 0\n",
    "def assign_value(x):\n",
    "    if x <= 2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "cevd_df_first['full_screen_time_filter'] = cevd_df_first['full_screen_time'].apply(assign_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of px in first part of longitudinal analysis: 14508\n"
     ]
    }
   ],
   "source": [
    "cevd_df_first_onlycevd = cevd_df_first.dropna(subset=['cevd_day'])\n",
    "print('Number of px in first part of longitudinal analysis:', len(cevd_df_first_onlycevd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#to csv for dataframe for baseline cross-sectional analysis\n",
    "cevd_df_first_onlycevd.to_csv('cevd_first_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create second part of longitudinal analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "624"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cevd_df_second = cevd_df_first_onlycevd.dropna(subset=['date_i1'])\n",
    "len(cevd_df_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "624"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cevd_df_second = cevd_df_second.dropna(subset=['phq2_1_followup'])\n",
    "len(cevd_df_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "624"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cevd_df_second['cevd_day'] = pd.to_datetime(cevd_df_second['cevd_day'])\n",
    "cevd_df_second['cevd_before_i1'] = np.where(cevd_df_second['cevd_day'] < cevd_df_second['date_i1'], 1, 0)\n",
    "len(cevd_df_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "623"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cevd_df_second = cevd_df_second.drop(cevd_df_second.loc[cevd_df_second['phq2_1_followup'] < -1].index)\n",
    "cevd_df_second['phq2_1_followup'] = cevd_df_second['phq2_1_followup'].replace(-1, 0)\n",
    "len(cevd_df_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cevd_df_second['met_minweek_1'] = pd.to_numeric(cevd_df_second['met_minweek_1'], errors='coerce')\n",
    "#cevd_df_second['met_minweek_filter_1'] = cevd_df_second['met_minweek_1']/7\n",
    "#cevd_df_second['met_minweek_filter_1'] = cevd_df_second['met_minweek_filter_1']/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#convert sr_illness to string\n",
    "cevd_df_second['sr_illness_1'].dtype\n",
    "cevd_df_second['sr_illness_1'] = cevd_df_second['sr_illness_1'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after dropping px with missing sleep survey data 622\n"
     ]
    }
   ],
   "source": [
    "#drop -1 (\"do not know\") and -3 (\"prefer not to answer\") from sleep duration survey\n",
    "cevd_df_second = cevd_df_second[~(cevd_df_second['sleep_1'] == -3)]\n",
    "cevd_df_second = cevd_df_second[~(cevd_df_second['sleep_1'] == -1)]\n",
    "print('after dropping px with missing sleep survey data', len(cevd_df_second))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after dropping px with missing smoking survey data 620\n"
     ]
    }
   ],
   "source": [
    "#for smoking status, drop anyone who answered \"-3/prefer not to answer\"\n",
    "cevd_df_second['smok_stat_1'] = cevd_df_second['smok_stat_1'].astype(str)\n",
    "cevd_df_second = cevd_df_second[~cevd_df_second['smok_stat_1'].str.contains('-3')]\n",
    "print('after dropping px with missing smoking survey data', len(cevd_df_second))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after dropping px with incomplete screen surveys data 618\n",
      "after dropping px with missing screen survey data 603\n"
     ]
    }
   ],
   "source": [
    "#drop -1 and -3 from tv and computer and mobile phone use; fix tv use value of -10\n",
    "cevd_df_second = cevd_df_second[~(cevd_df_second['tv_1'] == -3)]\n",
    "cevd_df_second = cevd_df_second[~(cevd_df_second['tv_1'] == -1)]\n",
    "cevd_df_second = cevd_df_second[~(cevd_df_second['computer_1'] == -3)]\n",
    "cevd_df_second = cevd_df_second[~(cevd_df_second['computer_1'] == -1)]\n",
    "cevd_df_second = cevd_df_second[~(cevd_df_second['mobile phone_1'] == -3)]\n",
    "cevd_df_second = cevd_df_second[~(cevd_df_second['mobile phone_1'] == -1)]\n",
    "cevd_df_second['tv_1'] = cevd_df_second['tv_1'].replace(-10, 0.5)\n",
    "print('after dropping px with incomplete screen surveys data', len(cevd_df_second))\n",
    "cevd_df_second.dropna(subset=['computer_1', 'tv_1', 'mobile phone_1'], inplace=True)\n",
    "print('after dropping px with missing screen survey data', len(cevd_df_second))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after dropping px with incomplete alcohol survey data 603\n"
     ]
    }
   ],
   "source": [
    "#drop alc_freq values for \"prefer not to answer\"/-3\n",
    "cevd_df_second = cevd_df_second[~(cevd_df_second['alc_freq_1'] == -3)]\n",
    "print('after dropping px with incomplete alcohol survey data', len(cevd_df_second))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reorder alc_freq values \n",
    "#if 4\tOne to three times a month | 5\tSpecial occasions only | 6\tNever, then make seldom=1\n",
    "#if 2\tThree or four times a week | 3\tOnce or twice a week, then make sometimes=2\n",
    "#if 1\tDaily or almost daily, then make daily=3\n",
    "cevd_df_second['alc_freq_filter_1'] = np.where(cevd_df_second['alc_freq_1'].isin([4, 5, 6]), 1, \n",
    "                    np.where(cevd_df_second['alc_freq_1'].isin([2, 3]), 2, \n",
    "                             np.where(cevd_df_second['alc_freq_1'].isin([1]), 3, np.nan)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "603"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop phq2 scores where response was \"-3/prefer not to answer\"\n",
    "cevd_df_second = cevd_df_second.drop(cevd_df_second.loc[cevd_df_second['phq2_1_followup'] < -1].index)\n",
    "len(cevd_df_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "603"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace all occurrences of -1 (do not know if depressed) in column PHQ2_1 and PHQ2_2 with 0\n",
    "cevd_df_second['phq2_1'] = cevd_df_second['phq2_1'].replace(-1, 0)\n",
    "len(cevd_df_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create screen use covariate in multiple steps\n",
    "screen_column_1 = ['computer_1', 'tv_1', 'mobile phone_1']\n",
    "cevd_df_second['tv_1'] = cevd_df_second['tv_1'].replace(-10, 0.5)\n",
    "cevd_df_second['computer_1'] = cevd_df_second['computer_1'].replace(-10, 0.5)\n",
    "cevd_df_second[screen_column_1] = cevd_df_second[screen_column_1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reorder mobile phone use per week values \n",
    "#convert to hours or percentage of hours tied to \n",
    "cevd_df_second['mobile phone_filter_1'] = np.where(cevd_df_second['mobile phone_1'].isin([5]), 6, \n",
    "                    np.where(cevd_df_second['mobile phone_1'].isin([4]), 4,\n",
    "                    np.where(cevd_df_second['mobile phone_1'].isin([3]), 2,\n",
    "                    np.where(cevd_df_second['mobile phone_1'].isin([2]), 0.742, \n",
    "                    np.where(cevd_df_second['mobile phone_1'].isin([1]), 0.2, \n",
    "                             np.where(cevd_df_second['mobile phone_1'].isin([0]), 0.083, np.nan))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#screentime per day\n",
    "cevd_df_second['full_screen_time_1'] = (cevd_df_second['computer_1']*1) + (cevd_df_second['tv_1']*1) + (cevd_df_second['mobile phone_1']/7)\n",
    "cevd_df_second['full_screen_time_1'].head(4)\n",
    "#screen time per day filter; greater than 2 = 0\n",
    "def assign_value(x):\n",
    "    if x <= 2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "cevd_df_second['full_screen_time_filter_1'] = cevd_df_second['full_screen_time_1'].apply(assign_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cevd_df_second[\"date_i1\"] = pd.to_datetime(cevd_df_second[\"date_i1\"])\n",
    "#create hypertension category\n",
    "#prep htn cases\n",
    "cevd_df_second['i10_time'] = (cevd_df_second['date_i1'] - cevd_df_second['i10_date']).dt.days\n",
    "cevd_df_second['i15_time'] = (cevd_df_second['date_i1'] - cevd_df_second['i15_date']).dt.days\n",
    "\n",
    "# check for htn; if yes, return 1\n",
    "def check_htn_criteria(row):\n",
    "    if row['i10_time'] > 0 or row['i15_time'] > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "cevd_df_second['htn_yes_1'] = cevd_df_second.apply(check_htn_criteria, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create heart failure category\n",
    "#prep  cases\n",
    "cevd_df_second['i50_time'] = (cevd_df_second['date_i1'] - cevd_df_second['i50_date']).dt.days\n",
    "\n",
    "# check for htn; if yes, return 1\n",
    "def check_htn_criteria(row):\n",
    "    if row['i50_time'] > 0  or '1076' in row['sr_illness_1']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "cevd_df_second['hf_yes_1'] = cevd_df_second.apply(check_htn_criteria, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create cad category\n",
    "#prep  cases\n",
    "cevd_df_second['i25_time'] = (cevd_df_second['date_i1'] - cevd_df_second['i25_date']).dt.days\n",
    "\n",
    "# check for htn; if yes, return 1\n",
    "def check_htn_criteria(row):\n",
    "    if row['i25_time'] > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "cevd_df_second['cad_yes_1'] = cevd_df_second.apply(check_htn_criteria, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create cancer covariate\n",
    "cevd_df_second['cancer_yes_1'] = cevd_df_second['p134_i1'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create hyperlipidemia category\n",
    "#prep hchol cases\n",
    "cevd_df_second['e78_time'] = (cevd_df_second['date_i1'] - cevd_df_second['e78_date']).dt.days\n",
    "def check_hchol_criteria(row):\n",
    "    if row['e78_time'] > 0 or '1473' in row['sr_illness_1']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "cevd_df_second['hchol_yes_1'] = cevd_df_second.apply(check_hchol_criteria, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create diabetes category\n",
    "#prep diabetes cases\n",
    "cevd_df_second['e10_date'] = pd.to_datetime(cevd_df_second['e10_date'])\n",
    "cevd_df_second['e11_date'] = pd.to_datetime(cevd_df_second['e11_date'])\n",
    "cevd_df_second['e10_time'] = (cevd_df_second['date_i0'] - cevd_df_second['e10_date']).dt.days\n",
    "cevd_df_second['e11_time'] = (cevd_df_second['date_i0'] - cevd_df_second['e11_date']).dt.days\n",
    "\n",
    "def check_diab_criteria(row):\n",
    "    if row['e10_time'] > 0 or row['e11_time'] > 0 or '1222' in row['sr_illness_1'] or '1223' in row['sr_illness_1']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "cevd_df_second['diab_yes_1'] = cevd_df_second.apply(check_diab_criteria, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create comorbidity covariate for analysis and filter covariate for baseline characteristics table considering set of comorbidities created above\n",
    "cevd_df_second['comorb_sum_1'] = cevd_df_second[['htn_yes_1', 'hf_yes_1', 'cad_yes_1', 'cancer_yes_1', 'hchol_yes_1', 'diab_yes_1']].sum(axis=1)\n",
    "cevd_df_second['comorb_sum_filter_1'] = cevd_df_second['comorb_sum_1'].apply(lambda x: 0 if x == 0 else (1 if x == 1 else 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of px with depression diagnosis (F31, F32, F33): 64\n"
     ]
    }
   ],
   "source": [
    "#prep depression cases in \"days\" format\n",
    "cevd_df_second['f31_time'] = (cevd_df_second['date_i1'] - cevd_df_second['f31_date']).dt.days\n",
    "cevd_df_second['f32_time'] = (cevd_df_second['date_i1'] - cevd_df_second['f32_date']).dt.days\n",
    "cevd_df_second['f33_time'] = (cevd_df_second['date_i1'] - cevd_df_second['f33_date']).dt.days\n",
    "# check for diagnosed depression; if yes, return 1\n",
    "def check_for_dep_diag(row):\n",
    "    if row['f31_time'] > 0 or row['f32_time'] > 0 or row['f33_time'] > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "cevd_df_second['dep_diag_yes_1'] = cevd_df_second.apply(check_for_dep_diag, axis=1)\n",
    "check_dep_diag = cevd_df_second['dep_diag_yes_1'].sum()\n",
    "print(\"Number of px with depression diagnosis (F31, F32, F33):\", check_dep_diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#to csv for dataframe for follow-up cross-sectional analysis\n",
    "cevd_df_second.to_csv('cevd_second_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of participants in baseline dataset: 14508\n",
      "Number of participants in baseline dataset w/o depression: 13106\n",
      "Number of participants in baseline dataset w/depression: 1402\n",
      "Number of participants in follow-up dataset: 603\n",
      "Number of participants in follow-up dataset w/depression: 64\n",
      "Number of participants in follow-up dataset w/o depression: 539\n",
      "Number of participants in follow-up dataset w/cevd: 603\n",
      "Number of participants in follow-up dataset w/cevd w/dep: 16\n",
      "Number of participants in follow-up dataset w/cevd w/o dep: 105\n"
     ]
    }
   ],
   "source": [
    "print('Number of participants in baseline dataset:', len(cevd_df_first_onlycevd))\n",
    "print('Number of participants in baseline dataset w/o depression:', len(cevd_df_first_onlycevd[(cevd_df_first_onlycevd['dep_diag_yes'] == 0)]))\n",
    "print('Number of participants in baseline dataset w/depression:', len(cevd_df_first_onlycevd[(cevd_df_first_onlycevd['dep_diag_yes'] == 1)]))\n",
    "print('Number of participants in follow-up dataset:', len(cevd_df_second))\n",
    "print('Number of participants in follow-up dataset w/depression:', len(cevd_df_second[(cevd_df_second['dep_diag_yes_1'] == 1)]))\n",
    "print('Number of participants in follow-up dataset w/o depression:', len(cevd_df_second[(cevd_df_second['dep_diag_yes_1'] == 0)]))\n",
    "print('Number of participants in follow-up dataset w/cevd:', len(cevd_df_second['cevd_before_i1'] == 1))\n",
    "print('Number of participants in follow-up dataset w/cevd w/dep:', len(cevd_df_second[(cevd_df_second['cevd_before_i1'] == 1) & (cevd_df_second['dep_diag_yes_1'] == 1)]))\n",
    "print('Number of participants in follow-up dataset w/cevd w/o dep:', len(cevd_df_second[(cevd_df_second['cevd_before_i1'] == 1) & (cevd_df_second['dep_diag_yes_1'] == 0)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of px with F31: 38\n"
     ]
    }
   ],
   "source": [
    "def count_f31(df):\n",
    "    new_df = pd.DataFrame(columns=df.columns) \n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        value_1 = row['f31_time']\n",
    "        value_2 = row['f32_time']\n",
    "        value_3 = row['f33_time']\n",
    "        \n",
    "        if value_1 > 0 and (pd.isna(value_2) or value_2 < 0) and (pd.isna(value_3) or value_3 < 0):\n",
    "            new_df = pd.concat([new_df, pd.DataFrame([row], columns=df.columns)], ignore_index=True)\n",
    "    \n",
    "    new_df.reset_index(drop=True, inplace=True) \n",
    "    return new_df\n",
    "\n",
    "count_f31 = count_f31(cevd_df_first_onlycevd)\n",
    "print('Number of px with F31:', len(count_f31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of px with F32: 1240\n"
     ]
    }
   ],
   "source": [
    "def count_f32(df):\n",
    "    new_df = pd.DataFrame(columns=df.columns) \n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        value_1 = row['f31_time']\n",
    "        value_2 = row['f32_time']\n",
    "        value_3 = row['f33_time']\n",
    "        \n",
    "        if value_2 > 0 and (pd.isna(value_1) or value_1 < 0) and (pd.isna(value_3) or value_3 < 0):\n",
    "            new_df = pd.concat([new_df, pd.DataFrame([row], columns=df.columns)], ignore_index=True)\n",
    "    \n",
    "    new_df.reset_index(drop=True, inplace=True)  # Reset the index of the new DataFrame\n",
    "    return new_df\n",
    "\n",
    "count_f32 = count_f32(cevd_df_first_onlycevd)\n",
    "print('Number of px with F32:', len(count_f32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of px with F33: 26\n"
     ]
    }
   ],
   "source": [
    "def count_f33(df):\n",
    "    new_df = pd.DataFrame(columns=df.columns) \n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        value_1 = row['f31_time']\n",
    "        value_2 = row['f32_time']\n",
    "        value_3 = row['f33_time']\n",
    "        \n",
    "        if value_3 > 0 and (pd.isna(value_1) or value_1 < 0) and (pd.isna(value_2) or value_2 < 0):\n",
    "            new_df = pd.concat([new_df, pd.DataFrame([row], columns=df.columns)], ignore_index=True)\n",
    "    \n",
    "    new_df.reset_index(drop=True, inplace=True)  # Reset the index of the new DataFrame\n",
    "    return new_df\n",
    "\n",
    "count_f33 = count_f33(cevd_df_first_onlycevd)\n",
    "print('Number of px with F33:', len(count_f33))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get baseline characteristics table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14508"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cevd_df_first_onlycevd[cevd_df_first_onlycevd['cevd_event'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Age with exp cohort: 58.86519258202568\n",
      "Standard Deviation of Age with exp cohort: 6.96762787876629\n",
      "P-value for tdi:\n",
      "Mean Age with control cohort: 60.66641233023043\n",
      "Standard Deviation of Age with control cohort: 6.788984697060469\n",
      "P-value for tdi:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MannwhitneyuResult(statistic=210482064.0, pvalue=0.0)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = cevd_df_first_onlycevd[cevd_df_first_onlycevd['dep_diag_yes'] == 1]\n",
    "mean_age = subset['age_i0'].mean()\n",
    "std_dev_age = subset['age_i0'].std()\n",
    "\n",
    "print(\"Mean Age with exp cohort:\", mean_age)\n",
    "print(\"Standard Deviation of Age with exp cohort:\", std_dev_age)\n",
    "print('P-value for tdi:')\n",
    "stats.mannwhitneyu(x=cevd_df_first_onlycevd['age_i0'], y=cevd_df_first_onlycevd['dep_diag_yes'], alternative = 'two-sided')\n",
    "subset = cevd_df_first_onlycevd[cevd_df_first_onlycevd['dep_diag_yes'] == 0]\n",
    "mean_age = subset['age_i0'].mean()\n",
    "std_dev_age = subset['age_i0'].std()\n",
    "\n",
    "print(\"Mean Age with control cohort:\", mean_age)\n",
    "print(\"Standard Deviation of Age with control cohort:\", std_dev_age)\n",
    "print('P-value for tdi:')\n",
    "stats.mannwhitneyu(x=cevd_df_first_onlycevd['age_i0'], y=cevd_df_first_onlycevd['dep_diag_yes'], alternative = 'two-sided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Age with exp cohort: 61.359375\n",
      "Standard Deviation of Age with exp cohort: 7.284179829853928\n",
      "P-value for tdi:\n",
      "Mean Age with control cohort: 64.87408759124088\n",
      "Standard Deviation of Age with control cohort: 6.3306676514964195\n",
      "P-value for tdi:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MannwhitneyuResult(statistic=363609.0, pvalue=6.172571257401554e-218)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = cevd_df_second[cevd_df_second['dep_diag_yes_1'] == 1]\n",
    "mean_age = subset['age_i1'].mean()\n",
    "std_dev_age = subset['age_i1'].std()\n",
    "\n",
    "print(\"Mean Age with exp cohort:\", mean_age)\n",
    "print(\"Standard Deviation of Age with exp cohort:\", std_dev_age)\n",
    "print('P-value for tdi:')\n",
    "stats.mannwhitneyu(x=cevd_df_second['age_i1'], y=cevd_df_second['dep_diag_yes_1'], alternative = 'two-sided')\n",
    "subset = cevd_df_second[cevd_df_second['dep_diag_yes'] == 0]\n",
    "mean_age = subset['age_i1'].mean()\n",
    "std_dev_age = subset['age_i1'].std()\n",
    "\n",
    "print(\"Mean Age with control cohort:\", mean_age)\n",
    "print(\"Standard Deviation of Age with control cohort:\", std_dev_age)\n",
    "print('P-value for tdi:')\n",
    "stats.mannwhitneyu(x=cevd_df_second['age_i1'], y=cevd_df_second['dep_diag_yes_1'], alternative = 'two-sided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp Cohort Percent male: 41.72610556348074\n",
      "exp Cohort Total male: 585\n",
      "Control Cohort Percent male: 57.54616206317718\n",
      "Control Cohort Total male: 7542\n",
      "% of men in total 18,873 cohort 43.06151645207439\n",
      "MALE - Chi-square test statistic: 128.0125469757392\n",
      "MALE - P-value: 1.1153563679554194e-29\n"
     ]
    }
   ],
   "source": [
    "#get male/female data\n",
    "percent_male_d = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 1, 'male'].mean() * 100\n",
    "male_sum_d = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 1, 'male'].sum()\n",
    "print('exp Cohort Percent male:', percent_male_d)\n",
    "print('exp Cohort Total male:', male_sum_d)\n",
    "percent_male_c = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 0, 'male'].mean() * 100\n",
    "male_sum_c = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 0, 'male'].sum()\n",
    "print('Control Cohort Percent male:', percent_male_c)\n",
    "print('Control Cohort Total male:', male_sum_c)\n",
    "perc_men_total = ((male_sum_d+male_sum_c)/18873)*100\n",
    "print('% of men in total 18,873 cohort', perc_men_total)\n",
    "data = pd.crosstab(cevd_df_first_onlycevd['male'], cevd_df_first_onlycevd['dep_diag_yes']).to_numpy()\n",
    "# chi-square test for independence for male data\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(data)\n",
    "print('MALE - Chi-square test statistic:', chi2_stat)\n",
    "print('MALE - P-value:', p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp Cohort Percent male: 53.125\n",
      "exp Cohort Total male: 34\n",
      "Control Cohort Percent male: 63.07977736549165\n",
      "Control Cohort Total male: 340\n",
      "% of men in total 18,873 cohort 1.9816669315954007\n",
      "MALE - Chi-square test statistic: 2.0027357383020066\n",
      "MALE - P-value: 0.1570155916970494\n"
     ]
    }
   ],
   "source": [
    "#get male/female data\n",
    "percent_male_d = cevd_df_second.loc[cevd_df_second['dep_diag_yes_1'] == 1, 'male'].mean() * 100\n",
    "male_sum_d = cevd_df_second.loc[cevd_df_second['dep_diag_yes_1'] == 1, 'male'].sum()\n",
    "print('exp Cohort Percent male:', percent_male_d)\n",
    "print('exp Cohort Total male:', male_sum_d)\n",
    "percent_male_c = cevd_df_second.loc[cevd_df_second['dep_diag_yes_1'] == 0, 'male'].mean() * 100\n",
    "male_sum_c = cevd_df_second.loc[cevd_df_second['dep_diag_yes_1'] == 0, 'male'].sum()\n",
    "print('Control Cohort Percent male:', percent_male_c)\n",
    "print('Control Cohort Total male:', male_sum_c)\n",
    "perc_men_total = ((male_sum_d+male_sum_c)/18873)*100\n",
    "print('% of men in total 18,873 cohort', perc_men_total)\n",
    "data = pd.crosstab(cevd_df_second['male'], cevd_df_second['dep_diag_yes_1']).to_numpy()\n",
    "# chi-square test for independence for male data\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(data)\n",
    "print('MALE - Chi-square test statistic:', chi2_stat)\n",
    "print('MALE - P-value:', p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp Cohort Percent white: 97.21825962910128\n",
      "exp Cohort Total white: 1363.0\n",
      "Control Cohort Percent white: 95.11674042423317\n",
      "Control Cohort Total white: 12466.0\n",
      "% of white in total 76505 cohort 18.075942748839946\n",
      "white - Chi-square test statistic: 12.071456950721432\n",
      "white - P-value: 0.0005119967283486465\n"
     ]
    }
   ],
   "source": [
    "#get white/ethnicity data\n",
    "percent_male_d = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 1, 'white_yes'].mean() * 100\n",
    "male_sum_d = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 1, 'white_yes'].sum()\n",
    "print('exp Cohort Percent white:', percent_male_d)\n",
    "print('exp Cohort Total white:', male_sum_d)\n",
    "percent_male_c = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 0, 'white_yes'].mean() * 100\n",
    "male_sum_c = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 0, 'white_yes'].sum()\n",
    "print('Control Cohort Percent white:', percent_male_c)\n",
    "print('Control Cohort Total white:', male_sum_c)\n",
    "perc_men_total = ((male_sum_d+male_sum_c)/76505)*100\n",
    "print('% of white in total 76505 cohort', perc_men_total)\n",
    "data = pd.crosstab(cevd_df_first_onlycevd['white_yes'], cevd_df_first_onlycevd['dep_diag_yes']).to_numpy()\n",
    "# chi-square test for independence for male data\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(data)\n",
    "print('white - Chi-square test statistic:', chi2_stat)\n",
    "print('white - P-value:', p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp Cohort Percent white: 96.875\n",
      "exp Cohort Total white: 62.0\n",
      "Control Cohort Percent white: 98.33024118738405\n",
      "Control Cohort Total white: 530.0\n",
      "% of men in total 18,873 cohort 3.1367562125788164\n",
      "white - Chi-square test statistic: 0.10791030871755816\n",
      "white - P-value: 0.7425358780269807\n"
     ]
    }
   ],
   "source": [
    "#get male/female data\n",
    "percent_male_d = cevd_df_second.loc[cevd_df_second['dep_diag_yes_1'] == 1, 'white_yes'].mean() * 100\n",
    "male_sum_d = cevd_df_second.loc[cevd_df_second['dep_diag_yes_1'] == 1, 'white_yes'].sum()\n",
    "print('exp Cohort Percent white:', percent_male_d)\n",
    "print('exp Cohort Total white:', male_sum_d)\n",
    "percent_male_c = cevd_df_second.loc[cevd_df_second['dep_diag_yes_1'] == 0, 'white_yes'].mean() * 100\n",
    "male_sum_c = cevd_df_second.loc[cevd_df_second['dep_diag_yes_1'] == 0, 'white_yes'].sum()\n",
    "print('Control Cohort Percent white:', percent_male_c)\n",
    "print('Control Cohort Total white:', male_sum_c)\n",
    "perc_men_total = ((male_sum_d+male_sum_c)/18873)*100\n",
    "print('% of men in total 18,873 cohort', perc_men_total)\n",
    "data = pd.crosstab(cevd_df_second['white_yes'], cevd_df_second['dep_diag_yes_1']).to_numpy()\n",
    "# chi-square test for independence for male data\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(data)\n",
    "print('white - Chi-square test statistic:', chi2_stat)\n",
    "print('white - P-value:', p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp Percentages of smoke\n",
      "1    55.563481\n",
      "0    44.436519\n",
      "Name: smok_filter, dtype: float64\n",
      "CONTROL Percentages of smoke\n",
      "1    52.945216\n",
      "0    47.054784\n",
      "Name: smok_filter, dtype: float64\n",
      "smoke - Chi-square test statistic: 3.3828436559399275\n",
      "smoke - P-value: 0.06587829990440892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_151/770987846.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cevd_df_first_onlycevd['smok_stat'] = pd.to_numeric(cevd_df_first_onlycevd['smok_stat'], errors='coerce').astype(pd.Int64Dtype())\n",
      "/tmp/ipykernel_151/770987846.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cevd_df_first_onlycevd['smok_filter'] = cevd_df_first_onlycevd['smok_stat'].apply(lambda x: 1 if (x > 0) else 0)\n"
     ]
    }
   ],
   "source": [
    "#get smoking data\n",
    "cevd_df_first_onlycevd['smok_stat'] = pd.to_numeric(cevd_df_first_onlycevd['smok_stat'], errors='coerce').astype(pd.Int64Dtype())\n",
    "cevd_df_first_onlycevd['smok_filter'] = cevd_df_first_onlycevd['smok_stat'].apply(lambda x: 1 if (x > 0) else 0)\n",
    "perc_alc_freq_d = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 1, 'smok_filter'].value_counts(normalize=True)\n",
    "print('exp Percentages of smoke')\n",
    "print(perc_alc_freq_d * 100)\n",
    "perc_alc_freq_c = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 0, 'smok_filter'].value_counts(normalize=True)\n",
    "# print the value counts as percentages\n",
    "print('CONTROL Percentages of smoke')\n",
    "print(perc_alc_freq_c * 100)\n",
    "\n",
    "data = pd.crosstab(cevd_df_first_onlycevd['smok_filter'], cevd_df_first_onlycevd['dep_diag_yes']).to_numpy()\n",
    "# chi-square test for independence\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(data)\n",
    "print('smoke - Chi-square test statistic:', chi2_stat)\n",
    "print('smoke - P-value:', p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp Percentages of smoke\n",
      "0    62.5\n",
      "1    37.5\n",
      "Name: smok_filter_1, dtype: float64\n",
      "CONTROL Percentages of smoke\n",
      "0    53.246753\n",
      "1    46.753247\n",
      "Name: smok_filter_1, dtype: float64\n",
      "smoke - Chi-square test statistic: 1.6182204517460128\n",
      "smoke - P-value: 0.2033400988046694\n"
     ]
    }
   ],
   "source": [
    "#get smoking data\n",
    "cevd_df_second['smok_stat_1'] = pd.to_numeric(cevd_df_second['smok_stat_1'], errors='coerce').astype(pd.Int64Dtype())\n",
    "cevd_df_second['smok_filter_1'] = cevd_df_second['smok_stat_1'].apply(lambda x: 1 if (x > 0) else 0)\n",
    "perc_alc_freq_d = cevd_df_second.loc[cevd_df_second['dep_diag_yes_1'] == 1, 'smok_filter_1'].value_counts(normalize=True)\n",
    "print('exp Percentages of smoke')\n",
    "print(perc_alc_freq_d * 100)\n",
    "perc_alc_freq_c = cevd_df_second.loc[cevd_df_second['dep_diag_yes_1'] == 0, 'smok_filter_1'].value_counts(normalize=True)\n",
    "# print the value counts as percentages\n",
    "print('CONTROL Percentages of smoke')\n",
    "print(perc_alc_freq_c * 100)\n",
    "\n",
    "data = pd.crosstab(cevd_df_second['smok_filter_1'], cevd_df_second['dep_diag_yes_1']).to_numpy()\n",
    "# chi-square test for independence\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(data)\n",
    "print('smoke - Chi-square test statistic:', chi2_stat)\n",
    "print('smoke - P-value:', p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp Cohort Percent bmi: 31.526390870185452\n",
      "exp Cohort Total bmi: 442\n",
      "Control Cohort Percent bmi: 37.65450938501449\n",
      "Control Cohort Total bmi: 4935\n",
      "bmi - Chi-square test statistic: 20.12833984771519\n",
      "bmi - P-value: 7.241567990639089e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_151/1515765443.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cevd_df_first_onlycevd['bmi_filter'] = cevd_df_first_onlycevd['bmi'].apply(lambda x: 1 if (x > 18.4 and x < 26) else 0)\n"
     ]
    }
   ],
   "source": [
    "#get bmi data\n",
    "cevd_df_first_onlycevd['bmi_filter'] = cevd_df_first_onlycevd['bmi'].apply(lambda x: 1 if (x > 18.4 and x < 26) else 0)\n",
    "percent_male_d = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 1, 'bmi_filter'].mean() * 100\n",
    "male_sum_d = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 1, 'bmi_filter'].sum()\n",
    "print('exp Cohort Percent bmi:', percent_male_d)\n",
    "print('exp Cohort Total bmi:', male_sum_d)\n",
    "percent_male_c = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 0, 'bmi_filter'].mean() * 100\n",
    "male_sum_c = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 0, 'bmi_filter'].sum()\n",
    "print('Control Cohort Percent bmi:', percent_male_c)\n",
    "print('Control Cohort Total bmi:', male_sum_c)\n",
    "data = pd.crosstab(cevd_df_first_onlycevd['bmi_filter'], cevd_df_first_onlycevd['dep_diag_yes']).to_numpy()\n",
    "# chi-square test for independence for bmi data\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(data)\n",
    "print('bmi - Chi-square test statistic:', chi2_stat)\n",
    "print('bmi - P-value:', p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp Cohort Percent bmi: 21.818181818181817\n",
      "exp Cohort Total bmi: 12\n",
      "Control Cohort Percent bmi: 41.605839416058394\n",
      "Control Cohort Total bmi: 228\n",
      "bmi - Chi-square test statistic: 7.363302243627533\n",
      "bmi - P-value: 0.00665684232507151\n"
     ]
    }
   ],
   "source": [
    "#get bmi data\n",
    "cevd_df_second['bmi_filter_1'] = cevd_df_second['bmi_1'].apply(lambda x: 1 if (x > 18.4 and x < 26) else 0)\n",
    "percent_male_d = cevd_df_second.loc[cevd_df_second['dep_diag_yes'] == 1, 'bmi_filter_1'].mean() * 100\n",
    "male_sum_d = cevd_df_second.loc[cevd_df_second['dep_diag_yes'] == 1, 'bmi_filter_1'].sum()\n",
    "print('exp Cohort Percent bmi:', percent_male_d)\n",
    "print('exp Cohort Total bmi:', male_sum_d)\n",
    "percent_male_c = cevd_df_second.loc[cevd_df_second['dep_diag_yes'] == 0, 'bmi_filter_1'].mean() * 100\n",
    "male_sum_c = cevd_df_second.loc[cevd_df_second['dep_diag_yes'] == 0, 'bmi_filter_1'].sum()\n",
    "print('Control Cohort Percent bmi:', percent_male_c)\n",
    "print('Control Cohort Total bmi:', male_sum_c)\n",
    "data = pd.crosstab(cevd_df_second['bmi_filter_1'], cevd_df_second['dep_diag_yes']).to_numpy()\n",
    "# chi-square test for independence for bmi data\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(data)\n",
    "print('bmi - Chi-square test statistic:', chi2_stat)\n",
    "print('bmi - P-value:', p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp Cohort Percent sleep: 28.459343794579173\n",
      "exp Cohort Total sleep: 399\n",
      "Control Cohort Percent sleep: 29.497939874866475\n",
      "Control Cohort Total sleep: 3866\n",
      "sleep - Chi-square test statistic: 0.6091339672802981\n",
      "sleep - P-value: 0.435114073199547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_151/1927799745.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cevd_df_first_onlycevd['sleep_filter'] = cevd_df_first_onlycevd['sleep'].apply(lambda x: 1 if (x > 7 and x < 9) else 0)\n"
     ]
    }
   ],
   "source": [
    "#get bmi data\n",
    "cevd_df_first_onlycevd['sleep_filter'] = cevd_df_first_onlycevd['sleep'].apply(lambda x: 1 if (x > 7 and x < 9) else 0)\n",
    "percent_male_d = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 1, 'sleep_filter'].mean() * 100\n",
    "male_sum_d = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 1, 'sleep_filter'].sum()\n",
    "print('exp Cohort Percent sleep:', percent_male_d)\n",
    "print('exp Cohort Total sleep:', male_sum_d)\n",
    "percent_male_c = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 0, 'sleep_filter'].mean() * 100\n",
    "male_sum_c = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 0, 'sleep_filter'].sum()\n",
    "print('Control Cohort Percent sleep:', percent_male_c)\n",
    "print('Control Cohort Total sleep:', male_sum_c)\n",
    "data = pd.crosstab(cevd_df_first_onlycevd['sleep_filter'], cevd_df_first_onlycevd['dep_diag_yes']).to_numpy()\n",
    "# chi-square test for independence for bmi data\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(data)\n",
    "print('sleep - Chi-square test statistic:', chi2_stat)\n",
    "print('sleep - P-value:', p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp Cohort Percent sleep: 30.909090909090907\n",
      "exp Cohort Total sleep: 17\n",
      "Control Cohort Percent sleep: 37.22627737226277\n",
      "Control Cohort Total sleep: 204\n",
      "sleep - Chi-square test statistic: 0.6085755551780765\n",
      "sleep - P-value: 0.4353246434047967\n"
     ]
    }
   ],
   "source": [
    "#get bmi data\n",
    "cevd_df_second['sleep_filter_1'] = cevd_df_second['sleep_1'].apply(lambda x: 1 if (x > 7 and x < 9) else 0)\n",
    "percent_male_d = cevd_df_second.loc[cevd_df_second['dep_diag_yes'] == 1, 'sleep_filter_1'].mean() * 100\n",
    "male_sum_d = cevd_df_second.loc[cevd_df_second['dep_diag_yes'] == 1, 'sleep_filter_1'].sum()\n",
    "print('exp Cohort Percent sleep:', percent_male_d)\n",
    "print('exp Cohort Total sleep:', male_sum_d)\n",
    "percent_male_c = cevd_df_second.loc[cevd_df_second['dep_diag_yes'] == 0, 'sleep_filter_1'].mean() * 100\n",
    "male_sum_c = cevd_df_second.loc[cevd_df_second['dep_diag_yes'] == 0, 'sleep_filter_1'].sum()\n",
    "print('Control Cohort Percent sleep:', percent_male_c)\n",
    "print('Control Cohort Total sleep:', male_sum_c)\n",
    "data = pd.crosstab(cevd_df_second['sleep_filter_1'], cevd_df_second['dep_diag_yes']).to_numpy()\n",
    "# chi-square test for independence for bmi data\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(data)\n",
    "print('sleep - Chi-square test statistic:', chi2_stat)\n",
    "print('sleep - P-value:', p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean full_screen_time with exp cohort: 4.502241695536988\n",
      "Standard Deviation of full_screen_time with exp cohort: 2.5583438171403583\n",
      "P-value for full_screen_time:\n",
      "Mean full_screen_time with control cohort: 4.222809618277344\n",
      "Standard Deviation of full_screen_time with control cohort: 2.2231832764100337\n",
      "P-value for full_screen_time:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MannwhitneyuResult(statistic=209120125.0, pvalue=0.0)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = cevd_df_first_onlycevd[cevd_df_first_onlycevd['dep_diag_yes'] == 1]\n",
    "mean_age = subset['full_screen_time'].mean()\n",
    "std_dev_age = subset['full_screen_time'].std()\n",
    "\n",
    "print(\"Mean full_screen_time with exp cohort:\", mean_age)\n",
    "print(\"Standard Deviation of full_screen_time with exp cohort:\", std_dev_age)\n",
    "print('P-value for full_screen_time:')\n",
    "stats.mannwhitneyu(x=cevd_df_first_onlycevd['full_screen_time'], y=cevd_df_first_onlycevd['dep_diag_yes'], alternative = 'two-sided')\n",
    "subset = cevd_df_first_onlycevd[cevd_df_first_onlycevd['dep_diag_yes'] == 0]\n",
    "mean_age = subset['full_screen_time'].mean()\n",
    "std_dev_age = subset['full_screen_time'].std()\n",
    "\n",
    "print(\"Mean full_screen_time with control cohort:\", mean_age)\n",
    "print(\"Standard Deviation of full_screen_time with control cohort:\", std_dev_age)\n",
    "print('P-value for full_screen_time:')\n",
    "stats.mannwhitneyu(x=cevd_df_first_onlycevd['full_screen_time'], y=cevd_df_first_onlycevd['dep_diag_yes'], alternative = 'two-sided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean full_screen_time_1 with exp cohort: 4.486607142857142\n",
      "Standard Deviation of full_screen_time_1 with exp cohort: 2.2363893846452156\n",
      "P-value for full_screen_time_1:\n",
      "Mean full_screen_time_1 with control cohort: 4.538563477338988\n",
      "Standard Deviation of full_screen_time_1 with control cohort: 2.257320598908777\n",
      "P-value for full_screen_time_1:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MannwhitneyuResult(statistic=362681.0, pvalue=9.874833042004527e-216)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = cevd_df_second[cevd_df_second['dep_diag_yes_1'] == 1]\n",
    "mean_age = subset['full_screen_time_1'].mean()\n",
    "std_dev_age = subset['full_screen_time_1'].std()\n",
    "\n",
    "print(\"Mean full_screen_time_1 with exp cohort:\", mean_age)\n",
    "print(\"Standard Deviation of full_screen_time_1 with exp cohort:\", std_dev_age)\n",
    "print('P-value for full_screen_time_1:')\n",
    "stats.mannwhitneyu(x=cevd_df_second['full_screen_time_1'], y=cevd_df_second['dep_diag_yes_1'], alternative = 'two-sided')\n",
    "subset = cevd_df_second[cevd_df_second['dep_diag_yes_1'] == 0]\n",
    "mean_age = subset['full_screen_time_1'].mean()\n",
    "std_dev_age = subset['full_screen_time_1'].std()\n",
    "\n",
    "print(\"Mean full_screen_time_1 with control cohort:\", mean_age)\n",
    "print(\"Standard Deviation of full_screen_time_1 with control cohort:\", std_dev_age)\n",
    "print('P-value for full_screen_time_1:')\n",
    "stats.mannwhitneyu(x=cevd_df_second['full_screen_time_1'], y=cevd_df_second['dep_diag_yes_1'], alternative = 'two-sided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_151/2273173053.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cevd_df_first_onlycevd['met_minweek_filter'] = pd.to_numeric(cevd_df_first_onlycevd['met_minweek_filter'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean met with exp cohort: 5.812286699273147\n",
      "Standard Deviation of met with exp cohort: 6.352166994680101\n",
      "P-value for met:\n",
      "Mean met with control cohort: 6.542845007375757\n",
      "Standard Deviation of met with control cohort: 6.600851839090991\n",
      "P-value for met:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MannwhitneyuResult(statistic=205048665.0, pvalue=0.0)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cevd_df_first_onlycevd['met_minweek_filter'] = pd.to_numeric(cevd_df_first_onlycevd['met_minweek_filter'], errors='coerce')\n",
    "subset = cevd_df_first_onlycevd[cevd_df_first_onlycevd['dep_diag_yes'] == 1]\n",
    "mean_age = subset['met_minweek_filter'].mean()\n",
    "std_dev_age = subset['met_minweek_filter'].std()\n",
    "\n",
    "print(\"Mean met with exp cohort:\", mean_age)\n",
    "print(\"Standard Deviation of met with exp cohort:\", std_dev_age)\n",
    "print('P-value for met:')\n",
    "stats.mannwhitneyu(x=cevd_df_first_onlycevd['met_minweek_filter'], y=cevd_df_first_onlycevd['dep_diag_yes'], alternative = 'two-sided')\n",
    "subset = cevd_df_first_onlycevd[cevd_df_first_onlycevd['dep_diag_yes'] == 0]\n",
    "mean_age = subset['met_minweek_filter'].mean()\n",
    "std_dev_age = subset['met_minweek_filter'].std()\n",
    "\n",
    "print(\"Mean met with control cohort:\", mean_age)\n",
    "print(\"Standard Deviation of met with control cohort:\", std_dev_age)\n",
    "\n",
    "print('P-value for met:')\n",
    "stats.mannwhitneyu(x=cevd_df_first_onlycevd['met_minweek_filter'], y=cevd_df_first_onlycevd['dep_diag_yes'], alternative = 'two-sided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean met with exp cohort: 4.696119791666666\n",
      "Standard Deviation of met with exp cohort: 4.235193053864862\n",
      "P-value for met:\n",
      "Mean met with control cohort: 5.9630766852195425\n",
      "Standard Deviation of met with control cohort: 5.7445593324480475\n",
      "P-value for met:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MannwhitneyuResult(statistic=354665.5, pvalue=1.4503357471739504e-198)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cevd_df_second['met_minweek_filter'] = pd.to_numeric(cevd_df_second['met_minweek_filter'], errors='coerce')\n",
    "subset = cevd_df_second[cevd_df_second['dep_diag_yes_1'] == 1]\n",
    "mean_age = subset['met_minweek_filter'].mean()\n",
    "std_dev_age = subset['met_minweek_filter'].std()\n",
    "\n",
    "print(\"Mean met with exp cohort:\", mean_age)\n",
    "print(\"Standard Deviation of met with exp cohort:\", std_dev_age)\n",
    "print('P-value for met:')\n",
    "stats.mannwhitneyu(x=cevd_df_second['met_minweek_filter'], y=cevd_df_second['dep_diag_yes_1'], alternative = 'two-sided')\n",
    "subset = cevd_df_second[cevd_df_second['dep_diag_yes_1'] == 0]\n",
    "mean_age = subset['met_minweek_filter'].mean()\n",
    "std_dev_age = subset['met_minweek_filter'].std()\n",
    "\n",
    "print(\"Mean met with control cohort:\", mean_age)\n",
    "print(\"Standard Deviation of met with control cohort:\", std_dev_age)\n",
    "\n",
    "print('P-value for met:')\n",
    "stats.mannwhitneyu(x=cevd_df_second['met_minweek_filter'], y=cevd_df_second['dep_diag_yes_1'], alternative = 'two-sided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset = cevd_df_first_onlycevd[cevd_df_first_onlycevd['dep_diag_yes_1'] == 1]\n",
    "#mean_age = subset['met_minweek_filter_1'].mean()\n",
    "#std_dev_age = subset['met_minweek_filter_1'].std()\n",
    "\n",
    "#print(\"Mean met with exp cohort:\", mean_age)\n",
    "#print(\"Standard Deviation of met with exp cohort:\", std_dev_age)\n",
    "#print('P-value for met:')\n",
    "#stats.mannwhitneyu(x=cevd_df_first_onlycevd['met_minweek_filter_1'], y=cevd_df_first_onlycevd['dep_diag_yes_1'], alternative = 'two-sided')\n",
    "#subset = cevd_df_first_onlycevd[cevd_df_first_onlycevd['dep_diag_yes_1'] == 0]\n",
    "#mean_age = subset['met_minweek_filter_1'].mean()\n",
    "#std_dev_age = subset['met_minweek_filter_1'].std()\n",
    "\n",
    "#print(\"Mean met with control cohort:\", mean_age)\n",
    "#print(\"Standard Deviation of met with control cohort:\", std_dev_age)\n",
    "#print('P-value for met:')\n",
    "#stats.mannwhitneyu(x=cevd_df_first_onlycevd['met_minweek_filter_1'], y=cevd_df_first_onlycevd['dep_diag_yes_1'], alternative = 'two-sided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean tdi with exp cohort: -0.6260640410075639\n",
      "Standard Deviation of tdi with exp cohort: 3.4127689704131865\n",
      "P-value for tdi:\n",
      "Mean tdi with control cohort: -1.3366746369227531\n",
      "Standard Deviation of tdi with control cohort: 3.0883912733794125\n",
      "P-value for tdi:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MannwhitneyuResult(statistic=60386170.0, pvalue=0.0)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = cevd_df_first_onlycevd[cevd_df_first_onlycevd['dep_diag_yes'] == 1]\n",
    "mean_age = subset['tdi'].mean()\n",
    "std_dev_age = subset['tdi'].std()\n",
    "\n",
    "print(\"Mean tdi with exp cohort:\", mean_age)\n",
    "print(\"Standard Deviation of tdi with exp cohort:\", std_dev_age)\n",
    "print('P-value for tdi:')\n",
    "stats.mannwhitneyu(x=cevd_df_first_onlycevd['tdi'], y=cevd_df_first_onlycevd['dep_diag_yes'], alternative = 'two-sided')\n",
    "subset = cevd_df_first_onlycevd[cevd_df_first_onlycevd['dep_diag_yes'] == 0]\n",
    "mean_age = subset['tdi'].mean()\n",
    "std_dev_age = subset['tdi'].std()\n",
    "\n",
    "print(\"Mean tdi with control cohort:\", mean_age)\n",
    "print(\"Standard Deviation of tdi with control cohort:\", std_dev_age)\n",
    "print('P-value for tdi:')\n",
    "stats.mannwhitneyu(x=cevd_df_first_onlycevd['tdi'], y=cevd_df_first_onlycevd['dep_diag_yes'], alternative = 'two-sided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean tdi with exp cohort: -0.8737500000000001\n",
      "Standard Deviation of tdi with exp cohort: 3.4533799868148494\n",
      "P-value for tdi:\n",
      "Mean tdi with control cohort: -2.230037105751391\n",
      "Standard Deviation of tdi with control cohort: 2.548273633586423\n",
      "P-value for tdi:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MannwhitneyuResult(statistic=66502.0, pvalue=8.198966976390271e-89)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = cevd_df_second[cevd_df_second['dep_diag_yes_1'] == 1]\n",
    "mean_age = subset['tdi'].mean()\n",
    "std_dev_age = subset['tdi'].std()\n",
    "\n",
    "print(\"Mean tdi with exp cohort:\", mean_age)\n",
    "print(\"Standard Deviation of tdi with exp cohort:\", std_dev_age)\n",
    "print('P-value for tdi:')\n",
    "stats.mannwhitneyu(x=cevd_df_second['tdi'], y=cevd_df_second['dep_diag_yes_1'], alternative = 'two-sided')\n",
    "subset = cevd_df_second[cevd_df_second['dep_diag_yes_1'] == 0]\n",
    "mean_age = subset['tdi'].mean()\n",
    "std_dev_age = subset['tdi'].std()\n",
    "\n",
    "print(\"Mean tdi with control cohort:\", mean_age)\n",
    "print(\"Standard Deviation of tdi with control cohort:\", std_dev_age)\n",
    "print('P-value for tdi:')\n",
    "stats.mannwhitneyu(x=cevd_df_second['tdi'], y=cevd_df_second['dep_diag_yes_1'], alternative = 'two-sided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp Percentages of alc freq\n",
      "1.0    40.299572\n",
      "2.0    37.945792\n",
      "3.0    21.754636\n",
      "Name: alc_freq_filter, dtype: float64\n",
      "econtrol Percentages of alc freq\n",
      "2.0    46.383336\n",
      "1.0    28.437357\n",
      "3.0    25.179307\n",
      "Name: alc_freq_filter, dtype: float64\n",
      "ALC FREQ - Chi-square test statistic: 86.00577819644408\n",
      "ALC FREQ - P-value: 2.1090290351575827e-19\n"
     ]
    }
   ],
   "source": [
    "# get the %s of each category for alcohol frequency \n",
    "perc_alc_freq_d = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 1, 'alc_freq_filter'].value_counts(normalize=True)\n",
    "print('exp Percentages of alc freq')\n",
    "print(perc_alc_freq_d * 100)\n",
    "perc_alc_freq_c = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 0, 'alc_freq_filter'].value_counts(normalize=True)\n",
    "# print the value counts as percentages\n",
    "print('econtrol Percentages of alc freq')\n",
    "print(perc_alc_freq_c * 100)\n",
    "data = pd.crosstab(cevd_df_first_onlycevd['alc_freq_filter'], cevd_df_first_onlycevd['dep_diag_yes']).to_numpy()\n",
    "# chi-square test for independence\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(data)\n",
    "print('ALC FREQ - Chi-square test statistic:', chi2_stat)\n",
    "print('ALC FREQ - P-value:', p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp Percentages of alc freq\n",
      "2.0    50.0\n",
      "1.0    37.5\n",
      "3.0    12.5\n",
      "Name: alc_freq_filter_1, dtype: float64\n",
      "econtrol Percentages of alc freq\n",
      "2.0    51.020408\n",
      "1.0    27.087199\n",
      "3.0    21.892393\n",
      "Name: alc_freq_filter_1, dtype: float64\n",
      "ALC FREQ - Chi-square test statistic: 4.627050349730823\n",
      "ALC FREQ - P-value: 0.09891195431918683\n"
     ]
    }
   ],
   "source": [
    "# get the %s of each category for alcohol frequency \n",
    "perc_alc_freq_d = cevd_df_second.loc[cevd_df_second['dep_diag_yes_1'] == 1, 'alc_freq_filter_1'].value_counts(normalize=True)\n",
    "print('exp Percentages of alc freq')\n",
    "print(perc_alc_freq_d * 100)\n",
    "perc_alc_freq_c = cevd_df_second.loc[cevd_df_second['dep_diag_yes_1'] == 0, 'alc_freq_filter_1'].value_counts(normalize=True)\n",
    "# print the value counts as percentages\n",
    "print('econtrol Percentages of alc freq')\n",
    "print(perc_alc_freq_c * 100)\n",
    "data = pd.crosstab(cevd_df_second['alc_freq_filter_1'], cevd_df_second['dep_diag_yes_1']).to_numpy()\n",
    "# chi-square test for independence\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(data)\n",
    "print('ALC FREQ - Chi-square test statistic:', chi2_stat)\n",
    "print('ALC FREQ - P-value:', p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp Percentages of comorbid\n",
      "0    41.298146\n",
      "1    35.520685\n",
      "2    23.181170\n",
      "Name: comorb_sum_filter, dtype: float64\n",
      "CONTROL Percentages of comorbid\n",
      "0    46.841141\n",
      "1    33.709751\n",
      "2    19.449107\n",
      "Name: comorb_sum_filter, dtype: float64\n",
      "comorbid - Chi-square test statistic: 18.53431748872452\n",
      "comorbid - P-value: 9.447656493148253e-05\n"
     ]
    }
   ],
   "source": [
    "# get comorbid filter data\n",
    "perc_alc_freq_d = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 1, 'comorb_sum_filter'].value_counts(normalize=True)\n",
    "print('exp Percentages of comorbid')\n",
    "print(perc_alc_freq_d * 100)\n",
    "perc_alc_freq_c = cevd_df_first_onlycevd.loc[cevd_df_first_onlycevd['dep_diag_yes'] == 0, 'comorb_sum_filter'].value_counts(normalize=True)\n",
    "print('CONTROL Percentages of comorbid')\n",
    "print(perc_alc_freq_c * 100)\n",
    "data = pd.crosstab(cevd_df_first_onlycevd['comorb_sum_filter'], cevd_df_first_onlycevd['dep_diag_yes']).to_numpy()\n",
    "# chi-square test for independence\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(data)\n",
    "print('comorbid - Chi-square test statistic:', chi2_stat)\n",
    "print('comorbid - P-value:', p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp Percentages of comorbid\n",
      "2    35.9375\n",
      "1    34.3750\n",
      "0    29.6875\n",
      "Name: comorb_sum_filter_1, dtype: float64\n",
      "CONTROL Percentages of comorbid\n",
      "0    36.920223\n",
      "1    35.250464\n",
      "2    27.829314\n",
      "Name: comorb_sum_filter_1, dtype: float64\n",
      "comorbid - Chi-square test statistic: 2.151154608340237\n",
      "comorbid - P-value: 0.3411007795465394\n"
     ]
    }
   ],
   "source": [
    "# get comorbid filter data\n",
    "perc_alc_freq_d = cevd_df_second.loc[cevd_df_second['dep_diag_yes_1'] == 1, 'comorb_sum_filter_1'].value_counts(normalize=True)\n",
    "print('exp Percentages of comorbid')\n",
    "print(perc_alc_freq_d * 100)\n",
    "perc_alc_freq_c = cevd_df_second.loc[cevd_df_second['dep_diag_yes_1'] == 0, 'comorb_sum_filter_1'].value_counts(normalize=True)\n",
    "print('CONTROL Percentages of comorbid')\n",
    "print(perc_alc_freq_c * 100)\n",
    "data = pd.crosstab(cevd_df_second['comorb_sum_filter_1'], cevd_df_second['dep_diag_yes_1']).to_numpy()\n",
    "# chi-square test for independence\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(data)\n",
    "print('comorbid - Chi-square test statistic:', chi2_stat)\n",
    "print('comorbid - P-value:', p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#person year calculation if needed\n",
    "#i69_days = cevd_df_first['i69_t2e'].sum()\n",
    "#i69_personyears = i69_days/365\n",
    "#print(i69_personyears)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sensitivity analysis 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'gov_assistance_filter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'gov_assistance_filter'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#drop all px on disability assistance\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m cevd_df_noassist \u001b[38;5;241m=\u001b[39m cevd_df_first[\u001b[43mcevd_df_first\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgov_assistance_filter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mlen\u001b[39m(cevd_df_noassist)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'gov_assistance_filter'"
     ]
    }
   ],
   "source": [
    "#drop all px on disability assistance\n",
    "cevd_df_noassist = cevd_df_first[cevd_df_first['gov_assistance_filter']==0]\n",
    "len(cevd_df_noassist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#drop all px with comorbidities\n",
    "cevd_df_noassist_nocomor = cevd_df_noassist[cevd_df_noassist['comorb_sum_filter']==0]\n",
    "len(cevd_df_noassist_nocomor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cevd_df_noassist_nocomor.to_csv('cevd_first_model_sa.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sensitivity analysis 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#filter time to event for cevd for sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_cevd_t2e(value):\n",
    "    if pd.isna(value):\n",
    "        return 0\n",
    "    elif value < 1096:\n",
    "        return 1\n",
    "    elif 1096 <= value <= 1825:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "cevd_df_first['cevd_t2e_filter'] = cevd_df_first['cevd_t2e'].apply(filter_cevd_t2e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cevd_0 = cevd_df_first[cevd_df_first['cevd_t2e_filter']==0]\n",
    "print('Number of px with no cevd:',len(cevd_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cevd_1 = cevd_df_first[cevd_df_first['cevd_t2e_filter']==1]\n",
    "print('Number of px with cevd within 3 years of accelerometer study:',len(cevd_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cevd_2 = cevd_df_first[cevd_df_first['cevd_t2e_filter']==2]\n",
    "print('Number of px with cevd between 3-5 years after accelerometer study:',len(cevd_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cevd_3 = cevd_df_first[cevd_df_first['cevd_t2e_filter']==3]\n",
    "print('Number of px with cevd 5 or more years after accelerometer study:',len(cevd_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cevd_df_first.to_csv('cevd_first_model_sa_cevdt2e.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_t2e_cevd = cevd_df_first['cevd_t2e'].mean()\n",
    "std_t2e_cevd = cevd_df_first['cevd_t2e'].std()\n",
    "\n",
    "print('mean years from end of study to CeVD:', (mean_t2e_cevd/365))\n",
    "print('std:', (std_t2e_cevd/365))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sensitvity analysis 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_dep_dur(value):\n",
    "    if pd.isna(value):\n",
    "        return 0\n",
    "    elif value < 1896:\n",
    "        return 1\n",
    "    elif 1896 <= value <= 3650:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "cevd_df_first['dep_dur_filter'] = cevd_df_first['dep_dur'].apply(filter_dep_dur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dep_0 = cevd_df_first[cevd_df_first['dep_dur_filter']==0]\n",
    "print('number of patients without depression diagnosis prior to accelerometer study:', len(dep_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dep_1 = cevd_df_first[cevd_df_first['dep_dur_filter']==1]\n",
    "print('number of patients with depression diagnosis within 5 yrs before accelerometer study:', len(dep_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dep_2 = cevd_df_first[cevd_df_first['dep_dur_filter']==2]\n",
    "print('number of patients with depression diagnosis conferred between 5 and 10 yrs before accelerometer study:', len(dep_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dep_3 = cevd_df_first[cevd_df_first['dep_dur_filter']==3]\n",
    "print('number of patients with depression diagnosis conferred 10 or more yrs before accelerometer study:', len(dep_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_t2e_dep = cevd_df_first['dep_dur'].mean()\n",
    "std_t2e_dep = cevd_df_first['dep_dur'].std()\n",
    "\n",
    "print('mean years from dep diag to start of study:', (mean_t2e_dep/365))\n",
    "print('std:', (std_t2e_dep/365))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cevd_df_first.to_csv('cevd_first_model_sa_cevdt2eanddep.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#to upload on RAP UKBB\n",
    "#%%bash\n",
    "#dx upload <insert file name here>.ipynb --dest /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
